{
  "hash": "bbda9cb1942d7368fa2be0c252a50921",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ROC and AUC Interpretation\"\nauthor: \"Madiyar Aitbayev\"\ndate: \"2025-01-26\"\ncategories: [math, cv]\nimage: \"images/image.jpg\"\n---\n\n\n# AUC Interpretation\n\n## Introduction \n\nA **binary classification** is a machine learning model that classifies input data into two classes. We need different metrics to train or evaluate the performance of ML models. \nThe **ROC AUC** score is a popular metric for evaluating binary classification models. In this post, we will try to understand the intuition behind the **ROC AUC** with simple visualizations.\n\n## Recap\n\nLet's say we have two classes (\"positive\" and \"negative\") and a machine learning model that predicts a probability score between 0 and 1. A probability of **0.9** signifes that the input is closer to **\"positive\"** than \"negative\", a probability of **0.2** signifies that the input is closer to **\"negative\"**, and so on.\n\nHowever, our task is to produce a binary output: \"positive\" or \"negative\". To achieve this, we choose a **threshold**. Any input with a predicted probability score above the threshold is classified as positive, while inputs with lower scores are classified as negative.\n\n### Confusion Matrix\n\nThere are four outcomes of the predicted binary output, which can be visualized with the following table, called a confusion matrix:\n\n![confusion matrix](images/confusion_matrix.jpg)\n\n\nThe green row corresponds to the positive items in our dataset, while the red row corresponds to the negative items in the dataset. The columns correspond to the model predictions. The cells outlined with dark green are the items our model classified correctly, i.e., the accurate predictions of our model.   \n\nNow, we can give a few relevant metrics based on the confusion matrix.\n\n### Accuracy \n\nAccuracy is the proportion of all accurate predictions **among all items** in the dataset. It is defined as:\n\n$$\n\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}\n$$\n\nAccuracy can be misleading, especially with imbalanced datasets. For example, if 99% of our dataset is positive, a model that always predicts positive will have an accuracy of 99%, but this doesn't provide meaningful insight. Hence, we need other metrics. \n\n### True Positive Rate\n\nThe true positive rate or recall is the proportion of accurate predictions **among positive items**:\n\n$$\n\\text{Recall or TPR} = \\frac{TP}{TP+FN}\n$$\n\nThe recall only considers the green row (actual positives) from our confusion table, and completely ignores the red row.\n\n### True Negative Rate\n\nThe true negative rate or specificity is the proportion of accurate predictions **among negative items**:\n\n$$\n\\text{Specificity or TNR} = \\frac{TN}{FP+TN}\n$$\n\nThe specificity only consider the red row (actual negatives) from our confusion table. \n\n### False Positive Rate \n\nThe false positive rate is the proportion of **inaccurate** predictions **among negative items**:\n\n$$\n\\text{FPR} = \\frac{FP}{FP+TN}\n$$\n\nalternatively:\n\n$$\n\\text{FPR}=1-\\text{TNR}\n$$\n\nThe false positive rate is related to the true negative rate. However, we will be using FPR more than TNR in the next sections. \n\n### Visualizations\n\n\n```{ojs}\nstats = {\n    function recall(rank, threshold) {\n        var truePositive = 0;\n        for (var i in rank) {\n            if (rank[i] > threshold) {\n                truePositive += 1;\n            }\n        }\n        return truePositive * 1.0 / rank.length;\n    }\n\n    function specifity(rank, threshold) {\n        var trueNegative = 0;\n        for (var i in rank) {\n            if (rank[i] <= threshold) {\n                trueNegative += 1;\n            }\n        }\n        return trueNegative * 1.0 / rank.length;\n    }\n\n    function fallout(rank, threshold) {\n        return 1.0 - specifity(rank, threshold);\n    }\n\n    return {\n        recall: recall,\n        specifity: specifity,\n        fallout: fallout\n    }\n}\n```\n\n```{ojs}\npositives = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0];\n```\n\n```{ojs}\nnegatives = [0.1, 0.2, 0.3, 0.5, 0.8, 1.0];\n```\n\n```{ojs}\nlightRed = \"#ff8886\"\n```\n\n```{ojs}\n{\nvar positiveDots = []\nfor (var i in positives) {\n    positiveDots.push({\n        x: positives[i], \n        y: 1,\n        r: positives[i],\n        strokeWidth: positives[i] >= thresholdSlider ? 3 : 0,\n        stroke: \"green\",\n    })\n}\nvar negativeDots = [];\nfor (var i in negatives) {\n    negativeDots.push({\n        x: negatives[i],\n        y: 0,\n        r: negatives[i],\n        strokeWidth: negatives[i] < thresholdSlider ? 3 : 0,\n        // stroke: \"#8b0000\",\n        stroke: \"green\"\n    })\n}\nreturn Plot.plot({\n    r: {range: [0, 20]},\n    height: 100,\n    width: 600,\n    x: { label: \"probability\" },\n    y: { axis: null },\n    marginRight: 25,\n    marginTop: 25,\n    marginBottom: 25,\n    marginLeft: 25,\n    marks: [\n        Plot.dot(positiveDots, {\n            x: \"x\", y: \"y\", r: \"r\", \n            stroke: \"stroke\", strokeWidth: \"strokeWidth\", \n            fill: \"lightgreen\"\n        }),\n        Plot.dot(negativeDots, {\n            x: \"x\", y: \"y\", r: \"r\", \n            stroke: \"stroke\", strokeWidth: \"strokeWidth\", \n            fill: lightRed\n        }),\n        Plot.line([\n            [Math.min(thresholdSlider, 1), 0], \n            [Math.min(thresholdSlider, 1), 1]\n        ], { strokeWidth: 3, strokeDasharray: \"2, 6\"})\n    ]\n})\n}\n```\n\n```{ojs}\nviewof thresholdSlider = htl.html`<input type=range min=0 max=1.1 step=0.01 value=50 style=\"width:600px\">`\n``` \n\n```{ojs}\nInputs.table([{\n    \"True Positive Rate\": \"2/5\",\n    \"False Positive Rate\": 2,\n}])\n```\n\n```{ojs}\n{\n\n\n\nfunction roc_curve(positives, negatives) {\n    var thresholds = positives.concat(negatives);\n    thresholds.push(-0.1); \n    thresholds.push(1.1); \n    thresholds.sort();\n    thresholds.reverse();\n    var result = []\n    for (var i in thresholds) {\n        const threshold = thresholds[i]; \n        if (i == 0 || threshold != thresholds[i-1]) {\n            const f = stats.fallout(negatives, threshold);\n            const r = stats.recall(positives, threshold);\n            const prevx = (i == 0 ? -1 : result[result.length - 1].x);\n            result.push({x: prevx >= f ? prevx + 1e-6 : f, y: r})\n        }\n    }\n    return result;\n}\n\nvar falloutDots = []\nfor (var i in negatives) {\n    const value = stats.fallout(negatives, negatives[i]);\n    falloutDots.push({\n        x: value, \n        y: 0.0,\n        r: negatives[i] \n    })\n}\nvar recallDots = []\nfor (var i in positives) {\n    const value = stats.recall(positives, positives[i]);\n    recallDots.push({\n        x: 0.0, \n        y: value,\n        r: positives[i],\n    })\n}\n\nvar aucPoints = [];\nfor (var i in falloutDots) {\n    for (var j in recallDots) {\n        if (falloutDots[i].r <= recallDots[j].r) {\n            aucPoints.push([falloutDots[i].x, recallDots[j].y])\n        }\n    }\n}\naucPoints.reverse();\nconst point = aucPoints[Math.floor(pointSelection / 101 * aucPoints.length)];\n\nreturn Plot.plot({\n    grid: true,\n    x: {label: \"False Negative Rate\"},\n    y: {label: \"True Positive Rate\"},\n    r: {range: [0, 20]},\n    aspectRatio: 1.0,\n    marks: [\n        Plot.line([[0, point[1]], point], {\n           strokeDasharray: \"1, 7\",\n        }),\n        Plot.line([[point[0], 0], point], {\n           strokeDasharray: \"1, 7\",\n        }),\n        Plot.areaY(roc_curve(positives, negatives), {\n            x: \"x\",\n            y: \"y\",\n            opacity: 0.15,\n        }),\n        Plot.line(roc_curve(positives, negatives), {\n            x: \"x\",\n            y: \"y\",\n            strokeWidth: 2\n        }),\n        Plot.dot(falloutDots, {x: \"x\", y: \"y\", r: \"r\", fill: lightRed}),\n        Plot.dot(recallDots, {x: \"x\", y: \"y\", r: \"r\", fill: \"lightgreen\"}),\n        Plot.line([[0, 0], [1, 1]], {\n            stroke: \"orange\",\n            opacity: 0.0,\n        }),\n        Plot.dot([{x: point[0], y: point[1], r: 0.15}], {\n            x: \"x\", y: \"y\", r: \"r\",\n            fill: \"black\",\n        }),\n    ]\n})\n}\n```\n\n```{ojs}\nviewof pointSelection = htl.html`<input type=range min=1 max=100 step=1 value=50 style=\"width:400px\">`\n```\n\n::: {#d1a86fc8 .cell execution_count=1}\n``` {.python .cell-code}\nprint(\"Hello World\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHello World\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}