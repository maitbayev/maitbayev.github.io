{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Auto-Encoding Variational Bayes Notes\"\n",
        "author: \"Madiyar Aitbayev\"\n",
        "date: \"2024-07-21\"\n",
        "categories: [diffusion, paper, autoencoder]\n",
        "image: \"images/image.jpg\"\n",
        "---\n",
        "\n",
        "\n",
        "# Notes for \"Auto-Encoding Variational Bayes\"\n",
        "\n",
        "My notes for the [\"Auto-Encoding Variational Bayes\"](https://arxiv.org/pdf/1312.6114) paper. Feel free to ask questions on [my telegram channel](https://t.me/swemanml)\n",
        "\n",
        "## Method\n",
        "\n",
        "### Problem scenario\n",
        "\n",
        "Let us consider some independently and identically distributed (i.i.d) dataset $\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}$. We assume that the data involves an unobserved random variable $\\mathbf{z}$. Then, the process consists of two steps:\n",
        "\n",
        "1.  $\\mathbf{z[i]}$ is generated from some prior distribution $\\mathbf{p_{\\theta*}(z)}$\n",
        "\n",
        "2.  $\\mathbf{x[i]}$ is generated from some conditional distribution $\\mathbf{p_{\\theta*}(x|z)}$\n",
        "\n",
        "Unfortunately, the true parameters $\\mathbf{ \\theta* }$ and the latent variables $\\mathbf{z[i]}$ are unknown to us. Additionally, $\\mathbf{p_\\theta(z|x)}$ is intractable, so we approximate it with $\\mathbf{ q_\\phi(z|x) }$. The model $\\mathbf{ q_\\phi(z|x) }$ is a probabilistic encoder model parameterized by $\\phi$ . Similarly, $\\mathbf{p_\\theta(x|z)}$ is a probabilistic decoder model parameterized by $\\mathbf{\\theta}$.\n",
        "\n",
        "### The variational lower bound\n",
        "\n",
        "Ideally, we would like to optimize the marginal likelihoods of the dataset **X**:\n",
        "\n",
        "$$\n",
        "\\mathbf{\n",
        "\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n",
        "}\n",
        "$$\n",
        "\n",
        "Where each term can be rewritten as:\n",
        "\n",
        "$$\n",
        "\\mathbf{\n",
        "\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n",
        "}\n",
        "$$\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Proof\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log p_\\theta(\\mathbf{x}) \n",
        "&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}} \n",
        "\\qquad \\text{(Multiplying by 1)}\\\\\n",
        "&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n",
        "&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[ \n",
        "\\log p_\\theta(\\mathbf{x}) \n",
        "\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n",
        "&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n",
        "\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n",
        "&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n",
        "\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n",
        "\\right]\\\\\n",
        "&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n",
        "\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n",
        "\\right]\\\\\n",
        "&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n",
        "+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We used the chain rule of probability:\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n",
        "$$\n",
        ":::\n",
        "\n",
        "The first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (**variational) lower bound.** Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ**.** Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\n",
        "\n",
        "**The variational lower bound,** also called as th**e evidence lower bound (**ELBO) can be also rewritten as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n",
        "&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n",
        "-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n",
        "\\right]\\\\\n",
        "&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n",
        "+\n",
        "\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n",
        "\\log p_\\theta(\\mathbf{x[i]|z})\n",
        "\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Proof\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathcal{L}(\\theta, \\phi; \\mathbf{x}) \n",
        "&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n",
        "\\right]\\\\\n",
        "&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n",
        "\\right]\\\\\n",
        "&= \n",
        "\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n",
        "\\right]\n",
        "+\n",
        "\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n",
        "\\log  p_\\theta(\\mathbf{x|z}) \n",
        "\\right]\\\\\n",
        "&= \n",
        "-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n",
        "+\n",
        "\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n",
        "\\log p_\\theta(\\mathbf{x|z})\n",
        "\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        ":::\n",
        "\n",
        "### The reparameterization trick\n",
        "\n",
        "The sampling from $\\mathbf{q_\\phi(z|x)}$ is a stochastic process which is not differentiable w.r.t. $\\phi$. We can use an alternative method for generating sample from $\\mathbf{q_\\phi(z|x)}$, i.e., the reparameterization trick. We can often express the random variable **z** as a deterministic variable $\\mathbf{z=g_\\phi(\\epsilon, x)}$, where $\\epsilon$ is an independent variable and $\\mathbf{g_\\phi}$ is a function parameterized by $\\phi$.\n",
        "\n",
        "The $\\mathbf{q_\\phi(z|x)}$ is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n",
        "\n",
        "$$\n",
        "\\mathbf{\n",
        "q_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n",
        "}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }$ and we can choose $g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon$\n",
        "\n",
        "Therefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable.\n",
        "\n",
        "## Variational Auto-Encoder\n",
        "\n",
        "We use a neural network for the probabilistic encoder $\\mathbf{ q_\\phi(z|x)}$ and where the parameters $\\phi$ and $\\theta$ are optimized jointly. We also assume that:\n",
        "\n",
        "-   $p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})$ - the prior over the latent variables is a standard Gaussian\n",
        "\n",
        "-   $p_\\theta(\\mathbf{x|z})$ is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n",
        "\n",
        "-   $q_\\phi(\\mathbf{z|x})$ is approximately Gaussian with an approximately diagonal covariance: $\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})$\n",
        "\n",
        "We use the reparameterization trick to sample from the posterior using $\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}$. In this model both $\\mathbf{p_\\theta(z)}$ and $q_\\phi(\\mathbf{z|x})$ are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]}) \n",
        "\\simeq \n",
        "\\frac{1}{2}\\sum_{j=1}^J{\\left(\n",
        "1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n",
        "\\right)}\n",
        "+\n",
        "\\frac{1}{L}\\sum_{l=1}^L{\n",
        "\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n",
        "}\n",
        "$$\n",
        "\n",
        "In the above equation , only the reconstruction error $E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]$ requires estimation by sampling, since he KL-divergence term is integrated analytically.\n"
      ],
      "id": "20a04e70"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}