---
title: "Denoising Diffusion Probabilistic Models"
author: "Madiyar Aitbayev"
date: "2024-07-15"
categories: [diffusion, paper]
image: "image.jpg"
---

# Notes for "Denoising Diffusion Probabilistic Models"

My notes for the ["Denoising Diffusion Probabilistic Models"](https://arxiv.org/pdf/2006.11239) paper.

## Background

![](figure2.jpg){width="490"}

### Forward Process

We have latent variables $\mathbf{x_1},\mathbf{x_2}, ..., \mathbf{x_T}$ of the same dimensionality as the image $\mathbf{x_0}$. The f*orward process* or *diffusion process* is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule $\beta_1, ..., \beta_T$:

$$
\mathbf{
q(x_t|x_{t-1})= \mathcal{N} (x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_{t}I),
\quad 
q(x_{1:T}|x_0)= \prod_{t=1}^T{q(x_t|x_{t-1})}
}
$$

The above equation means that we start with our data $\mathbf{x_0}$, then we sample a noisy version of our image $\mathbf{x_1}$ with the mean $\mathbf{x_0}$ scaled by $\mathbf{\sqrt{1-\beta_1}}$ and the variance $\mathbf{\beta_1}$. Finally, we repeat this process $\mathbf{T}$ times and arrive at pure standard Gaussian noise when small enough $\mathbf{\beta}$ and large enough $\mathbf{T}$ values are used. This process is fixed and not learned, unlike variational auto-encoder models.

A nice benefit from the forward process is that we can sample $\mathbf{x_t}$ at any timestep $\mathbf{t}$ in closed form without simulating the forward process steps $\mathbf{t}$ times. Let $\mathbf{\alpha_t=1-\beta_t}$ and $\mathbf{ \bar{\alpha}_t = \prod_{i=1}^t\alpha_i }$ :

$$
\mathbf {
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1-\bar{\alpha}_t)I)
}
$$

::: {.callout-note collapse="true" icon="false"}
## Proof

Suppose we have access to random noise variables $\{\mathbf{\epsilon_t^*}, \mathbf{\epsilon_t}\}_{t=0}^T \sim \mathcal{N}(\mathbf{\epsilon_; 0, I})$:

$$
\mathbf {
\begin{aligned}
x_t&=\sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon^*_{t-1}\\ 
&= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon^*_{t-2}) + \sqrt{1-\alpha_t}\epsilon^*_{t-1}\\
&= \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} +\sqrt{1-\alpha_t \alpha_{t-1}}\epsilon_{t-2}\\
&= \cdots\\
&= \sqrt{\prod_{t=1}^t{\alpha_i}}x_0 + \sqrt{1-\prod_{t=1}^t{\alpha_i}}\epsilon_0\\
&= \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0\\
&\sim \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1-\bar{\alpha}_t)I)\\
\end{aligned}
}
$$\
Here we used the fact that [the sum of two independent Gaussian random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables) remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.
:::

### Reverse Process

The *reverse process*is a **learned** Gaussian transition starting at a pure standard Gaussian noise $p(\mathbf{x}_T)=\mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})$, then:

$$
p_{\theta}(\mathbf{x_{t-1}|x_t}) = \mathcal{N}(\mathbf{x_{t-1}}; \mathbf{\mu_\theta(x_t, t), \Sigma_\theta(x_t, t)}) 
\quad p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T)\prod_{t=1}^T{q(\mathbf{x_t|x_{t-1}})}
$$

The above means that we start with a pure noise $\mathbf{x}_T$, then we gradually remove noises by sampling from learned means $\mu_\theta(\mathbf{x_t; t})$ and variances $\mathbf{\Sigma_\theta(x_t, t)}$ parameterized by $\mathbf{\theta}$.