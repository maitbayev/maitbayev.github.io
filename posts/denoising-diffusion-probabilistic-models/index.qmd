---
title: "Denoising Diffusion Probabilistic Models"
author: "Madiyar Aitbayev"
date: "2024-07-15"
categories: [diffusion, paper]
image: "images/image.jpg"
---

# Notes for "Denoising Diffusion Probabilistic Models"

My notes for the ["Denoising Diffusion Probabilistic Models"](https://arxiv.org/pdf/2006.11239) paper.

## Background

![](images/figure2.jpg){width="490"}

### Forward Process

We have latent variables $\mathbf{x_1},\mathbf{x_2}, ..., \mathbf{x_T}$ of the same dimensionality as the image $\mathbf{x_0}$. The f*orward process* or *diffusion process* is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule $\beta_1, ..., \beta_T$:

$$
\mathbf{
q(x_t|x_{t-1})= \mathcal{N} (x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_{t}I),
\quad 
q(x_{1:T}|x_0)= \prod_{t=1}^T{q(x_t|x_{t-1})}
}
$$

The above equation means that we start with our data $\mathbf{x_0}$, then we sample a noisy version of our image $\mathbf{x_1}$ with the mean $\mathbf{x_0}$ scaled by $\mathbf{\sqrt{1-\beta_1}}$ and the variance $\mathbf{\beta_1}$. Finally, we repeat this process $\mathbf{T}$ times and arrive at pure standard Gaussian noise when small enough $\mathbf{\beta}$ and large enough $\mathbf{T}$ values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.

A nice benefit from the forward process is that we can sample $\mathbf{x_t}$ at any timestep $\mathbf{t}$ in closed form without simulating the forward process steps $\mathbf{t}$ times. Let $\mathbf{\alpha_t=1-\beta_t}$ and $\mathbf{ \bar{\alpha}_t = \prod_{i=1}^t\alpha_i }$ :

$$
\mathbf {
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1-\bar{\alpha}_t)I)
}
$$

::: {.callout-note collapse="true" icon="false"}
## Proof

Suppose we have access to random noise variables $\{\mathbf{\epsilon_t^*}, \mathbf{\epsilon_t}\}_{t=0}^T \sim \mathcal{N}(\mathbf{\epsilon_; 0, I})$:

$$
\mathbf {
\begin{aligned}
x_t&=\sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon^*_{t-1}\\ 
&= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon^*_{t-2}) + \sqrt{1-\alpha_t}\epsilon^*_{t-1}\\
&= \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} +\sqrt{1-\alpha_t \alpha_{t-1}}\epsilon_{t-2}\\
&= \cdots\\
&= \sqrt{\prod_{t=1}^t{\alpha_i}}x_0 + \sqrt{1-\prod_{t=1}^t{\alpha_i}}\epsilon_0\\
&= \sqrt{\bar{\alpha_t}}x_0 + \sqrt{1-\bar{\alpha_t}}\epsilon_0\\
&\sim \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1-\bar{\alpha}_t)I)\\
\end{aligned}
}
$$\
Here we used the fact that [the sum of two independent Gaussian random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables) remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.
:::

We can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.

### Reverse Process

The *reverse process* is a **learned** Gaussian transition starting at a pure standard Gaussian noise $p(\mathbf{x}_T)=\mathcal{N}(\mathbf{x}_T; \mathbf{0}, \mathbf{I})$, then:

$$
p_{\theta}(\mathbf{x_{t-1}|x_t}) = \mathcal{N}(\mathbf{x_{t-1}}; \mathbf{\mu_\theta(x_t, t), \Sigma_\theta(x_t, t)}) 
\quad p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T)\prod_{t=1}^T{p_\theta(\mathbf{x_{t-1}|x_t})}
$$

The above means that we start with a pure noise $\mathbf{x}_T$, then we gradually remove noises by sampling from learned means $\mu_\theta(\mathbf{x_t; t})$ and variances $\mathbf{\Sigma_\theta(x_t, t)}$ parameterized by $\mathbf{\theta}$.

We can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from $\mathbf{x}_{1000}$, $\mathbf{x}_{750}$, $\mathbf{x}_{500}$, $\mathbf{x}_{250}$, $\mathbf{x}_{0}$

![](images/figure3.jpg)

### Loss

Training is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:

$$
\mathbb{E}[-\log p_\theta(\mathbf{x}_0)] \le 
\mathbb{E}_q\left[-\log 
  \frac{p_\theta(\mathbf{x_{0:T}})}{q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]=
\mathbb{E_q}\left[
-\log p(\mathbf{x_T}) - \sum_{t\ge1}\log{\frac{p_\theta(\mathbf{x_{t-1}|x_t})}{q(\mathbf{x_{t}|x_{t-1}})}}
\right]
$$

::: {.callout-note collapse="true"}
## Proof

**Proof 1: Using Jenson's Inequality**

$$
\mathbf {
\begin{aligned}
\log p_\theta(\mathbf{x})&=log\int{p_\theta(\mathbf{x_{0:T}})d\mathbf{x_{1:T}}}\\
&=log\int{ \frac {p_\theta(\mathbf{x_{0:T}}) q(\mathbf{x_{1:T}}|\mathbf{x_0}) } {q(\mathbf{x_{1:T}}|\mathbf{x_0})} d\mathbf{x_{1:T}} }\\
&=\log \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})}\left[ 
  \frac{p_\theta(\mathbf{x_{0:T}})}{q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]\\
&\ge \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})}\left[  \log
  \frac{p_\theta(\mathbf{x_{0:T}})}{q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]\\
\end{aligned}
}
$$

**Proof 2: Using KL Divergence**

$$
\mathbf {
\begin{aligned}
\log p(\mathbf{x_0})&=\log p(\mathbf{x_0}) \int { 
  q(\mathbf{x_{1:T}}|\mathbf{x_0})  d\mathbf{x_{1:T}}
} \\
&= \int { 
  q(\mathbf{x_{1:T}}|\mathbf{x_0}) (\log p(\mathbf{x_0})) d\mathbf{x_{1:T}}
} \\
&= \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log p(\mathbf{x_0})
\right]\\
&=  \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log
\frac{q(\mathbf{x_{0:T}})}{q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]\\
&=  \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log
\frac{ q(\mathbf{x_{0:T}}) p_\theta(\mathbf{x_{0:T}}) }{ q(\mathbf{x_{1:T}}|\mathbf{x_0}) p_\theta(\mathbf{x_{0:T}})}
\right]\\
&=\mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log
\frac{ p_\theta(\mathbf{x_{0:T}}) }{ q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]
+
 \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log
\frac{ q(\mathbf{x_{1:T}|x_0})p(\mathbf{x_0}) }{ p_\theta(\mathbf{x_{1:T}|x_0})p(\mathbf{x_0}) }
\right]\\
&=\mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log
\frac{ p_\theta(\mathbf{x_{0:T}}) }{ q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]
+
D_{KL}(q(\mathbf{x_{1:T}}|\mathbf{x_0}) || p_\theta(\mathbf{x_{1:T}|x_0}))\\
&\ge \mathbb{E}_{q(\mathbf{x_{1:T}}|\mathbf{x_0})} \left[
\log
\frac{ p_\theta(\mathbf{x_{0:T}}) }{ q(\mathbf{x_{1:T}}|\mathbf{x_0})}
\right]
\end{aligned}
}
$$

The KL divergence is always non-negative, hence the lower bound approximation
:::

The forward process variances $\beta_t$ are hyperparameters. The reverse process $p_\theta(\mathbf{x_{t-1} | x_t})$ is Gaussian, when the forward process is Gaussian and $\beta_t$ are small.

Efficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, $\mathbf{x_{t-1}}$ and $\mathbf{x_{t}}$, which introduces high variance Monte Carlo estimates. We can reduce the variance:

$$
E_q \left[
D_{KL}(q(\mathbf{x_T|x_0}||p(\mathbf{x_T}))
+ 
\sum_{t\gt1}{D_{KL}(q(\mathbf{x_{t-1}|x_t,x_0})||p_\theta(\mathbf{x_{t-1}|x_t}))}
-
\log p_\theta(\mathbf{x_0|x_1})
\right]
$$

::: {.callout-note collapse="true"}
## Proof from Appendix A

$$
\begin{aligned}
L &= \mathbb{E}_{q( \mathbf{x_{1:T}|x0} )} \left[
-\log \frac{p_\theta(\mathbf{x_{0:T}}) } { q(\mathbf{x_{1:T}|x_0}) }\\
\right]\\
&=  \mathbb{E}_{q( \mathbf{x_{1:T}|x0} )} \left[
-\log p(\mathbf{x_T}) 
-
\sum_{t\ge1}{\log \frac{ p_\theta(\mathbf{x_{t-1}|x_t}) } { q(\mathbf{x_t|x_{t-1}} ) } }
\right]\\
&=  \mathbb{E}_{q( \mathbf{x_{1:T}|x0} )} \left[
-\log p(\mathbf{x_T}) 
-
\sum_{t>1}{\log \frac{ p_\theta(\mathbf{x_{t-1}|x_t}) } { q(\mathbf{x_t|x_{t-1}} ) } }
- \log \frac{ p_\theta(\mathbf{x_{0}|x_1}) } { q(\mathbf{x_1|x_0} ) }
\right]\\
&=  -\mathbb{E}_{q( \mathbf{x_{1:T}|x0} )} \left[
-\log p(\mathbf{x_T}) 
-
\sum_{t>1}{\log \frac{ p_\theta(\mathbf{x_{t-1}|x_t}) } { q(\mathbf{x_{t-1}|x_t, x_0} ) } }
\cdot \frac{q(\mathbf{x_{t-1}|x_0}) }{ q(\mathbf{x_t|x_0}) }
- \log \frac{ p_\theta(\mathbf{x_{0}|x_1}) } { q(\mathbf{x_1|x_0} ) }
\right]\\
&=  -\mathbb{E}_{q( \mathbf{x_{1:T}|x0} )} \left[
-\log \frac{ p(\mathbf{x_T}) }{q(\mathbf{x_T|x_0})}
-
\sum_{t>1}{\log \frac{ p_\theta(\mathbf{x_{t-1}|x_t}) } { q(\mathbf{x_{t-1}|x_t, x_0} ) } }
- \log p_\theta(\mathbf{x_{0}|x_1})
\right]\\
&=  \mathbb{E}_{q( \mathbf{x_{1:T}|x0} )} \left[
D_{KL}(q(\mathbf{x_T|x_0} || p(\mathbf{x}_T))
+
\sum_{t>1}{ D_{KL}( q(\mathbf{x_{t-1}|x_t, x_0})||p_\theta(\mathbf{x_{t-1}|x_t})  )  }
- \log p_\theta(\mathbf{x_{0}|x_1})
\right]
\end{aligned}
$$
:::

The equation above is tractable when conditioned on $\mathbf{x_0}$:

$$
q(\mathbf{x_{t-1}|x_t, x_0})=\mathcal{N}(\mathbf{x_{t-1}; \mathbf{\tilde{\mu_t}(
x_t, x_0), \tilde{\beta_t}}I})
$$

where:

$$
 \mathbf{\tilde{\mu_t}(
x_t, x_0) = 
\frac{ \sqrt{\bar{\alpha}_{t-1}} \beta_t }{ 1 - \bar{\alpha}_t}x_0
+
\frac{ \sqrt{\alpha_t}(1-\bar{\alpha}_{t-1}) }{ 1 - \bar{\alpha}_t} x_t
}
$$

and

$$
\tilde{\beta}_t=\frac{ 1-\bar{\alpha}_{t-1} }{1-\bar{\alpha}_t}\beta_t
$$

All KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates.

## Diffusion models and denoising autoencoders