<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Madiyar&#39;s Page</title>
<link>https://maitbayev.github.io/esl/</link>
<atom:link href="https://maitbayev.github.io/esl/index.xml" rel="self" type="application/rss+xml"/>
<description>Madiyar&#39;s Page</description>
<generator>quarto-1.8.27</generator>
<lastBuildDate>Mon, 09 Feb 2026 23:44:52 GMT</lastBuildDate>
<item>
  <title>2.3 Least Squares and Nearest Neighbors</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors.html</link>
  <description><![CDATA[ 





<section id="from-least-squares-to-nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="from-least-squares-to-nearest-neighbors">2.3.3 From Least Squares to Nearest Neighbors</h3>
<ol type="1">
<li>Generates 10 means <img src="https://latex.codecogs.com/png.latex?m_k"> from a bivariate Gaussian distrubition for each color:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?N((1,%200)%5ET,%20%5Ctextbf%7BI%7D)"> for <span style="color: blue">BLUE</span></li>
<li><img src="https://latex.codecogs.com/png.latex?N((0,%201)%5ET,%20%5Ctextbf%7BI%7D)"> for <span style="color: orange">ORANGE</span></li>
</ul></li>
<li>For each color generates 100 observations as following:
<ul>
<li>For each observation it picks <img src="https://latex.codecogs.com/png.latex?m_k"> at random with probability 1/10.</li>
<li>Then generates a <img src="https://latex.codecogs.com/png.latex?N(m_k,%5Ctextbf%7BI%7D/5)"></li>
</ul></li>
</ol>
<div id="cell-4" class="cell" data-scrolled="false" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-6"></span>
<span id="cb1-7">sample_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_data(size, mean):</span>
<span id="cb1-10">    identity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.identity(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-11">    m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.multivariate_normal(mean, identity, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb1-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.array([</span>
<span id="cb1-13">        np.random.multivariate_normal(random.choice(m), identity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb1-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(size)</span>
<span id="cb1-15">    ])</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_data(orange_data, blue_data): </span>
<span id="cb1-18">    axes.plot(orange_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], orange_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>)</span>
<span id="cb1-19">    axes.plot(blue_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], blue_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>)</span>
<span id="cb1-20">    </span>
<span id="cb1-21">blue_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_data(sample_size, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-22">orange_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_data(sample_size, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb1-23"></span>
<span id="cb1-24">data_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.r_[blue_data, orange_data]</span>
<span id="cb1-25">data_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.r_[np.zeros(sample_size), np.ones(sample_size)]</span>
<span id="cb1-26"></span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plotting</span></span>
<span id="cb1-28">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.figure(figsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb1-29">axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-30">plot_data(orange_data, blue_data)</span>
<span id="cb1-31"></span>
<span id="cb1-32">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="linear-models-and-least-squares" class="level3">
<h3 class="anchored" data-anchor-id="linear-models-and-least-squares">2.3.1 Linear Models and Least Squares</h3>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20=%20%5Chat%7B%5Cbeta_0%7D%20+%20%5Csum_%7Bj=1%7D%5E%7Bp%7D%20X_j%5Chat%7B%5Cbeta_j%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_0%7D"> is the intercept, also know as the <em>bias</em>. It is convenient to include the constant variable 1 in X and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_0%7D"> in the vector of coefficients <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D">, and then write as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20=%20X%5ET%5Chat%7B%5Cbeta%7D%20"></p>
<section id="residual-sum-of-squares" class="level4">
<h4 class="anchored" data-anchor-id="residual-sum-of-squares">Residual sum of squares</h4>
<p>How to fit the linear model to a set of training data? Pick the coefficients <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> to minimize the <em>residual sum of squares</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?RSS(%5Cbeta)%20=%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20(y_i%20-%20x_i%5ET%5Cbeta)%20%5E%202%20=%20(%5Ctextbf%7By%7D%20-%20%5Ctextbf%7BX%7D%5Cbeta)%5ET%20(%5Ctextbf%7By%7D%20-%20%5Ctextbf%7BX%7D%5Cbeta)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctextbf%7BX%7D"> is an <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20p"> matrix with each row an input vector, and <img src="https://latex.codecogs.com/png.latex?%5Ctextbf%7By%7D"> is an N-vector of the outputs in the training set. Differentiating w.r.t. β we get the normal equations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cbeta)%20=%200"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D"> is nonsingular, then the unique solution is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%20=%20(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"></p>
<div id="cell-9" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> LinearRegression:</span>
<span id="cb2-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y):</span>
<span id="cb2-3">        X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.c_[np.ones((X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), X]</span>
<span id="cb2-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.inv(X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> y</span>
<span id="cb2-5"></span>
<span id="cb2-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb2-7">    </span>
<span id="cb2-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb2-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.dot(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.beta, np.r_[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, x])</span>
<span id="cb2-10"></span>
<span id="cb2-11">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression().fit(data_x, data_y)</span>
<span id="cb2-12"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta = "</span>, model.beta)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>beta =  [ 0.52677771 -0.15145005  0.15818643]</code></pre>
</div>
</div>
</section>
<section id="example-of-the-linear-model-in-a-classification-context" class="level4">
<h4 class="anchored" data-anchor-id="example-of-the-linear-model-in-a-classification-context">Example of the linear model in a classification context</h4>
<p>The fitted values <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D"> are converted to a fitted class variable <img src="https://latex.codecogs.com/png.latex?%5Chat%7BG%7D"> according to the rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7BG%7D%20=%20%5Cbegin%7Bcases%7D%0A%5Ctext%7BORANGE%7D%20&amp;%20%5Ctext%7B%20if%20%7D%20%5Chat%7BY%7D%20%5Cgt%200.5%20%5C%5C%0A%5Ctext%7BBLUE%20%20%20%20%7D%20&amp;%20%5Ctext%7B%20if%20%7D%20%5Chat%7BY%7D%20%5Cleq%200.5%0A%5Cend%7Bcases%7D%0A%5Cend%7Bequation%7D%0A"></p>
<div id="cell-11" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> filterfalse, product</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_grid(orange_grid, blue_grid):</span>
<span id="cb4-4">    axes.plot(orange_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], orange_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>, zorder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>,</span>
<span id="cb4-5">              color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>, alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-6"></span>
<span id="cb4-7">    axes.plot(blue_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], blue_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>, zorder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>,</span>
<span id="cb4-8">          color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-9"></span>
<span id="cb4-10">plot_xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axes.get_xlim()</span>
<span id="cb4-11">plot_ylim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axes.get_ylim()</span>
<span id="cb4-12"></span>
<span id="cb4-13">grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>product(np.linspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>plot_xlim, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>), np.linspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>plot_ylim, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>))])</span>
<span id="cb4-14"></span>
<span id="cb4-15">is_orange <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: model.predict(x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb4-16"></span>
<span id="cb4-17">orange_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(is_orange, grid)])</span>
<span id="cb4-18">blue_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>filterfalse(is_orange, grid)])</span>
<span id="cb4-19"></span>
<span id="cb4-20">axes.clear()</span>
<span id="cb4-21">axes.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Linear Regression of 0/1 Response"</span>)</span>
<span id="cb4-22">plot_data(orange_data, blue_data)</span>
<span id="cb4-23">plot_grid(orange_grid, blue_grid)</span>
<span id="cb4-24"></span>
<span id="cb4-25">find_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> model.beta[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> model.beta[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> model.beta[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb4-26">axes.plot(plot_xlim, [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(find_y, plot_xlim)], color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, </span>
<span id="cb4-27">          scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-28"></span>
<span id="cb4-29"></span>
<span id="cb4-30">fig</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="nearest-neighbor-methods" class="level3">
<h3 class="anchored" data-anchor-id="nearest-neighbor-methods">2.3.2 Nearest-Neighbor Methods</h3>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D(x)%20=%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_%7Bx_i%20%5Cin%20N_k(x)%7D%20y_i"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?N_k(x)"> is the neighborhood of <img src="https://latex.codecogs.com/png.latex?x"> defined by the <img src="https://latex.codecogs.com/png.latex?k"> closest points <img src="https://latex.codecogs.com/png.latex?x_i"> in the training sample.</p>
<div id="cell-14" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> KNeighborsRegressor:</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, k):</span>
<span id="cb5-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k</span>
<span id="cb5-4"></span>
<span id="cb5-5">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y):</span>
<span id="cb5-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X</span>
<span id="cb5-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y</span>
<span id="cb5-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb5-9">    </span>
<span id="cb5-10">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb5-11">        X, y, k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._X, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._y, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._k</span>
<span id="cb5-12">        distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ((X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-13">      </span>
<span id="cb5-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.mean(y[distances.argpartition(k)[:k]])</span></code></pre></div></div>
</div>
<div id="cell-15" class="cell" data-scrolled="false" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_k_nearest_neighbors(k):</span>
<span id="cb6-2">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KNeighborsRegressor(k).fit(data_x, data_y)</span>
<span id="cb6-3">    is_orange <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: model.predict(x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb6-4">    orange_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(is_orange, grid)])</span>
<span id="cb6-5">    blue_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>filterfalse(is_orange, grid)])</span>
<span id="cb6-6"></span>
<span id="cb6-7">    axes.clear()</span>
<span id="cb6-8">    axes.set_title(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(k) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-Nearest Neighbor Classifier"</span>)</span>
<span id="cb6-9"></span>
<span id="cb6-10">    plot_data(orange_data, blue_data)</span>
<span id="cb6-11">    plot_grid(orange_grid, blue_grid)</span>
<span id="cb6-12"></span>
<span id="cb6-13">plot_k_nearest_neighbors(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb6-14">fig</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It appears that k-nearest-neighbor have a single parameter (<em>k</em>), however the effective number of parameters is N/k and is generally bigger than the p parameters in least-squares fits. <strong>Note:</strong> if the neighborhoods were nonoverlapping, there would be N/k neighborhoods and we would fit one parameter (a mean) in each neighborhood.</p>
<div id="cell-17" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">plot_k_nearest_neighbors(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span>
<span id="cb7-2"></span>
<span id="cb7-3">fig</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>2.4 Statistical Decision Theory</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.4-statistical-decision-theory.html</link>
  <description><![CDATA[ 





<section id="loss-function" class="level4">
<h4 class="anchored" data-anchor-id="loss-function">Loss function</h4>
<p>The most common is <em>squared error loss</em>: <img src="https://latex.codecogs.com/png.latex?L(Y,%20f(X))%20=%20(Y%20-%20f(X))%5E2"></p>
</section>
<section id="expected-prediction-error" class="level4">
<h4 class="anchored" data-anchor-id="expected-prediction-error">Expected prediction Error</h4>
<p>(2.9, 2.10) This leads us to a criterion for choosing <img src="https://latex.codecogs.com/png.latex?f">,</p>
<p><img src="https://latex.codecogs.com/png.latex?EPE(f)%20=%20E(Y%20-%20f(X))%5E2%20=%20%5Cint%7B%5By%20-%20f(x)%5D%5E2%7DPr(dx,%20dy)"></p>
<p>(2.11) By conditioning on X, we can write EPE as:</p>
<p><img src="https://latex.codecogs.com/png.latex?EPE(f)%20=%20E_X%20E_%7BY%7CX%7D%20(%5BY%20-%20f(X)%5D%5E2%7CX)%20"></p>
<p>Proof:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cint%7B%5By%20-%20f(x)%5D%5E2%7DPr(dx,%20dy)%20%5C%5C%0A=%20%5Cint%7B%5By%20-%20f(x)%5D%5E2%7Dp(x,%20y)dxdy%20%5C%5C%0A=%20%5Cint%7B%5By%20-%20f(x)%5D%5E2%7Dp(x)p(y%20%7C%20x)dxdy%20%5C%5C%0A=%20%5Cint_x%20%7B%20%5Cleft(%20%5Cint_y%20%7B%5By%20-%20f(x)%5D%5E2p(y%20%7C%20x)dy%7D%20%5Cright)p(x)dx%20%7D%20%5C%5C%0A=%20E_X%20E_%7BY%7CX%7D%20(%5BY%20-%20f(X)%5D%5E2%7CX)%0A%5Cend%7Bequation%7D%0A"></p>
<p>(2.12) It suffices to minimize EPE pointwise: <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20argmin_c%20E_%7BY%7CX%7D%20(%20%5BY%20-%20c%5D%5E2%20%7C%20X%20=%20x)"></p>
<p>(2.13) The solution is : <img src="https://latex.codecogs.com/png.latex?f(x)=E(Y%20%7C%20X%20=%20x)"></p>
<p>The <strong>nearest-neighbor</strong> methods attempt to directly implement this recipe using the training data. Since there is typically at most one observation at any point x, we settle for: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)%20=%20%5Ctext%7BAve%7D(y_i%7Cx_i%20%5Cin%20N_k(x))"></p>
<p>For large training sample size N, the points in the neighborhood are likely to be close to x, and as k gets large the average will get more stable.</p>
<p>How does <strong>linear regression fit</strong> into this framework ?</p>
<p><img src="https://latex.codecogs.com/png.latex?f(x)%20%5Capprox%20x%5ET%5Cbeta"></p>
<p>Plugging this into EPE (2.9) and differentiating we can solve for β:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta=%20%5BE(XX%5ET)%5D%5E%7B-1%7DE(XY)"></p>
<p>Proof:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cint%7B%5By%20-%20x%5ET%5Cbeta%5D%5E2%5Ctext%7B%20Pr%7D(dx,%20dy)%7D%20%5C%5C%0A=%20%5Cint%7B%5By%5E2%20-%202yx%5ET+(x%5ET%5Cbeta)%5E2%5D%5Ctext%7B%20Pr%7D(dx,%20dy)%7D%20%5C%5C%0A%5Cend%7Bequation%7D%0A"></p>
<p>Differentiating w.r.t <img src="https://latex.codecogs.com/png.latex?%5Cbeta">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A2%5Cint%7Bxx%5ET%5Cbeta%5Ctext%7B%20Pr%7D(dx,%20dy)%7D%20-%20%202%5Cint%7Bxy%5Ctext%7B%20Pr%7D(dx,%20dy)%7D%20=%200%20%5C%5C%0A2%20%5Ctimes%20(E(XX%5ET%5Cbeta)%20-%20E(XY))%20=%200%20%5C%5C%0AE(XX%5ET)%5Cbeta%20=%20E(XY)%20%5C%5C%0A%5Cbeta%20=%20%5BE(XX%5ET)%5D%5E%7B-1%7DE(XY)%0A%5Cend%7Bequation%7D%0A"></p>
<p>The least squares solution <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"> amounts to replacing the expectation in <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%20%5BE(XX%5ET)%5D%5E%7B-1%7DE(XY)"> by averages over the training data. <strong>Note:</strong> Each expectation produces N times more than average, however the constant (i.e 1/N) in two expectations cancel out each other.</p>
</section>
<section id="bayes-classifier" class="level4">
<h4 class="anchored" data-anchor-id="bayes-classifier">Bayes Classifier</h4>
<div id="cell-3" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-6"></span>
<span id="cb1-7">sample_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_data(means, size):</span>
<span id="cb1-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.array([</span>
<span id="cb1-11">        np.random.multivariate_normal(random.choice(means), np.eye(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb1-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(size)</span>
<span id="cb1-13">    ])</span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_data(orange_data, blue_data): </span>
<span id="cb1-16">    axes.plot(orange_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], orange_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>)</span>
<span id="cb1-17">    axes.plot(blue_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], blue_data[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>)</span>
<span id="cb1-18"></span>
<span id="cb1-19">blue_means <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.multivariate_normal([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], np.identity(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb1-20">orange_means <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.multivariate_normal([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], np.identity(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb1-21"></span>
<span id="cb1-22">blue_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_data(blue_means, sample_size)</span>
<span id="cb1-23">orange_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_data(orange_means, sample_size)</span>
<span id="cb1-24"></span>
<span id="cb1-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plotting</span></span>
<span id="cb1-26">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.figure(figsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb1-27">axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-28">plot_data(orange_data, blue_data)</span>
<span id="cb1-29"></span>
<span id="cb1-30">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.4-statistical-decision-theory_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-4" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> filterfalse, product</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> multivariate_normal</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_grid(orange_grid, blue_grid):</span>
<span id="cb2-5">    axes.plot(orange_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], orange_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>, zorder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>,</span>
<span id="cb2-6">          color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>, alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-7"></span>
<span id="cb2-8">    axes.plot(blue_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], blue_grid[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>, zorder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>,</span>
<span id="cb2-9">          color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-10"></span>
<span id="cb2-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> pdf(x, means):</span>
<span id="cb2-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.mean([multivariate_normal.pdf(x, mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m, cov <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.eye(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>) </span>
<span id="cb2-13">                    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> means], axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-14"></span>
<span id="cb2-15">plot_xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axes.get_xlim()</span>
<span id="cb2-16">plot_ylim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axes.get_ylim()</span>
<span id="cb2-17"></span>
<span id="cb2-18">grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>product(np.linspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>plot_xlim, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>), np.linspace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>plot_ylim, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>))])</span>
<span id="cb2-19"></span>
<span id="cb2-20">orange_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pdf(grid, orange_means)</span>
<span id="cb2-21">blue_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pdf(grid, blue_means)</span>
<span id="cb2-22"></span>
<span id="cb2-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plotting</span></span>
<span id="cb2-24">axes.clear()</span>
<span id="cb2-25">axes.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bayes Optimal Classifier"</span>)</span>
<span id="cb2-26"></span>
<span id="cb2-27">plot_data(orange_data, blue_data)</span>
<span id="cb2-28">plot_grid(grid[orange_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> blue_pdf], grid[orange_pdf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> blue_pdf])</span>
<span id="cb2-29"></span>
<span id="cb2-30">fig</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.4-statistical-decision-theory_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.4-statistical-decision-theory.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>2.5 Local Methods in High Dimensions</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.5-local-methods-in-high-dimensions.html</link>
  <description><![CDATA[ 





<p><strong>Mean squared error for estimating f(0):</strong></p>
<p><em>Assume that the relationship between X and Y is: <img src="https://latex.codecogs.com/png.latex?Y%20=%20f(X)"></em></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BMSE%7D(x_0)%20&amp;%20=%20E_%5Ctau%5Bf(x_0)%20-%20%5Chat%7By_0%7D%5D%5E2%5C%5C%0A&amp;%20=%20E_%5Ctau%5B(f(x_0)%20-%20E_%5Ctau(%5Chat%7By_0%7D))%20+%20(E_%5Ctau(%5Chat%7By_0%7D)%20-%20%5Chat%7By_0%7D)%5D%5E2%5C%5C%0A&amp;%20=%20E_%5Ctau%5B(E_%5Ctau(%5Chat%7By_0%7D)%20-%20%5Chat%7By_0%7D)%5E2%20%20+%202(f(x_0)%20-%20E_%5Ctau(%5Chat%7By_0%7D))(E_%5Ctau(%5Chat%7By_0%7D)%20-%20%5Chat%7By_0%7D)+%20(f(x_0)%20-%20E_%5Ctau(%5Chat%7By_0%7D))%5E2%5D%5C%5C%0A&amp;%20=%20E_%5Ctau%5B(E_%5Ctau(%5Chat%7By_0%7D)%20-%20%5Chat%7By_0%7D)%5E2%5D%20+%20E_%5Ctau%5B(E_%5Ctau(%5Chat%7By_0%7D)%20-%20f(x_0))%5E2%5D%5C%5C%0A&amp;%20=%20E_%5Ctau%5B%5Chat%7By_0%7D%20-%20E_%5Ctau(%5Chat%7By_0%7D)%5D%5E2%20+%20%5BE_%5Ctau(%5Chat%7By_0%7D)%20-%20f(x_0)%5D%5E2%5C%5C%0A&amp;%20=%20Var_%5Ctau(%5Chat%7By_0%7D)%20+%20Bias%5E2(%5Chat%7By_0%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>We have broken down the MSE into two components: variance and squared bias. Such decomposition is always possible and is known as <em>the bias-variance decomposition</em>.</p>
<p><em>(2.26) Suppose that the relationship between Y and X is linear with some noise:</em></p>
<p><img src="https://latex.codecogs.com/png.latex?Y%20=%20X%5ET%5Cbeta%20+%20%5Cvarepsilon%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%20%5Csim%20N(0,%20%5Csigma%5E2)"> and we fit the model by least squares to the training data. For a test point <img src="https://latex.codecogs.com/png.latex?x_0"> we have <img src="https://latex.codecogs.com/png.latex?%5Chat%7By_0%7D=x_0%5ET%5Chat%7B%5Cbeta%7D"> which can be written as <img src="https://latex.codecogs.com/png.latex?%5Chat%7By_0%7D%20=%20x_0%5ET%5Cbeta%20+%20%5Csum_%7Bi=1%7D%5EN%20%7Bl_i(x_0)%5Cvarepsilon_i%7D"> where <img src="https://latex.codecogs.com/png.latex?l_i(x_0)"> is the <img src="https://latex.codecogs.com/png.latex?i">th element of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7Dx_0"></p>
<p>Proof:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%20%5C%5C%0A%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7BX%7D%5Cbeta%20+%20%5Cvarepsilon)%20%5C%5C%0A%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7BX%7D%5Cbeta%20+%20%5Cvarepsilon)%20%5C%5C%0A%5Chat%7B%5Cbeta%7D=%5Cbeta%20+%20(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cvarepsilon%20%5C%5C%0A%5Cend%7Bequation%7D%0A"></p>
<p>and by plugging <img src="https://latex.codecogs.com/png.latex?%5Chat%7BB%7D"> into the linear model:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7By_0%7D%20=%20x_0%5ET(%5Cbeta+(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cvarepsilon)%20%5C%5C%0A%5Chat%7By_0%7D%20=%20x_0%5ET%5Cbeta+x_0%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cvarepsilon%0A%5Cend%7Bequation%7D%0A"></p>
<p>we can get <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7Dx_0"> from <img src="https://latex.codecogs.com/png.latex?(x_0%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET)%5ET"> by using two matrix properties:</p>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BAB%7D)%5ET=%5Cmathbf%7BB%7D%5ET%5Cmathbf%7BA%7D%5ET"></p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%5E%7B-1%7D)%5ET%20=%20(%5Cmathbf%7BA%7D%5ET)%5E%7B-1%7D"></p></li>
</ol>
<p>Under this model the least square estimates are unbiased, so the expected prediction error will be: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BEPE%7D(x_0)%20&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau(y_0-%5Chat%7By_0%7D)%5E2%5C%5C%0A&amp;%20=%20%5Ctext%7BVar%7D(y_0%7Cx_0)%20+%20Var_%5Ctau(%5Chat%7By_0%7D)%20+%20%5Ctext%7BBias%7D%5E2(%5Chat%7By_0%7D)%5C%5C%0A&amp;%20=%20%5Csigma%5E2%20+%20E_%7B%5Ctau%7Dx_0%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7Dx_0%5Csigma%5E2%20+%200%5E2%0A%5Cend%7Balign%7D%0A"></p>
<p><em>Proof</em>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BEPE%7D(x_0)%20&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau(y_0-%5Chat%7By_0%7D)%5E2%5C%5C%0A&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau((y_0%20-%20f(x_0))%20+%20(f(x_0)%20-%20%5Chat%7By_0%7D))%5E2%5C%5C%0A&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau(y_0%20-%20f(x_0))%5E2%20+%202E_%7By_0%7Cx_0%7DE_%5Ctau(y_0%20-%20f(x_0))(f(x_0)%20-%20%5Chat%7By_0%7D)%20+%20E_%7By_0%7Cx_0%7DE_%5Ctau(f(x_0)%20-%20%5Chat%7By_0%7D)%5E2%5C%5C%0A&amp;%20=%20U_1%20+%20U_2%20+%20U_3%0A%5Cend%7Balign%7D%0A"></p>
<p>There are three components <img src="https://latex.codecogs.com/png.latex?U_1">, <img src="https://latex.codecogs.com/png.latex?U_2">, <img src="https://latex.codecogs.com/png.latex?U_3"> and we’re going to expand them as well.</p>
<p><img src="https://latex.codecogs.com/png.latex?U_1%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau(y_0%20-%20f(x_0))%5E2%20=%20E_%7By_0%7Cx_0%7D(y_0-f(x_0))%5E2%20=%20%5Csigma%5E2"></p>
<p><em>Note</em>: $f(x_0) = E_{y_0|x_0}(y_0) $</p>
<p><img src="https://latex.codecogs.com/png.latex?U_2%20=%202E_%7By_0%7Cx_0%7DE_%5Ctau(y_0%20-%20f(x_0))(f(x_0)%20-%20%5Chat%7By_0%7D)%20=%200"></p>
<p><em>Note</em>: <img src="https://latex.codecogs.com/png.latex?E_%7By_0%7Cx_0%7D(y_0-f(x_0))%20=%200"></p>
<p><em><img src="https://latex.codecogs.com/png.latex?U_3"></em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AU_3%20&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau(f(x_0)%20-%20%5Chat%7By_0%7D)%5E2%5C%5C%0A&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau((%5Chat%7By_0%7D%20-%20E_%5Ctau(%5Chat%7By_0%7D))%20+%20(E_%5Ctau(%5Chat%7By_0%7D)%20-%20f(x_0)))%5E2%5C%5C%0A&amp;%20=%20E_%7By_0%7Cx_0%7DE_%5Ctau(%5Chat%7By_0%7D%20-%20E_%5Ctau(%5Chat%7By_0%7D))%5E2%20+%202E_%7By_0%7Cx_0%7DE_%5Ctau%5B(%5Chat%7By_0%7D%20-%20E_%5Ctau(%5Chat%7By_0%7D))(E_%5Ctau(%5Chat%7By_0%7D)%20-%20f(x_0))%5D%20+%20E_%7By_0%7Cx_0%7DE_%5Ctau(E_%5Ctau(%5Chat%7By_0%7D)%20-%20f(x_0))%5E2%5C%5C%0A&amp;%20=%20E_%5Ctau(%5Chat%7By_0%7D%20-%20E_%5Ctau(%5Chat%7By_0%7D))%5E2%20+%20(E_%5Ctau(%5Chat%7By_0%7D)%20-%20f(x_0))%5E2%5C%5C%0A&amp;%20=%20%5Ctext%7BVar%7D_%5Ctau(%5Chat%7By_0%7D)%20+%20%5Ctext%7BBias%7D_%5Ctau%5E2(%5Chat%7By_0%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>Finally if we sum all <img src="https://latex.codecogs.com/png.latex?U_i"> we get: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BEPE%7D(x_0)%20=%20U_1+U_2+U_3%20=%20%5Csigma%5E2%20+%200%20+%20(%5Ctext%7BVar%7D_%5Ctau(%5Chat%7By_0%7D)%20+%20%5Ctext%7BBias%7D_%5Ctau%5E2(%5Chat%7By_0%7D))"></p>
<p><img src="https://latex.codecogs.com/png.latex?E_%5Ctau(%5Chat%7By_0%7D)%20=%20E_%5Ctau(x_0%5ET%5Cbeta%20+%20%5Csum_%7Bi=1%7D%5EN%20%7Bl_i(x_0)%5Cvarepsilon_i%7D)=x_0%5ET%5Cbeta%20+%20E(%5Csum_%7Bi=1%7D%5EN%20%7Bl_i(x_0)%5Cvarepsilon_i%7D)%20=%20x_0%5ET%5Cbeta%20+%200"> thus <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BBias%7D_%5Ctau%7B%5Chat%7By_0%7D%7D%20=%200"></p>
<p>(2.27) and we can find variance: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BVar%7D_%5Ctau(%5Chat%7By_0%7D)%20&amp;%20=%20E_%5Ctau(%5Chat%7By_0%7D%20-%20E_%5Ctau(%5Chat%7By_0%7D))%20%5E%202%5C%5C%0A&amp;%20=%20E_%5Ctau(x_0%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cvarepsilon)%5C%5C%0A&amp;%20=%20E_%5Ctau(x_0%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cvarepsilon%5Cvarepsilon%5ET%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7Dx_0)%0A%5Cend%7Balign%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cvarepsilon%5ET=%5Csigma%5E2%5Cmathbf%7BI%7D_n">, so we can simplify further: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BVar%7D_%5Ctau(%5Chat%7By_0%7D)%20=%20%5Csigma%5E2x_0%5E%7BT%7DE_%5Ctau%5B(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D)%5Dx_0"></p>
<p>(2.28) if N is large and <img src="https://latex.codecogs.com/png.latex?%5Ctau"> were selected at random, and assuming E(X) = 0, then <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D">-&gt;<img src="https://latex.codecogs.com/png.latex?NCov(%5Cmathbf%7BX%7D)">.</p>
<p><em>Proof</em>: By definition of covariance <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BCov%7D(X)%20=%20E%5B(X-E(X))(X-E(X))%5ET%5D%20=%20E(XX%5ET)%20=%20%5Cfrac%7B%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%7D%7BN%7D"></p>
<p>and we can derive that: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AE_%7Bx_0%7D%5Ctext%7BEPE%7D(x_0)%20&amp;%20=%20E_%7Bx_0%7Dx_0%5E%7BT%7D%5Ctext%7BCov%7D%5E%7B-1%7D(X)x_0%5Csigma%5E2/N+%5Csigma%5E2%5C%5C%0A&amp;%20=%20%5Ctext%7Btrace%7D%5B%5Ctext%7BCov%7D%5E%7B-1%7D(X)%5Ctext%7BCov%7D(x_0)%5D%5Csigma%5E2/N+%5Csigma%5E2%5C%5C%0A&amp;%20=%20%5Csigma%5E2(p/N)+%5Csigma%5E2%0A%5Cend%7Balign%7D%0A"></p>
<p><strong>FIGURE 2.9</strong></p>
<div id="cell-3" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-5"></span>
<span id="cb1-6">simulations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb1-7">sample_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span></span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> least_square_error(x_0, y_0, train_x, train_y):</span>
<span id="cb1-10">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.c_[np.ones((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_x), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), train_x]</span>
<span id="cb1-11">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.lstsq(X, train_y, rcond <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (np.dot(np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_0]), beta) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_0) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1-nearest neighbor error</span></span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> nn_error(x_0, y_0, train_x, train_y):</span>
<span id="cb1-16">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> train_x).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (y_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_y[X.argmin()]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-18"></span>
<span id="cb1-19">cubic_epe_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-20">linear_epe_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-21"></span>
<span id="cb1-22"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> p <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>):</span>
<span id="cb1-23">    least_square_epe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-24">    nn_epe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(simulations):</span>
<span id="cb1-26">        error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.standard_normal(sample_size)</span>
<span id="cb1-27">        train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.uniform(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(sample_size, p))</span>
<span id="cb1-28">        train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [train_x[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> error, </span>
<span id="cb1-29">                   <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (train_x[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> error]</span>
<span id="cb1-30">        x_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(p)</span>
<span id="cb1-31">        y_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.standard_normal(),</span>
<span id="cb1-32">               <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.standard_normal()]</span>
<span id="cb1-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb1-34">            least_square_epe[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> least_square_error(x_0, y_0[i], </span>
<span id="cb1-35">                                                      train_x, train_y[i])</span>
<span id="cb1-36">            nn_epe[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> nn_error(x_0, y_0[i], </span>
<span id="cb1-37">                                  train_x, train_y[i])</span>
<span id="cb1-38">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb1-39">        least_square_epe[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> simulations</span>
<span id="cb1-40">        nn_epe[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> simulations</span>
<span id="cb1-41">    linear_epe_ratio.append(nn_epe[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> least_square_epe[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-42">    cubic_epe_ratio.append(nn_epe[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> least_square_epe[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb1-43"></span>
<span id="cb1-44"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot</span></span>
<span id="cb1-45">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.figure(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, figsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb1-46">axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-47"></span>
<span id="cb1-48">axes.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Expected Prediction Error of 1NN vs. OLS"</span>)</span>
<span id="cb1-49"></span>
<span id="cb1-50">axes.plot(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>), linear_epe_ratio, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-o'</span>,</span>
<span id="cb1-51">          color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Linear"</span>)</span>
<span id="cb1-52"></span>
<span id="cb1-53">axes.plot(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>), cubic_epe_ratio, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-o'</span>,</span>
<span id="cb1-54">          color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Cubic"</span>)</span>
<span id="cb1-55"></span>
<span id="cb1-56">axes.legend()</span>
<span id="cb1-57">axes.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dimension"</span>)</span>
<span id="cb1-58">axes.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EPE Ration"</span>)</span>
<span id="cb1-59">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.5-local-methods-in-high-dimensions_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.5-local-methods-in-high-dimensions.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>2.6 Statistical Models, Supervised Learning and Function Approximation</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.6-statistical-models-supervised-learning-and-function-approximation.html</link>
  <description><![CDATA[ 





<p>Our goal is to find a useful approximation <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf(x)%7D"> to the function <img src="https://latex.codecogs.com/png.latex?f(x)"> that underlies the predictive relationship between the inputs and outputs.</p>
<ul>
<li><p>We saw that squared error loss lead us to the regression function <img src="https://latex.codecogs.com/png.latex?f(x)=E(Y%7CX%20=%20x)"> for a qualitive response.</p></li>
<li><p>The nearest-neighbor methods estimates directly the conditional expections, but may result in large errors for the high dimension input spaces. (<em>The curse of dimensionality</em>)</p></li>
</ul>
<p>We anticipate using other classes of models for f(x) to overcome the dimensionality problems.</p>
<section id="a-statistical-model-for-the-joint-distribution-prx-y" class="level3">
<h3 class="anchored" data-anchor-id="a-statistical-model-for-the-joint-distribution-prx-y">2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y)</h3>
<p>(2.29) Suppose that our data arose from a statistical model: <img src="https://latex.codecogs.com/png.latex?Y%20=%20f(X)%20+%20%5Cvarepsilon"></p>
<p>where the random error <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> has <img src="https://latex.codecogs.com/png.latex?E(%5Cvarepsilon)%20=%200"> and independent of X.</p>
<p>The additive error model is a useful approximation to the truth. Generally there will be unmeasured variables that also contribute to the output, including measurement error. The additive model assumes that can capture all departures via the error <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon">.</p>
<p>The assumption in (2.29) that the errors are independent and identically distributed is not strictly necessary. For example, simple modifications can be made to avoid the independence assumption, e.g <img src="https://latex.codecogs.com/png.latex?Var(Y%7C%20X%20=%20x)%20=%20%5Csigma(x)">, and now both the mean and variance depend on X.</p>
</section>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">2.6.2 Supervised Learning</h3>
<p>Suppose that errors are additive and that the model <img src="https://latex.codecogs.com/png.latex?Y%20=%20f(X)%20+%20%5Cvarepsilon">. Supervised learning attempts to learn <img src="https://latex.codecogs.com/png.latex?f"> through a teacher and learns by examples (i.e by a training set of observations (<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BT%7D%20=%20(x_i,%20y_i),%20i%20=%201...N">)</p>
</section>
<section id="function-approximation" class="level3">
<h3 class="anchored" data-anchor-id="function-approximation">2.6.3 Function Approximation</h3>
<p>The goal is to obtain a useful approximation to f(x) for all x in some region of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ep">, given the representations in <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BT%7D">.</p>
<p>Many of the approximations have associated a set of parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> that can be modified to suit the data at hand, e.g the linear model <img src="https://latex.codecogs.com/png.latex?f(x)=x%5ET%5Cbeta"> has <img src="https://latex.codecogs.com/png.latex?%5Ctheta=%5Cbeta">. Another class of useful approximators can be expressed as <em>linear basis expansions</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?f_%5Ctheta(x)%20=%20%5Csum_%7Bk=1%7D%5E%7BK%7Dh_k(x)%5Ctheta_k"></p>
<p>where the <img src="https://latex.codecogs.com/png.latex?h_k"> are a suitable set of functions or transformations of the input vector x. We also encounter <em>nonlinear expansions</em>, such as the sigmoid transformation:</p>
<p><img src="https://latex.codecogs.com/png.latex?h_k(x)%20=%20%5Cfrac%7B1%7D%7B1+exp(-x%5ET%5Cbeta_k)%7D"></p>
<p>We can use least squares to estimate the parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> in <img src="https://latex.codecogs.com/png.latex?f_%5Ctheta">, by minimizing the residual sum-of-squares:</p>
<p><img src="https://latex.codecogs.com/png.latex?RSS(%5Ctheta)=%5Csum_%7Bi=1%7D%5EN(y_i%20-%20f_%5Ctheta(x_i))%20%5E%202"></p>
<p>While least squares is very convenient, it is not only criterion used and in some cases would not make sense. A more general principle for estimation is <em>maximum likelihood estimation</em>. Suppose we have a random sample <img src="https://latex.codecogs.com/png.latex?y_i">, i = 1…N from a density <img src="https://latex.codecogs.com/png.latex?Pr_%5Ctheta(y)"> indexed by some parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta."> The log-probability of the observed sample is: <img src="https://latex.codecogs.com/png.latex?L(%5Ctheta)=%5Csum_%7Bi=1%7D%5EN%20logPr_%5Ctheta(y_i)"></p>
<p>Least squares for the additive error model <img src="https://latex.codecogs.com/png.latex?Y%20=%20f_%5Ctheta(X)+%5Cvarepsilon">, with <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%20%5Csim%20N(0,%20%5Csigma%5E2)"> is equivalent to maximum likelihood using the conditional likelihood</p>
<p><img src="https://latex.codecogs.com/png.latex?Pr(Y%7CX,%20%5Ctheta)=N(f_%5Ctheta(X),%20%5Csigma%5E2)"></p>
<p>The log-likelihood of the data is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AL(%5Ctheta)%20&amp;=%20%5Csum_%7Bi=1%7D%5EN%20log%20%5Cleft(%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2%7D%7De%5E%7B-%5Cfrac%7B(y_i-f_%5Ctheta(x_i))%5E2%7D%7B2%5Csigma%5E2%7D%7D%5Cright)%5C%5C%0A&amp;=%20%5Csum_%7Bi=1%7D%5EN%20log%20%5Cleft(%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2%7D%7D%5Cright)%20-%20%5Csum_%7Bi=1%7D%5EN(%5Cfrac%7B(y_i-f_%5Ctheta(x_i))%5E2%7D%7B2%5Csigma%5E2%7D)%5C%5C%0A&amp;=%20%5Csum_%7Bi=1%7D%5EN%20log%20%5Cleft((2%5Cpi%5Csigma%5E2)%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D%5Cright)%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi=1%7D%5EN(y_i-f_%5Ctheta(x_i))%5E2%5C%5C%0A&amp;=%20%5Csum_%7Bi=1%7D%5EN%20%5Cleft(-%5Cfrac%7B1%7D%7B2%7Dlog(2%5Cpi)-log(%5Csigma)%5Cright)%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi=1%7D%5EN(y_i-f_%5Ctheta(x_i))%5E2%5C%5C%0A&amp;=%20%5Cfrac%7BN%7D%7B2%7Dlog(2%5Cpi)-Nlog(%5Csigma)%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi=1%7D%5EN(y_i-f_%5Ctheta(x_i))%5E2%0A%5Cend%7Balign%7D%0A"></p>
<p>and the only term involving <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the last.</p>
<p>A more interesting example is the multinomial likelihood for the regression function Pr(G|X) for a qualitative output G. Suppose we have a model <img src="https://latex.codecogs.com/png.latex?Pr(G%20=%20%5Cmathcal%7BG%7D_k%7C%20X%20=%20x)%20=%20p_%7Bk,%20%5Ctheta%7D(x),%20k%20=%201...K"> indexed by <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Then the log-likelihood (a.k.a the cross-entropy) is :</p>
<p><img src="https://latex.codecogs.com/png.latex?L(%5Ctheta)=%5Csum_%7Bi=1%7D%5EN%20log%20(p_%7Bg_i,%20%5Ctheta%7D(x_i))"></p>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.6-statistical-models-supervised-learning-and-function-approximation.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>2.7 Structured Regression Models</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.7-structured-regression-models.html</link>
  <description><![CDATA[ 





<p>We have seen that although kNN and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more efficient use of the data. This section introduces classes of such approaches.</p>
<section id="difficulty-of-the-problem" class="level3">
<h3 class="anchored" data-anchor-id="difficulty-of-the-problem">2.7.1 Difficulty of the Problem</h3>
<p>(2.37) RSS criterian for an arbitrary function <img src="https://latex.codecogs.com/png.latex?f">:</p>
<p><img src="https://latex.codecogs.com/png.latex?RSS(f)%20=%20%5Csum_%7Bi=1%7D%5EN(y_i-f(x_i))%5E2"></p>
<p>Minimizing (2.37) leads to infinitely many solutions: any function <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D"> passing through the training points is a solution. It might be a poor predictor at test points different from the training points. If there are multiple observations at each point <img src="https://latex.codecogs.com/png.latex?x_i">, i.e <img src="https://latex.codecogs.com/png.latex?y_%7Bil%7D,%20l%20=%201%20...%20N">, the risk is limited.</p>
<p>In order to obtain useful results for finite N, we must restrict the eligible solutions to a smaller set of functions.</p>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.7-structured-regression-models.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>2.8 Classes of Restricted Estimators</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.8-classes-of-restricted-estimators.html</link>
  <description><![CDATA[ 





<p>The variety of nonparametric regression techniques or learning methods fall into a number of different classes depending on the nature of the restrictions imposed. Each of the classes has associated with it one or more parameters, sometimes called <em>smoothing</em> parameters, that control the effective size of the local neighborhood.</p>
<section id="roughness-penalty-and-bayesian-methods" class="level3">
<h3 class="anchored" data-anchor-id="roughness-penalty-and-bayesian-methods">2.8.1 Roughness Penalty and Bayesian Methods</h3>
<p>Here the class of functions is controlled by explicitly penalizing RSS(f) with a roughness penalty</p>
<p><img src="https://latex.codecogs.com/png.latex?PRSS(f;%20%5Clambda)%20=%20RSS(f)%20+%20%5Clambda%20J(f)"></p>
<p>The user-selected <img src="https://latex.codecogs.com/png.latex?J(f)"> will be large for functions <img src="https://latex.codecogs.com/png.latex?f"> that vary too rapidly over small regions of input space. For example, the popular <em>cubic smoothing spline</em> for one-dimensional inputs is the solution to the PRSS:</p>
<p><img src="https://latex.codecogs.com/png.latex?PRSS(f;%20%5Clambda)%20=%20%5Csum_%7Bi=1%7D%5EN(y_i%20-%20f(x_i))%5E2%20+%20%5Clambda%5Cint%5Bf%5E%7B''%7D(x)%5D%5E2dx"></p>
<p>The amount of penalty is dictated by <img src="https://latex.codecogs.com/png.latex?%5Clambda%3E=0">: - For <img src="https://latex.codecogs.com/png.latex?%5Clambda=0">, no penalty imposed, and any interpolating function will do - While for <img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20%5Cinfty"> only linear functions are permitted</p>
<p>Penalty function, or <em>regularization</em> methods, express our prior belief that the type of functions we seek exhibit a certain type of smooth behavior, and indeed can usually be cast in a Bayesian framework: - The penalty J corresponds to a log-prior - and <img src="https://latex.codecogs.com/png.latex?PRSS(f;%20%5Clambda)"> the log-posterior distribution; - and minimizing <img src="https://latex.codecogs.com/png.latex?PRSS(f;%20%5Clambda)"> amounts to finding the posterior mode.</p>
</section>
<section id="kernel-methods-and-local-regression" class="level3">
<h3 class="anchored" data-anchor-id="kernel-methods-and-local-regression">2.8.2 Kernel Methods and Local Regression</h3>
<p>These methods can be thought of as explicitly providing estimates of the regression function or conditional expectation by specifying the nature of the local neighborhood, and of the class of regular functions fitted locally.</p>
<p>The local neighborhood is specified by a <em>kernel function</em> <img src="https://latex.codecogs.com/png.latex?K_%5Clambda(x_0,%20x)"> which assigns weights to points x in a region around <img src="https://latex.codecogs.com/png.latex?x_0">, e.g the <em>Gaussian kernel</em> based on the Gaussian density function:</p>
<p><img src="https://latex.codecogs.com/png.latex?K_%5Clambda(x_0,%20x)%20=%20%5Cfrac%7B1%7D%7B%5Clambda%7Dexp%5Cleft%5B-%5Cfrac%7B%7C%7Cx-x_0%7C%7C%5E2%7D%7B2%5Clambda%7D%5Cright%5D"></p>
<p>and assigns weights to points that die exponentially with their squared euclidean distance from <img src="https://latex.codecogs.com/png.latex?x_0"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> corresponds to the variance of the Gaussian density and controls the width of the neighborhood.</p>
<p>(2.40) The simplest form of kernel estimate is the Nadaraya-Watson weighted average:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x_0)=%5Cfrac%7B%5Csum_%7Bi=1%7D%5EN%20K_%5Clambda(x_0,%20x_i)y_i%7D%7B%5Csum_%7Bi=1%7D%5EN%20K_%5Clambda(x_0,%20x_i)%7D"></p>
<p>(2.41) In general we can define a local regression estimate of <img src="https://latex.codecogs.com/png.latex?f(x_0)"> as <img src="https://latex.codecogs.com/png.latex?f_%5Chat%7B%5Ctheta%7D(x_0)">, where <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D"> minimizes:</p>
<p><img src="https://latex.codecogs.com/png.latex?RSS(f_%5Ctheta,%20x_0)%20=%20%5Csum_%7Bi=1%7D%5EN%20K_%5Clambda(x_0,%20x_i)(y_i%20-%20f_%5Ctheta(x_i))%5E2"></p>
<p>and <img src="https://latex.codecogs.com/png.latex?f_%5Ctheta"> is some parameterized function, e.g:</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?f_%5Ctheta(x)%20=%20%5Ctheta_0">, the constant function; this results in the Nadaraya-Watson estimate in (2.41)</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?f_%5Ctheta(x)%20=%20%5Ctheta_0+%5Ctheta_1x"> gives the local linear regression model.</p></li>
</ul>
<p>These methods needs to be modified in high dimensions, to avoid curse of dimensionality.</p>
</section>
<section id="basis-functions-and-dictionary-methods" class="level3">
<h3 class="anchored" data-anchor-id="basis-functions-and-dictionary-methods">2.8.3 Basis Functions and Dictionary Methods</h3>
<p>This class of methods includes the linear and polynomial expansions, but more importantly a wide variety of more flexible models. The models for <img src="https://latex.codecogs.com/png.latex?f"> is a <em>linear expansion of basis functions</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?f_%5Ctheta(x)%20=%20%5Csum_%7Bm=1%7D%5EM%20%5Ctheta_%7Bm%7Dh_m(x)"></p>
<p>the term linear here refers to the action of the parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p><em>TODO</em>: 1D polynomial splines of degrees K.</p>
<p><em>Radial basis functions</em> are symmetric p-dimensional kernels located at particular centroids:</p>
<p><img src="https://latex.codecogs.com/png.latex?f_%5Ctheta(x)%20=%20%5Csum_%7Bm=1%7D%5EM%7BK_%7B%5Clambda_m%7D(%5Cmu_m,%20x)%7D%5Ctheta_m"></p>
<p>e.g the Gaussian kernel <img src="https://latex.codecogs.com/png.latex?K_%5Clambda(%5Cmu,%20x)%20=%20e%5E%7B-%7C%7Cx-%5Cmu%7C%7C%5E2%7D/2%5Clambda"> is popular. Radial basis functions have centroids <img src="https://latex.codecogs.com/png.latex?%5Cmu_m"> and scales <img src="https://latex.codecogs.com/png.latex?%5Clambda_m"> that have to be determined. In general, we would like the data to dictate them.</p>
<p>A single-layer feed-forward neural networks model with linear output weights can be thought of as an adaptive basis function methods:</p>
<p><img src="https://latex.codecogs.com/png.latex?f_%5Ctheta(x)%20=%20%5Csum_%7Bm=1%7D%5EM%7B%5Cbeta_m%5Csigma(%5Calpha_m%5ET%7Bx%7D+b_m)%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma(x)%20=%201/(1+e%5E%7B-x%7D)"> is know as the <em>activation</em> function.</p>
<p>These adaptively chosen basis function methods are also know as <em>dictionary</em> methods, where one has available a infinite set or dictionary <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D"> of candidate basis function from which to choose.</p>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.8-classes-of-restricted-estimators.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>2.9 Model Selection and the Bias-Variance Tradeoff</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.9-model-selection-and-the-bias-variance-tradeoff.html</link>
  <description><![CDATA[ 





<p>All the models described have a <em>smoothing</em> or <em>complexity</em> parameter that has to be determined:</p>
<ul>
<li><p>the multiplier of the penalty term;</p></li>
<li><p>the width of the kernel</p></li>
<li><p>or the number of basis functions</p></li>
</ul>
<p>We cannont use RSS on the training data to determine these parameters, since we would always pick those that gave interpolating fits and have zero residuals.</p>
<p>The kNN regression fit <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf_k%7D(x_0)"> illustrates the competing forces that effect the predictive ability of such approximations. Suppose the data arise from a model <img src="https://latex.codecogs.com/png.latex?Y%20=%20f(X)%20+%20%5Cvarepsilon"> with <img src="https://latex.codecogs.com/png.latex?E(%5Cvarepsilon)=0"> and <img src="https://latex.codecogs.com/png.latex?Var(%5Cvarepsilon)%20=%20%5Csigma%5E2">. We assume that the values of <img src="https://latex.codecogs.com/png.latex?x_i"> in the sample are fixed. The EPE at <img src="https://latex.codecogs.com/png.latex?x_0">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AEPE_k(x_0)%20&amp;=%20E%5B(Y%20-%20%5Chat%7Bf_k%7D(x_0))%5E2%7CX=x_0%5D%5C%5C%0A&amp;=%5Csigma%5E2%20+%20%5BBias%5E2(%5Chat%7Bf_k%7D(x_0))%20+%20Var_%5Ctau(%5Chat%7Bf_k%7D(x_0))%5D%5C%5C%0A&amp;=%5Csigma%5E2%20+%20%5Cleft%5Bf(x_0)%20-%20%5Cfrac%7B1%7D%7Bk%7D%5Csum_%7Bl=1%7D%5Ek%7Bf(x_%7B(l)%7D)%7D%20%5Cright%5D%5E2%20+%20%5Cfrac%7B%5Csigma%5E2%7D%7Bk%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>The subscripts in parentheses (<img src="https://latex.codecogs.com/png.latex?l">) indicates the sequence of nearest neighbors to <img src="https://latex.codecogs.com/png.latex?x_0">. There are three terms in this expression:</p>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> is the <em>irreducible</em> error - is beyond our control, even if we know the true <img src="https://latex.codecogs.com/png.latex?f(x_0)">.</p></li>
<li><p>The bias term and the expected value of the estimate - <img src="https://latex.codecogs.com/png.latex?%5BE_%5Ctau(%5Chat%7Bf_k%7D(x_0))-f(x_0)%5D%5E2"> - where the expected averages the randomness in the training data. This term increases with <img src="https://latex.codecogs.com/png.latex?k"> if the function is smooth.</p></li>
<li><p>The variance term and it decreases as the inverse of k. The expected value of the variance is:</p></li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AVar_%5Ctau(%5Chat%7Bf_k%7D(x_0))%20&amp;=%20E_%5Ctau%5Cleft%5B%5Chat%7Bf_k%7D(x_0)%20-%20E_%5Ctau(%5Chat%7Bf_k%7D(x_0))%5Cright%5D%5E2%5C%5C%0A&amp;=%20E_%5Ctau%5Cleft%5B%5Cfrac%7B1%7D%7Bk%7D%5Csum_%7Bl=1%7D%5Ek%20(f(x_%7B(l)%7D)%20+%20%5Cvarepsilon_l)%20-%20%5Cfrac%7B1%7D%7Bk%7D%5Csum_%7Bl=1%7D%5Ek%7Bf(x_%7B(l)%7D)%7D%5Cright%5D%5E2%5C%5C%0A&amp;=%20E_%5Ctau%5Cleft%5B%5Cfrac%7B1%7D%7Bk%7D%5Csum_%7Bl=1%7D%5Ek%20%5Cvarepsilon_l%5Cright%5D%5E2%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bk%5E2%7DE_%5Ctau%5Cleft%5B%5Csum_%7Bl=1%7D%5Ek%20%5Cvarepsilon_l%5Cright%5D%5E2%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7Bk%5E2%7DE_%5Ctau%5Cleft%5B%5Csum_%7Bl=1%7D%5Ek%20%5Cvarepsilon_l%5E2%5Cright%5D%5C%5C%0A&amp;=%20%5Cfrac%7B%5Csigma%5E2%7D%7Bk%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>As the <em>model complexity</em> of our procedure is increased, the variance tends to increase and the squared bias tends to decrease. The opposite behavior occurs as the model complexity is decreased.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/2.9-model-selection-and-the-bias-variance-tradeoff.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>Ex. 2.8</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/exercise-solutions.html</link>
  <description><![CDATA[ 





<p>Compare the classification performance of linear regression and k– nearest neighbor classification on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from the book website www-stat.stanford.edu/ElemStatLearn.</p>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> filtered_data(path):</span>
<span id="cb1-6">    data_all <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.loadtxt(path)</span>
<span id="cb1-7">    mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.in1d(data_all[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb1-8">    data_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_all[mask, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: ]</span>
<span id="cb1-9">    data_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_all[mask, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> data_x, data_y</span>
<span id="cb1-11"></span>
<span id="cb1-12">train_x, train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> filtered_data(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../data/zipcode/zip.train'</span>)</span>
<span id="cb1-13">test_x, test_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> filtered_data(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../data/zipcode/zip.test'</span>)</span>
<span id="cb1-14">k_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>]</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> knn_error(k, x, y, data_x, data_y):</span>
<span id="cb1-17">    distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ((data_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> x)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (np.mean(data_y[distances.argpartition(k)[:k]]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> std(squared_errors):</span>
<span id="cb1-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""standard deviation of the given squared errors."""</span></span>
<span id="cb1-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.sqrt(np.mean(squared_errors))</span>
<span id="cb1-23"></span>
<span id="cb1-24"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> knn_stds():</span>
<span id="cb1-25">    train_stds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-26">    test_stds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> k_list:</span>
<span id="cb1-28">        train_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [knn_error(k, x, y, train_x, train_y)</span>
<span id="cb1-29">                        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (x, y) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(train_x, train_y)]</span>
<span id="cb1-30">        train_stds.append(std(train_errors))</span>
<span id="cb1-31">        test_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [knn_error(k, x, y, train_x, train_y)</span>
<span id="cb1-32">                        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (x, y) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(test_x, test_y)]</span>
<span id="cb1-33">        test_stds.append(std(test_errors))</span>
<span id="cb1-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> train_stds, test_stds</span>
<span id="cb1-35"></span>
<span id="cb1-36"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> least_square_stds():</span>
<span id="cb1-37">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.c_[np.ones((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_x), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), train_x]</span>
<span id="cb1-38">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.lstsq(X, train_y, rcond <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-39">    error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x, y: (np.dot(np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x]), beta) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-40"></span>
<span id="cb1-41">    train_stds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-42">    test_stds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-43"></span>
<span id="cb1-44">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> k_list:</span>
<span id="cb1-45">        train_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [error(x, y) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (x, y) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(train_x, train_y)]</span>
<span id="cb1-46">        train_stds.append(std(train_errors))</span>
<span id="cb1-47">        test_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [error(x, y) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (x, y) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(test_x, test_y)]</span>
<span id="cb1-48">        test_stds.append(std(test_errors))</span>
<span id="cb1-49"></span>
<span id="cb1-50">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> train_stds, test_stds</span>
<span id="cb1-51"></span>
<span id="cb1-52">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.figure(figsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb1-53">axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-54"></span>
<span id="cb1-55"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># kNN plotting</span></span>
<span id="cb1-56">train_stds, test_stds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> knn_stds()</span>
<span id="cb1-57">axes.plot(k_list, train_stds, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, </span>
<span id="cb1-58">          color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C0'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'knn-train'</span>)</span>
<span id="cb1-59">axes.plot(k_list, test_stds, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, </span>
<span id="cb1-60">          color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C1'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'knn-test'</span>)</span>
<span id="cb1-61"></span>
<span id="cb1-62"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># least square plotting</span></span>
<span id="cb1-63">train_stds, test_stds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> least_square_stds()</span>
<span id="cb1-64">axes.plot(k_list, train_stds, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, </span>
<span id="cb1-65">          color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C2'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'least-square-train'</span>)</span>
<span id="cb1-66">axes.plot(k_list, test_stds, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, </span>
<span id="cb1-67">          color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C3'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'least-square-test'</span>)</span>
<span id="cb1-68"></span>
<span id="cb1-69">axes.legend()</span>
<span id="cb1-70">axes.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>)</span>
<span id="cb1-71">axes.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Error"</span>)</span>
<span id="cb1-72">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/exercise-solutions_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-02/exercise-solutions.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.1 Introduction</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.1-introduction.html</link>
  <description><![CDATA[ 





<p>A linear regression model assumes that the regression function E(Y|X) is linear in the inputs <img src="https://latex.codecogs.com/png.latex?X_1,...,X_p">. For prediction purposes they can sometimes outperform nonlinear models, e.g in situations with small numbers of training cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be applied to transformations of inputs and this considerably expands their scope.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.1-introduction.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.2 Linear Regression Models and Least Squares</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2-linear-regression-models-and-least-squares.html</link>
  <description><![CDATA[ 





<p>We have an input vector <img src="https://latex.codecogs.com/png.latex?X%5ET=(X_1,...,X_p)"> and want to predict a real-valued output <img src="https://latex.codecogs.com/png.latex?Y">.The linear regression model has the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?f(X)%20=%20B_0%20+%20%5Csum_%7Bj=1%7D%5Ep%20%7BX_j%5Cbeta_j%7D"></p>
<p>Typically we have a set of training data <img src="https://latex.codecogs.com/png.latex?(x_1,%20y_1)...(x_N,%20y_n)"> from which to estimate the parameters <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. The most popular estimation method is <em>least squares</em>, in which we pick <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> to minimize the residual sum of squares, (3.2): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0ARSS(%5Cbeta)&amp;=%5Csum_%7Bi=1%7D%5EN(y_i-f(x_i))%5C%5C%0A&amp;=%5Csum_%7Bi=1%7D%5EN(y_i-%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%7Bx_%7Bij%7D%5Cbeta_j%7D)%5E2%0A%5Cend%7Balign%7D%0A"></p>
<p>How do we minimize (3.2)? We can write the (3.2) using matrix, (3.3): <img src="https://latex.codecogs.com/png.latex?RSS(%5Cbeta)=(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)"></p>
<p>Differentiating with respect to <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> we obtain: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%7BRSS%7D%7D%7B%5Cpartial%5Cbeta%7D%20=%20-2%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)%0A%5Cend%7Balign%7D%0A"></p>
<p>Assuming that <strong>X</strong> has full column rank, and hence the second derivative is positive definite: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)=0"></p>
<p>and the unique solution is: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"></p>
<p>The predicted value at an input vector <img src="https://latex.codecogs.com/png.latex?x_0"> are given by <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x_0)=(1:x_0)%5ET%5Chat%7B%5Cbeta%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D=%5Cmathbf%7BX%7D%5Chat%7B%5Cbeta%7D=%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D"></p>
<p>The matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BH%7D=%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET"> is sometimes called the “hat” matrix.</p>
<p><strong>Geometrical representation of the least squares:</strong> We denote the column vectors of <strong>X</strong> by <img src="https://latex.codecogs.com/png.latex?x_0,%20x_1,%20...,%20x_p">. These vectors span a subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BR%7D%5EN">, also referred as the column space of <strong>X</strong>. We minimize <img src="https://latex.codecogs.com/png.latex?RSS(%5Cbeta)=%7C%7C%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta%7C%7C%5E2"> by choosing <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D"> so that the residual vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20-%20%5Chat%7B%5Cmathbf%7By%7D%7D"> is orthogonal to this subspace and the orthogonality is expressed by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)=0">. The hat matrix <strong>H</strong> is the projection matrix.</p>
<p><strong>Sampling properties of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D"></strong>: In order to pin down the sampling properties of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D">, we assume that the observations <img src="https://latex.codecogs.com/png.latex?y_i"> are uncorrelated and have constant variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">, and that the <img src="https://latex.codecogs.com/png.latex?x_i"> are fixed. The variance-covariance matrix is given by (3.8):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AVar(%5Chat%7B%5Cbeta%7D)%20&amp;=%20E%5Cleft%5B(%5Chat%7B%5Cbeta%7D-E(%5Chat%7B%5Cbeta%7D))(%5Chat%7B%5Cbeta%7D-E(%5Chat%7B%5Cbeta%7D)%5ET)%5Cright%5D%5C%5C%0A&amp;=%20E%5Cleft%5B(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7B%5Cvarepsilon%7D%5Cmathbf%7B%5Cvarepsilon%7D%5ET%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cright%5D%5C%5C%0A&amp;=%20%5Csigma%5E2(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>One estimates the variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> by: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Csigma%7D%5E2%20=%20%5Cfrac%7B1%7D%7BN-p-1%7D%20%5Csum_%7Bi=1%7D%5EN(y_i-%5Chat%7By_i%7D)%5E2%0A"></p>
<p>The N-p-1 rather than N in the denominator makes <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2"> an unbiased estimate of <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">: <img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Csigma%7D%5E2)=%5Csigma%5E2">.</p>
<p><em>Proof</em>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Chat%7B%5Cvarepsilon%7D%20&amp;=%20%5Cmathbf%7By%7D%20-%20%5Cmathbf%7B%5Chat%7By%7D%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BX%7D%5Cbeta%20+%20%5Cvarepsilon%20-%20%5Cmathbf%7BX%7D%5Chat%7B%5Cbeta%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BX%7D%5Cbeta%20+%20%5Cvarepsilon%20-%20%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET(%5Cmathbf%7BX%7D%5Cbeta%20+%20%5Cvarepsilon)%5C%5C%0A&amp;=%20%5Cvarepsilon%20-%20%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cvarepsilon%5C%5C%0A&amp;=%20(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET)%5Cvarepsilon%5C%5C%0A&amp;=%20(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5Cvarepsilon%0A%5Cend%7Balign%7D%0A"></p>
<p>and we would like to find <img src="https://latex.codecogs.com/png.latex?Var(%5Chat%7B%5Cvarepsilon%7D)=E(%5Chat%7B%5Cvarepsilon%7D%5ET%5Chat%7B%5Cvarepsilon%7D)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AE%5B%5Chat%7B%5Cvarepsilon%7D%5ET%5Chat%7B%5Cvarepsilon%7D%5D%0A&amp;=%20E%5Cleft%5B%5Cvarepsilon%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5Cvarepsilon%5Cright%5D%5C%5C%0A&amp;=%20E%5Cleft%5Btr(%5Cvarepsilon%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5Cvarepsilon)%5Cright%5D%5C%5C%0A&amp;=%20E%5Cleft%5Btr(%5Cvarepsilon%5Cvarepsilon%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D))%5Cright%5D%5C%5C%0A&amp;=%20%5Csigma%5E2E%5Cleft%5Btr((%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5ET(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D))%5Cright%5D%5C%5C%0A&amp;=%20%5Csigma%5E2E%5Cleft%5Btr(%5Cmathbf%7BI%7D_n%20-%20%5Cmathbf%7BH%7D)%5Cright%5D%5C%5C%0A&amp;=%20%5Csigma%5E2E%5Cleft%5Btr(%5Cmathbf%7BI%7D_n)%20-%20tr(%5Cmathbf%7BI%7D_%7Bp+1%7D)%5Cright%5D%5C%5C%0A&amp;=%20%5Csigma%5E2(n-p-1)%0A%5Cend%7Balign%7D%0A"></p>
<p>Note that, both <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BH%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI_n%7D-%5Cmathbf%7BH%7D"> are:</p>
<ul>
<li><p>Symmetry matrix, i.e <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BH%7D%5ET=%5Cmathbf%7BH%7D"></p></li>
<li><p>Idempotent matrix, i.e <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BH%7D%5E2=%5Cmathbf%7BH%7D"></p></li>
</ul>
<p><strong>Inferences about the parameters and the model:</strong> We now assume that deviations of Y around its expectations and Gaussian. Hence (3.9):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AY%20&amp;=E(Y%7CX_1,...,X_p)+%5Cvarepsilon%5C%5C%0A&amp;=%20%5Cbeta_0%20+%20%5Csum_%7Bj=1%7D%5EP%7BX_j%5Cbeta_j%7D%20+%20%5Cvarepsilon%0A%5Cend%7Balign%7D%0A"></p>
<p>where $N(0, ^2) $</p>
<p>Under (3.9), it is easy to show that (3.10): <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D%20%5Csim%20N(%5Cbeta,%20(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Csigma%5E2)%0A"></p>
<p>Also (3.11): <img src="https://latex.codecogs.com/png.latex?(N-p-1)%5Chat%7B%5Csigma%7D%5E2%20%5Csim%20%5Csigma%5E2%5Cchi_%7BN-p-1%7D%5E2"></p>
<p>a chi-squared distribution with N-p-1 degrees of freedom and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D"> are statistically independent.</p>
<p><strong>Hypothesis test:</strong> To test <img src="https://latex.codecogs.com/png.latex?H_0:%20%5Cbeta_j%20=%200"> we form the standardized coefficient or <em>Z-score</em>: <img src="https://latex.codecogs.com/png.latex?%0Az_j=%5Cfrac%7B%5Chat%7BB%7D_j%7D%7B%5Chat%7B%5Csigma%7D%5Csqrt%7Bv_j%7D%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?v_j"> is the jth diagonal element of <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D">. Under the null hypothesis <img src="https://latex.codecogs.com/png.latex?z_j"> is distributed as <img src="https://latex.codecogs.com/png.latex?t_%7BN-p-1%7D">, and hence a large value of <img src="https://latex.codecogs.com/png.latex?z_j"> will lead to rejection. If <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D"> is replaced by <img src="https://latex.codecogs.com/png.latex?%5Csigma"> then <img src="https://latex.codecogs.com/png.latex?z_j"> is a standard normal distribution. The difference between tail quantiles of a t-distribution and a standard normal become negligible as the sample size increases, see the Figure (3.3) below:</p>
<div id="cell-6" class="cell" data-scrolled="false" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Figure 3.3</span></span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> norm, t</span>
<span id="cb1-7">   </span>
<span id="cb1-8">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.figure(figsize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb1-9">axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13">normal_probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> norm.cdf(z) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> norm.cdf(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>z)</span>
<span id="cb1-14">t_30_probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> t.cdf(z, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> t.cdf(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>z, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>) </span>
<span id="cb1-15">t_100_probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> t.cdf(z, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> t.cdf(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>z, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) </span>
<span id="cb1-16"></span>
<span id="cb1-17">axes.plot(z, normal_probabilities, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C0'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'normal'</span>)</span>
<span id="cb1-18">axes.plot(z, t_30_probabilities, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C1'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'$t_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{30}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">$'</span>)</span>
<span id="cb1-19">axes.plot(z, t_100_probabilities, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C2'</span>, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'$t_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{100}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">$'</span>)</span>
<span id="cb1-20"></span>
<span id="cb1-21">xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axes.get_xlim()</span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> y <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>]:</span>
<span id="cb1-24">    axes.plot(xlim, [y, y], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>, </span>
<span id="cb1-25">              scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-26"></span>
<span id="cb1-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> index, probs <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>([normal_probabilities, t_30_probabilities,</span>
<span id="cb1-28">                                   t_100_probabilities]):</span>
<span id="cb1-29">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z[np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y).argmin()]</span>
<span id="cb1-30">        axes.plot([x, x], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, y], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"C</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>index<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>,</span>
<span id="cb1-31">                  scalex <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, scaley <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-32">    </span>
<span id="cb1-33">axes.legend()</span>
<span id="cb1-34">axes.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Z'</span>)</span>
<span id="cb1-35">axes.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tail Probabilities'</span>)</span>
<span id="cb1-36">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2-linear-regression-models-and-least-squares_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Test for the significance of groups of coefficients simultaneously</strong>: We use the F-statistics (3.13):</p>
<p><img src="https://latex.codecogs.com/png.latex?F=%5Cfrac%7B(RSS_0-RSS_1)%20/%20(p_1%20-%20p_0)%7D%7BRSS_1/(N-p_1-1)%7D"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?RSS_1"> is for the bigger model with <img src="https://latex.codecogs.com/png.latex?p_1+1"> parameters and <img src="https://latex.codecogs.com/png.latex?RSS_0"> for the nested smaller model with <img src="https://latex.codecogs.com/png.latex?p_0+1"> parameters. Under the null hypothesis that the smaller model is correct, the F statistic will have a <img src="https://latex.codecogs.com/png.latex?F_%7Bp_1-p_0,N-p_1-1%7D"> distribution. The <img src="https://latex.codecogs.com/png.latex?z_j"> in (3.13) is equivalent to the F statistic for dropping the single coefficient <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j"> from the model.</p>
<p>Similarly, we can isolate <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j"> in (3.10) to obtain <img src="https://latex.codecogs.com/png.latex?1-2%5Calpha"> confidence interval (3.14) <img src="https://latex.codecogs.com/png.latex?(%5Chat%7B%5Cbeta_j%7D%20-%20z%5E%7B(1-%5Calpha)%7Dv_j%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%5Chat%7B%5Csigma%7D,%20%5Chat%7B%5Cbeta_j%7D%20+%20z%5E%7B(1-%5Calpha)%7Dv_j%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%5Chat%7B%5Csigma%7D)"></p>
<p>In a similar fashion we can obtain an approximate confidence set for the entire parameter vector <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> (3.15): <img src="https://latex.codecogs.com/png.latex?%20C_%7B%5Cbeta%7D%20=%20%5C%7B%7B%5Cbeta%7C(%5Chat%7B%5Cbeta%7D-%5Cbeta)%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D(%5Chat%7B%5Cbeta%7D-%5Cbeta)%7D%20%5Cle%20%5Chat%7B%5Csigma%7D%5E2%7B%5Cchi_%7Bp+1%7D%5E2%7D%5E%7B(1-%5Calpha)%7D%20%5C%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%7B%5Cchi_%7Bl%7D%5E2%7D%5E%7B(1-%5Calpha)%7D"> is the <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> percentile of the chi-squared distribution of <img src="https://latex.codecogs.com/png.latex?l"> degrees of freedom.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2-linear-regression-models-and-least-squares.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.2.2 The Gauss–Markov Theorem</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2.2-the-gauss-markov-theorem.html</link>
  <description><![CDATA[ 





<p>The Gauss-Markov theorem states that the least squares estimates of the <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> have the smallest variance among all linear unbiased estimates. We focus on estimation of any linear combination of the parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta=%5Calpha%5ET%5Chat%7B%5Cbeta%7D">, i.e predictions <img src="https://latex.codecogs.com/png.latex?f(x_0)=%7Bx_0%7D%5ET%5Cbeta"> are of this form. The least squares estimate is (3.17):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Ctheta%7D=%5Calpha%5ET%5Chat%7B%5Cbeta%7D%20=%20%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p>and <img src="https://latex.codecogs.com/png.latex?%5Calpha%5ET%5Chat%7B%5Cbeta%7D"> is unbiased, since (3.18):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AE(%5Calpha%5ET%5Chat%7B%5Cbeta%7D)%20&amp;=%20E(%20%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D)%5C%5C%0A&amp;=%20%20%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Cbeta%5C%5C%0A&amp;=%20%5Calpha%5ET%5Cbeta%0A%5Cend%7Balign%7D%0A"></p>
<p>The <em>Gauss-Markov</em> states that if we have any other linear estimator <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7B%5Ctheta%7D=%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D">, that is, <img src="https://latex.codecogs.com/png.latex?E(%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D)=%5Calpha%5ET%5Cbeta">, then (3.19):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AVar(%5Calpha%5ET%5Chat%7B%5Cbeta%7D)%5Cle%20Var(%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D)%0A"></p>
<p><em>Proof</em>:</p>
<p>Let’s assume that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D=(%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%20+%20%5Cmathbf%7Bd%7D%5ET)%5Cmathbf%7By%7D">, then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AE(%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D)&amp;=%20E((%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%20+%20%5Cmathbf%7Bd%7D%5ET)%5Cmathbf%7By%7D)%5C%5C%0A&amp;=%20E((%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%20+%20%5Cmathbf%7Bd%7D%5ET)(%5Cmathbf%7BX%7D%5Cbeta%20+%20%5Cvarepsilon))%5C%5C%0A&amp;=%20%5Calpha%5ET%5Cbeta%20+%20d%5ET%5Cmathbf%7BX%7D%5Cbeta%0A%5Cend%7Balign%7D%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?E(%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D)"> is unbiased <img src="https://latex.codecogs.com/png.latex?d%5ET%5Cmathbf%7BX%7D=0">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AVar(%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7By%7D)%20&amp;=%20%5Cmathbf%7Bc%7D%5E%7BT%7DVar(%5Cmathbf%7By%7D)%5Cmathbf%7Bc%7D%20=%20%5Csigma%5E2%5Cmathbf%7Bc%7D%5ET%5Cmathbf%7Bc%7D%5C%5C%0A&amp;=%20%5Csigma%5E2(%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%20+%20%5Cmathbf%7Bd%7D%5ET)(%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Calpha%20+%20%5Cmathbf%7Bd%7D)%5C%5C%0A&amp;=%20%5Csigma%5E2(%5Calpha%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Calpha+%5Cmathbf%7Bd%7D%5ET%5Cmathbf%7Bd%7D)%5C%5C%0A&amp;=%20Var(%5Calpha%5ET%5Chat%7B%5Cbeta%7D)+%5Csigma%5E2%5Cmathbf%7Bd%7D%5ET%5Cmathbf%7Bd%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bd%7D%5ET%5Cmathbf%7Bd%7D%5Cge%200">, the proof is completed.</p>
<p>Consider the MSE of an estimator <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7B%5Ctheta%7D"> is estimating <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AMSE(%5Ctilde%7B%5Ctheta%7D)%20&amp;=%20E(%5Ctilde%7B%5Ctheta%7D%20-%20%5Ctheta)%5E2%5C%5C%0A&amp;=%20Var(%5Ctilde%7B%5Ctheta%7D)+%5BE(%5Ctilde%7B%5Ctheta%7D)%20-%20%5Ctheta%5D%5E2%0A%5Cend%7Balign%7D%0A"></p>
<p>The first term is the variance, while the second term is the squared bias. The theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may exist a biased estimator with smaller MSE.</p>
<p>Mean squared error is related to prediction accuracy. Consider the prediction of the <img src="https://latex.codecogs.com/png.latex?x_0">, (3.21):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_0%20=%20f(x_0)%20+%20%5Cvarepsilon_0%0A"></p>
<p>Then the EPE of an estimate <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bf%7D(x_0)=%7Bx_0%7D%5ET%5Ctilde%7B%5Cbeta%7D"> is (3.22): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AE(Y_0%20-%20%5Ctilde%7Bf%7D(x_0))%20&amp;=%20%5Csigma%5E2%20+%20E(%7Bx_0%7D%5ET%5Ctilde%7B%5Cbeta%7D-f(x_0))%5E2%5C%5C%0A&amp;=%20%5Csigma%5E2%20+%20MSE(%5Ctilde%7Bf%7D(x_0))%0A%5Cend%7Balign%7D%0A"></p>
<p>Therefore, EPE and MSE differ only by the constant <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2.2-the-gauss-markov-theorem.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.2.3 Multiple Regression From Simple Univariate Regression</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2.3-multiple-regression-from-simple-multivariate-regression.html</link>
  <description><![CDATA[ 





<p>Suppose we have a <em>univariate</em> (p = 1) model with no intercept (3.23): <img src="https://latex.codecogs.com/png.latex?Y=X%5Cbeta+%5Cvarepsilon"></p>
<p>The least squares estimate and residuals are (3.24): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7B%5Cbeta%7D%20=%20%5Ccfrac%7B%5Csum_1%5EN%20%7Bx_iy_i%7D%7D%7B%5Csum_1%5EN%20%7Bx_i%5E2%7D%7D%20%5C%5C%0Ar_i%20=%20y_i%20-%20x_i%5Chat%7B%5Cbeta%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>With the inner product: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7B%5Cbeta%7D%20=%20%5Ccfrac%7B%5Clangle%20%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D%20%5Crangle%7D%7B%5Clangle%20%5Cmathbf%7Bx%7D,%20%5Cmathbf%7Bx%7D%5Crangle%7D%5C%5C%0A%5Cmathbf%7Br%7D%20=%20%5Cmathbf%7By%7D%20-%20%5Cmathbf%7Bx%7D%5Chat%7B%5Cbeta%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>Suppose that the columns of the matrix <strong>X</strong> are orthogonal; that is <img src="https://latex.codecogs.com/png.latex?%5Clangle%20%5Cmathbf%7Bx%7D_j,%20%5Cmathbf%7Bx%7D_k%20%5Crangle%20=%200"> then it is easy to check that <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_j%7D%20=%20%5Clangle%20%5Cmathbf%7Bx%7D_j,%20%5Cmathbf%7By%7D%20%5Crangle%20/%20%5Clangle%20%5Cmathbf%7Bx%7D_j,%20%5Cmathbf%7Bx%7D_j%20%5Crangle">, i.e the inputs have no effect on each other’s parameter estimates.</p>
<p>Suppose next that we have an intercept and a single input x (3.27): <img src="https://latex.codecogs.com/png.latex?%5Chat%7BB%7D_1%20=%20%5Ccfrac%7B%5Clangle%20%5Cmathbf%7Bx%7D%20-%20%5Coverline%7Bx%7D%5Cmathbf%7B1%7D,%20%5Cmathbf%7By%7D%20%5Crangle%7D%7B%20%5Clangle%20%5Cmathbf%7Bx%7D%20-%20%5Coverline%7Bx%7D%5Cmathbf%7B1%7D,%20%5Cmathbf%7Bx%7D%20-%20%5Coverline%7Bx%7D%5Cmathbf%7B1%7D%20%5Crangle%7D"></p>
<p>We can view the estimate as the result of two simple regression:</p>
<ol type="1">
<li><p>Regress <strong>x</strong> on <strong>1</strong> to produce the residual <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D%20=%20%5Cmathbf%7Bx%7D%20-%20%5Coverline%7Bx%7D%5Cmathbf%7B1%7D"></p></li>
<li><p>Regress <strong>y</strong> on the residual <strong>z</strong> to give the coefficient <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_1">.</p></li>
</ol>
<p>Regress <strong>b</strong> on <strong>a</strong> means <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cgamma%7D=%5Clangle%20%5Cmathbf%7Ba%7D,%5Cmathbf%7Bb%7D%20%5Crangle%20/%20%5Clangle%20%5Cmathbf%7Ba%7D,%20%5Cmathbf%7Ba%7D%5Crangle"> and the residual vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20-%20%5Chat%7B%5Cgamma%7D%5Cmathbf%7Ba%7D">.</p>
<p>This recipe generalizes to the case of <em>p</em> inputs, as shown in Algorithm 3.1.</p>
<p><strong>Algorithm 3.1 Regression by Successive Orthogonalization</strong> 1. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_0%20=%20%5Cmathbf%7Bx%7D_0%20=%20%5Cmathbf%7B1%7D"></p>
<ol start="2" type="1">
<li><p>For <img src="https://latex.codecogs.com/png.latex?j%20=%201,%202,%20%5Ccdots,%20p"></p>
<ul>
<li>Regress <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_0,...,%5Cmathbf%7Bz%7D_%7Bj%20-%201%7D"> to produce <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cgamma%7D_%7Blj%7D=%5Clangle%20%5Cmathbf%7Bz%7D_l,%20%5Cmathbf%7Bx%7D_j%20%5Crangle%20/%20%5Clangle%20%5Cmathbf%7Bz%7D_l,%5Cmathbf%7Bz%7D_l%20%5Crangle"> <img src="https://latex.codecogs.com/png.latex?l=0,%5Ccdots,j-1">, and residualt vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_j=%5Cmathbf%7Bx%7D_j%20-%20%5Csum_%7Bk=0%7D%5E%7Bj-1%7D%20%5Chat%7B%5Cgamma%7D_%7Bkj%7D%5Cmathbf%7Bz%7D_k"></li>
</ul></li>
<li><p>Regress <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> on the residual <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_p"> to give the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_p"></p></li>
</ol>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats, linalg</span>
<span id="cb1-4"></span>
<span id="cb1-5">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../data/prostate/prostate.data'</span>, delimiter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, index_col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-6">mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>)</span>
<span id="cb1-7">df_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lpsa'</span>)</span>
<span id="cb1-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(stats.zscore)</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> orthogonalize(X):</span>
<span id="cb1-11">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb1-12">    G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.eye(p)</span>
<span id="cb1-13">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.copy()</span>
<span id="cb1-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, p): </span>
<span id="cb1-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(j):</span>
<span id="cb1-16">            G[l, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(Z[:, l], X[:, j]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.dot(Z[:, l], Z[:, l])</span>
<span id="cb1-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(j):</span>
<span id="cb1-18">            Z[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> G[k, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Z[:, k]</span>
<span id="cb1-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Z, G</span></code></pre></div></div>
</div>
<p>The result of this algorithm is (3.28):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_p=%5Ccfrac%7B%5Clangle%20%5Cmathbf%7Bz%7D_p,%20%5Cmathbf%7By%7D%20%5Crangle%7D%7B%5Clangle%20%5Cmathbf%7Bz%7D_p,%5Cmathbf%7Bz%7D_p%20%5Crangle%7D"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_p"> is highly correlated with some of the other <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_k">’s the residual vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_p"> will be close to zero, and from (3.28) the coefficient <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_p"> will be unstable.</p>
<p>From (3.28) we also obtain an alternative formula for the variance estimates, (3.29):</p>
<p><img src="https://latex.codecogs.com/png.latex?Var(%5Chat%7B%5Cbeta%7D_p)%20=%20%5Ccfrac%7B%5Csigma%5E2%7D%7B%5Clangle%20%5Cmathbf%7Bz%7D_p,%20%5Cmathbf%7Bz%7D_p%20%5Crangle%7D=%5Ccfrac%7B%5Csigma%5E2%7D%7B%7C%7C%5Cmathbf%7Bz%7D_p%7C%7C%5E2%7D%20%20"></p>
<p>On other words, the precision with which we can estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_p"> depends on the lengths of the residual vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_p">;</p>
<p>Algorithm 3.1 is known as the <em>Gram–Schmidt</em> procedure for multiple regression. We can represent step 2 of Algorithm 3.1 in matrix form (3.30):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D=%5Cmathbf%7BZ%5CGamma%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D"> has as columns the <img src="https://latex.codecogs.com/png.latex?z_j"> (in order), and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CGamma%7D"> is the upper triangular matrix with entries <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cgamma%7D_%7Bkj%7D">. Introducing the diagonal matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D"> with <img src="https://latex.codecogs.com/png.latex?D_%7Bjj%7D=%7C%7Cz_j%7C%7C">, we get (3.31):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D=%5Cmathbf%7BZ%7D%5Cmathbf%7BD%7D%5E%7B-1%7D%5Cmathbf%7BD%7D%5Cmathbf%7B%5CGamma%7D=%5Cmathbf%7BQR%7D"></p>
<p>the so-called QR decomposition of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">. Here <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D"> is an N × (p +1) orthogonal matrix, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQ%7D%20=%20%5Cmathbf%7BI%7D">, and <strong>R</strong> is a (p + 1) × (p + 1) upper triangular matrix.</p>
<p>The least squares solution is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D=%5Cmathbf%7BR%7D%5E%7B-1%7D%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p><em>Proof</em>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%5Chat%7B%5Cbeta%7D%5C%5C%0A%5Cmathbf%7BR%7D%5ET%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7By%7D=%5Cmathbf%7BR%7D%5ET%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7BQ%7D%5Cmathbf%7BR%7D%5Chat%7B%5Cbeta%7D%5C%5C%0A%5Cmathbf%7BR%7D%5ET%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7By%7D=%5Cmathbf%7BR%7D%5ET%5Cmathbf%7BR%7D%5Chat%7B%5Cbeta%7D%5C%5C%0A%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7By%7D=%5Cmathbf%7BR%7D%5Chat%7B%5Cbeta%7D%5C%5C%0A%5Cend%7Bequation%7D%0A"> And the predicted training values:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cmathbf%7By%7D%7D=%5Cmathbf%7BQQ%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p><em>Proof</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Chat%7B%5Cmathbf%7By%7D%7D&amp;=%5Cmathbf%7BX%7D%5Chat%7B%5Cbeta%7D%5C%5C%0A&amp;=%5Cmathbf%7BQR%7D%5Cmathbf%7BR%7D%5E%7B-1%7D%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%5Cmathbf%7BQQ%7D%5ET%5Cmathbf%7By%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>We can obtain from it not just <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_p">, but also the entire multiple least squares fit.</p>
<p><em>Proof</em>: We can easily derive that: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BR%7D%5Chat%7B%5Cbeta%7D=%5Cmathbf%7BQ%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p>which can be expanded into: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20R_%7B0%200%7D%20&amp;%20R_%7B02%7D%20%20&amp;%20%5Cdots%20%20%20&amp;%20R_%7B0p%7D%20%5C%5C%0A%20%20%20%200%20%20%20%20%20%20%20&amp;%20R_%7B11%7D%20%20&amp;%20%5Cdots%20%20%20&amp;%20R_%7B1p%7D%20%5C%5C%0A%20%20%20%20%5Cvdots%20%20&amp;%20%5Cvdots%20%20&amp;%20%5Cddots%20%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20%20%200%20%20%20%20%20%20%20&amp;%200%20%20%20%20%20%20%20&amp;%20%5Cdots%20%20%20&amp;%20R_%7Bpp%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%5Chat%7B%5Cbeta_0%7D%20%5C%5C%0A%20%20%20%20%5Chat%7B%5Cbeta_1%7D%20%5C%5C%0A%20%20%20%20%5Cvdots%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20%5Chat%7B%5Cbeta_p%7D%0A%5Cend%7Bbmatrix%7D%0A=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%7BQ_%7B0%7D%7D%5ET%5Cmathbf%7By%7D%20%5C%5C%0A%20%20%20%20%7BQ_%7B1%7D%7D%5ET%5Cmathbf%7By%7D%20%5C%5C%0A%20%20%20%20%5Cvdots%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20%7BQ_%7Bp%7D%7D%5ET%5Cmathbf%7By%7D%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>Now by applying the backward substitution it is possible to obtain the entire multiple least squares fit. For example to find the <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_p">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0AR_%7Bpp%7D%5Chat%7B%5Cbeta%7D_p%20=%20%7BQ_%7Bp%7D%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A%5Chat%7B%5Cbeta%7D_p%20=%20%5Ccfrac%7B%5Clangle%20Q_p,%20%5Cmathbf%7By%7D%20%5Crangle%7D%7BR_%7Bpp%7D%7D=%5Ccfrac%7B%5Clangle%20%5Cmathbf%7Bz%7D_p,%20%5Cmathbf%7By%7D%20%5Crangle%7D%7B%5Clangle%20%5Cmathbf%7Bz%7D_p,%5Cmathbf%7Bz%7D_p%20%5Crangle%7D%0A%5Cend%7Bequation%7D%0A"></p>
<div id="cell-8" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> least_squares_qr(data_x, data_y):</span>
<span id="cb2-2">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.c_[np.ones((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(data_x), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), data_x]</span>
<span id="cb2-3">    Z, G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> orthogonalize(X)</span>
<span id="cb2-4"></span>
<span id="cb2-5">    D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linalg.norm(Z, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-6">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> D</span>
<span id="cb2-7">    R <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.diag(D) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> G</span>
<span id="cb2-8">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linalg.solve_triangular(R, Q.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> data_y)</span>
<span id="cb2-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta</span>
<span id="cb2-10"></span>
<span id="cb2-11">beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> least_squares_qr(df[mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T'</span>].as_matrix(), df_y[mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T'</span>].as_matrix())</span>
<span id="cb2-12"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Coefficient: "</span>, beta)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Coefficient:  [ 2.46493292  0.67601634  0.26169361 -0.14073374  0.20906052  0.30362332
 -0.28700184 -0.02119493  0.26557614]</code></pre>
</div>
</div>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2.3-multiple-regression-from-simple-multivariate-regression.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.2.4 Multiple Outputs</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2.4-multiple-outputs.html</link>
  <description><![CDATA[ 





<p>Suppose we have multiple outputs <img src="https://latex.codecogs.com/png.latex?Y_1,%20Y_2,%20...,%20Y_k"> that we wish to predict from our inputs <img src="https://latex.codecogs.com/png.latex?X_0,%20X_1,%20...,%20X_p">. We assume a linear model for each output (3.34, 3.35):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AY_k%20&amp;=%20%5Cbeta_%7B0k%7D%20+%20%5Csum_%7Bj=1%7D%5E%7Bp%7D%20X_j%5Cbeta_%7Bjk%7D%20+%20%5Cvarepsilon_k%5C%5C%0A&amp;=%20f_k(X)%20+%20%5Cvarepsilon_k%0A%5Cend%7Balign%7D%0A"></p>
<p>We can write the model in matrix notation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BY%7D%20=%20%5Cmathbf%7BXB%7D%20+%20%5Cmathbf%7BE%7D%0A"></p>
<p>Here <strong>Y</strong> is the $NK $ response matrix, <strong>X</strong> is the <img src="https://latex.codecogs.com/png.latex?N%5Ctimes(p+1)"> input matrix, <strong>B</strong> is the <img src="https://latex.codecogs.com/png.latex?(p+1)%5Ctimes%20K"> matrix and <strong>E</strong> is the <img src="https://latex.codecogs.com/png.latex?N%5Ctimes%20K"> matrix of errors.</p>
<p>A straightforward generalization of the univariate loss functio is (3.37, 3.38): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0ARSS(%5Cmathbf%7BB%7D)%20&amp;=%20%5Csum_%7Bk=1%7D%5EK%7B%5Csum_%7Bi=1%7D%5EN%20(y_%7Bik%7D%20-%20f_k(x_i))%5E2%7D%5C%5C%0A&amp;=%20tr%5Cleft%5B(%5Cmathbf%7BY%7D-%5Cmathbf%7BXB%7D)%5ET(%5Cmathbf%7BY%7D-%5Cmathbf%7BXB%7D)%5Cright%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>The least squares estimates is (3.39): <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7BB%7D%7D%20=%20(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BY%7D"></p>
<p>If the errors <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon=(%5Cvarepsilon_1,...,%5Cvarepsilon_K)"> are correlated, then (3.40):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ARSS(B;%20%5CSigma)=%5Csum_%7Bi=1%7D%5EN(y_i%20-%20f(x_i))%5ET%5CSigma%5E%7B-1%7D(y_i%20-%20f(x_i))%0A"></p>
<p>arises from multivariate Gaussian theory.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.2.4-multiple-outputs.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.3 Subset Selection</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.3-subset-selection.html</link>
  <description><![CDATA[ 





<p>There are two reasons why we are not satisfied with the least squares estimates:</p>
<ul>
<li><p><em>Prediction accuracy</em>: The least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero.</p></li>
<li><p><em>Interpretation</em>: In order to get the “big picture”, we are willing to sacrifice the small details by selecting a smaller subset.</p></li>
</ul>
<section id="best-subset-selection" class="level3">
<h3 class="anchored" data-anchor-id="best-subset-selection">3.3.1 Best-Subset Selection</h3>
<p>Best subset regression finds for each k the subset of size k that givest smallest RSS. An efficient algorithm - the <em>leaps and bounds</em> procedure (feasible for p ~ 30, 40). The question of how to choose k involves the tradeoff between bias and variance; typically we choose the smallest model that minimizes an EPE.</p>
<p><strong>TODO</strong>: implement FIGURE 3.5</p>
</section>
<section id="forward--and-backward-stepwise-selection" class="level3">
<h3 class="anchored" data-anchor-id="forward--and-backward-stepwise-selection">3.3.2 Forward- and Backward-Stepwise Selection</h3>
<p>Rather than search through all possible subset (which is infeasible for p &gt; 40), we can seek a good path through them.</p>
<p><strong>Forward-stepwise selection</strong>.</p>
<p>Forward-stepwise selection starts with the intercept, and sequentially adds the predictor that most improves the fit. Clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate. List best-subset regression, the subset size k must be determined.</p>
<p>The Forward-stepwise selection is a <em>greedy algorithm</em>, however, there are reasons why it might be preferred: - <em>Computational</em>: for large p we cannot compute the best subset sequence. - <em>Statistical</em>: will have lower variance, but perhaps more bias.</p>
<p><strong>Backward-stepwise selection</strong>.</p>
<p>Backward-stepwise selection starts with the full model, and sequentially deletes the predictors that has the least impact on the fit (i.e the smallest Z-score). Backward selection can be used only when N &gt; p.</p>
<p><strong>Hybrid stepwise selection</strong>.</p>
<p>Some software packages implement hybrid stepwise-selection strategies that consider both forward and backward at each step, and select the best of the two (i.e in the R package the “step” function).</p>
</section>
<section id="forward-stagewise-regression" class="level3">
<h3 class="anchored" data-anchor-id="forward-stagewise-regression">3.3.3 Forward-Stagewise Regression</h3>
<p>Forward-Stagewise Regression(FS) is even more constrained than forward-stepwise regression. It starts with an intercept equal to <img src="https://latex.codecogs.com/png.latex?%5Coverline%7By%7D">, and centered predictors with coefficients intally all 0. At each step it identifies the variable most correlated with the current residual. It then computes the simple linear regression coefficient of the residual on this chosen variable, and then adds it to the current coefficient for that variable. This is continue till none of variables have correlation with the residuals.</p>
</section>
<section id="prostate-cancer-data-example" class="level1">
<h1>3.3.4 Prostate Cancer Data Example</h1>
<p><strong>TODO:</strong></p>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.3-subset-selection.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.4 Shrinkage Methods</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4-shrinkage-methods.html</link>
  <description><![CDATA[ 





<p>Subset selection may produces lower prediction error, however, because it is a discrete - variables are either retained or discarded - it often exhibits high variance. Shrinkage methods are more continuous, and don’t suffer as much from high variability.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4-shrinkage-methods.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.4.1 Ridge Regression</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.1-ridge-regression.html</link>
  <description><![CDATA[ 





<p>Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized RSS (3.41): <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D%20=%20%5Cunderset%7B%5Cbeta%7D%7Bargmin%7D%20%5Cleft%5C%7B%0A%5Csum_%7Bi=1%7D%5EN%20%5Cleft(%20y_i-%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%7Bx_%7Bij%7D%5Cbeta_j%7D%20%5Cright)%5E2%0A+%5Cgamma%5Csum_%7Bj=1%7D%5Ep%7B%5Cbeta_j%5E2%7D%0A%5Cright%5C%7D%0A"> Here <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cge%200"> is a complexity parameter: <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Crightarrow%20%5Cinfty">, the coefficients are shrunk toward zero (and each other). This idea also used in neural networks (known as <em>weight decay</em>).</p>
<p>An equivalent way to write the ridge problem is (3.42): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D%20=%20%5Cunderset%7B%5Cbeta%7D%7Bargmin%7D%20%5Cleft%5C%7B%0A%5Csum_%7Bi=1%7D%5EN%20%5Cleft(%20y_i-%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%7Bx_%7Bij%7D%5Cbeta_j%7D%20%5Cright)%5E2%0A%5Cright%5C%7D,%5C%5C%0A%5Ctext%7Bsubject%20to%20%7D%20%5Csum_%7Bj=1%7D%5Ep%20%7B%5Cbeta_j%5E2%20%5Cle%20t%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>There is a one-to-one correspondence between <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Ctext%7B%20and%20%7D%20t">. A large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing constraints as in (3.42), this problem is alleviated.</p>
<p>The ridge solutions are not equivariant under scaling of the inputs, so one normally standardizes the inputs before solving (3.41).</p>
<p>Notice that the intercepts has been left out of the penalty term: because, adding a constant to each target <img src="https://latex.codecogs.com/png.latex?y_i"> would not simply result in a shift of the prediction by the same amount. It can be shown that the solution to (3.41) can be separated into two parts, after reparametrization using <em>centered</em> inputs, i.e replacing <img src="https://latex.codecogs.com/png.latex?x_%7Bij%7D%20%5Ctext%7B%20by%20%7D%20x_%7Bij%7D-%5Coverline%7Bx%7D_j">: 1. we estimate <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0%20%5Ctext%7B%20as%20%7D%20%5Coverline%7By%7D">, 2. The remaining coefficients get estimated by a ridge regression.</p>
<p><em>Proof</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D%20=%20%5Cunderset%7B%5Cbeta%7D%7Bargmin%7D%20%5Cleft%5C%7B%0A%5Csum_%7Bi=1%7D%5EN%20%5Cleft(%20y_i-%20%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%5Coverline%7Bx%7D_j%5Cbeta_j%20-%5Csum_%7Bj=1%7D%5Ep%7B(x_%7Bij%7D%20-%20%5Coverline%7Bx%7D_j)%5Cbeta_j%7D%20%5Cright)%5E2%0A+%5Cgamma%5Csum_%7Bj=1%7D%5Ep%7B%5Cbeta_j%5E2%7D%0A%5Cright%5C%7D%5C%5C%0A%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D%20=%20%5Cunderset%7B%5Cbeta%7D%7Bargmin%7D%20%5Cleft%5C%7B%0A%5Csum_%7Bi=1%7D%5EN%20%5Cleft(%20y_i-%20%5Cbeta_0%5EC%20%20-%5Csum_%7Bj=1%7D%5Ep%7B(x_%7Bij%7D%20-%20%5Coverline%7Bx%7D_j)%5Cbeta_j%7D%20%5Cright)%5E2%0A+%5Cgamma%5Csum_%7Bj=1%7D%5Ep%7B%5Cbeta_j%5E2%7D%0A%5Cright%5C%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0%5EC%20=%20%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%5Coverline%7Bx%7D_j%5Cbeta_j">. Now the stationary point w.r.t <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0%5EC"> will be: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cbeta_0%5EC%20&amp;=%20%5Cfrac%7B1%7D%7BN%7D(%5Csum_%7Bi=1%7D%5EN%20y_i-%20%5Csum_%7Bi=1%7D%5EN%5Csum_%7Bj=1%7D%5Ep%7Bx_%7Bij%7D%5Cbeta_j%20+%0A%5Csum_%7Bi=1%7D%5EN%5Csum_%7Bj=1%7D%5Ep%5Coverline%7Bx%7D_j%5Cbeta_j%7D)%0A%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7BN%7D(%5Csum_%7Bi=1%7D%5EN%20y_i-%20%5Csum_%7Bj=1%7D%5Ep%5Cbeta_j%5Csum_%7Bi=1%7D%5EN%20x_%7Bij%7D%20+%0AN%5Csum_%7Bj=1%7D%5Ep%5Coverline%7Bx%7D_j%5Cbeta_j)%0A%5C%5C%0A&amp;=%5Cfrac%7B1%7D%7BN%7D(%5Csum_%7Bi=1%7D%5EN%20y_i-%20%5Csum_%7Bj=1%7D%5Ep%5Cbeta_j%5Coverline%7Bx%7D_jN%20+%0AN%5Csum_%7Bj=1%7D%5Ep%5Coverline%7Bx%7D_j%5Cbeta_j)%0A%5C%5C%0A&amp;=%5Coverline%7By%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>This completes proof.</p>
<p>We assume that the centering has been done, so that the matrix <strong>X</strong> has p columns (3.43):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ARSS(%5Cgamma)=(%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cbeta)%5ET(%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cbeta)%20+%20%5Cgamma%5Cbeta%5ET%5Cbeta%0A"></p>
<p>the ridge regression solutions are easily seen to be: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A"></p>
<p>where <strong>I</strong> is the <img src="https://latex.codecogs.com/png.latex?p%5Ctimes%20p"> identity matrix. Notice that the solution is again a linear function of <strong>y</strong>.</p>
<p><em>Proof</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ccfrac%7B%5Cpartial%7BRSS(%5Cgamma)%7D%7D%7B%5Cpartial%7B%5Cbeta%7D%7D%0A&amp;=%5Ccfrac%7B%5Cpartial%7B((%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cbeta)%5ET(%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D%5Cbeta)%20+%20%5Cgamma%5Cbeta%5ET%5Cbeta)%7D%7D%7B%5Cpartial%7B%5Cbeta%7D%7D%5C%5C%0A&amp;=%5Ccfrac%7B%5Cpartial%7B%5Cleft(%0A%20%20%5Cmathbf%7By%7D%5ET%5Cmathbf%7By%7D%20-%202%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D%5Cbeta%0A%20%20+%20(%5Cmathbf%7BX%7D%5Cbeta)%5ET%5Cmathbf%7BX%7D%5Cbeta%0A%20%20+%20%5Cgamma%5Cbeta%5ET%5Cbeta%0A%5Cright)%7D%7D%7B%5Cpartial%7B%5Cbeta%7D%7D%5C%5C%0A&amp;=-2%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D+2%5Cbeta%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+2%5Cgamma%5Cbeta%5ET%0A%5Cend%7Balign%7D%0A"></p>
<p>We set the first derivative to zero:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Ccfrac%7B%5Cpartial%7BRSS(%5Cgamma)%7D%7D%7B%5Cpartial%7B%5Cbeta%7D%7D%20=%200%5C%5C%0A-%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D+%5Cbeta%5ET%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cbeta%5ET=0%5C%5C%0A%5Cbeta%5ET(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cmathbf%7BI%7D)=%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D%5C%5C%0A%5Cbeta%5ET=%5Cmathbf%7By%7D%5ET%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5C%5C%0A%5Cbeta=(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>Figure 3.8 shows the ridge coefficient estimates for the prostate cancer example, plotted as functions of <img src="https://latex.codecogs.com/png.latex?df(%5Cgamma)">, the <em>effective degrees of freedom</em> implied by the penalty <img src="https://latex.codecogs.com/png.latex?%5Cgamma">. In the case of orthonormal inputs, the ridge estimates are: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D=%5Chat%7B%5Cbeta%7D/(1+%5Cgamma)"></p>
<p><strong>TODO:</strong> Implement Figure 3.8.</p>
<p>Ridge expression can be also derived as the mean or mode of a posterior distribution, with a prior distribution. Suppose, <img src="https://latex.codecogs.com/png.latex?y_i%20%5Csim%20N(%5Cbeta_0+x_i%5ET%5Cbeta,%20%5Csigma%5E2)"> and the <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20%5Csim%20N(0,%20%5Cmathcal%7BT%7D%5E2)">, independently of one another. Then the log-posterior density of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, is equal to the expression in (3.41), with <img src="https://latex.codecogs.com/png.latex?%5Cgamma=%5Csigma%5E2/%5Cmathcal%7BT%7D%5E2">.</p>
<p><strong>TODO</strong>: proof.</p>
<p>The <em>singular value decomposition</em> (SVD) of the centered input matrix <strong>X</strong> gives us some insight into the nature of ridge regression. The SVD of the <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20p"> matrix <strong>X</strong> (3.45): <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BX%7D=%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET%0A"> - <strong>U</strong> - <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20p"> orthogonal matrix, the columns of <strong>U</strong> span the column space of <strong>X</strong>. - <strong>V</strong> - <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20p"> orthogonal matrix, the columns of <strong>V</strong> span the row space of <strong>X</strong>. - <strong>D</strong> - <img src="https://latex.codecogs.com/png.latex?p%20%5Ctimes%20p"> diagonal matrix, with diagonal entries <img src="https://latex.codecogs.com/png.latex?d_1%20%5Cge%20d_2%20%5Cge%20...%20%5Cge%20d_p%20%5Cge%200">, called singular values of <strong>X</strong>. If one or more values <img src="https://latex.codecogs.com/png.latex?d_j%20=%200">, <strong>X</strong> is singular.</p>
<p>Using SVD we can write the least squares fitted vector as (3.46):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathbf%7BX%7D%5Chat%7B%5Cbeta%7D%5E%7Bls%7D%20&amp;=%20%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET(%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET)%5E%7B-1%7D%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET(%5Cmathbf%7BVD%7D%5Cmathbf%7BD%7D%5Cmathbf%7BV%7D%5ET)%5E%7B-1%7D%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET%5B%5Cmathbf%7BV%7D%5ET%5D%5E%7B-1%7D%5Cmathbf%7BD%7D%5E%7B-1%7D%5Cmathbf%7BD%7D%5E%7B-1%7D%5Cmathbf%7BV%7D%5E%7B-1%7D%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BUD%7D%5Cmathbf%7BD%7D%5E%7B-1%7D%5Cmathbf%7BD%7D%5E%7B-1%7D%5Cmathbf%7BD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%20%5Cmathbf%7BU%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>Now the ridge solutions are (3.47):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathbf%7BX%7D%5Chat%7B%5Cbeta%7D%5E%7Bridge%7D&amp;=%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET%0A(%5Cmathbf%7BV%7D%5Cmathbf%7BD%7D%5E2%5Cmathbf%7BV%7D%5ET+%5Cgamma%5Cmathbf%7BV%7D%5Cmathbf%7BV%7D%5ET)%5E%7B-1%7D%0A%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET%0A(%5Cmathbf%7BV%7D(%5Cmathbf%7BD%7D%5E2+%5Cgamma%5Cmathbf%7BI%7D)%5Cmathbf%7BV%7D%5ET)%5E%7B-1%7D%0A%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5ET%0A%5Cmathbf%7BV%7D(%5Cmathbf%7BD%7D%5E2+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BV%7D%5ET%0A%5Cmathbf%7BVD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%5Cmathbf%7BUD%7D(%5Cmathbf%7BD%7D%5E2+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BD%7D%5Cmathbf%7BU%7D%5ET%5Cmathbf%7By%7D%5C%5C%0A&amp;=%5Csum_%7Bj=1%7D%5Ep%5Cmathbf%7Bu%7D_j%5Ccfrac%7Bd_j%5E2%7D%7Bd_j%5E2+%5Cgamma%7D%5Cmathbf%7Bu%7D_j%5ET%5Cmathbf%7By%7D%0A%5Cend%7Balign%7D%0A"> - Computes the coordinates of <strong>y</strong> w.r.t the orthonormal basis <strong>U</strong>. - Then shrinks these coordinates of <strong>y</strong> by the factors <img src="https://latex.codecogs.com/png.latex?d_j%5E2/(d_j%5E2+%5Cgamma)">. - A greater amount of shrinkage is applied to the coordinates of basis vectors with smaller <img src="https://latex.codecogs.com/png.latex?d_j%5E2">.</p>
<p><strong>Principal Components</strong></p>
<p>What does a small value of <img src="https://latex.codecogs.com/png.latex?d_j%5E2"> mean? The sample covariance matrix is <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BS%7D=%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D/N"> (3.46): <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D%20=%20%5Cmathbf%7BVD%7D%5E2%5Cmathbf%7BV%7D%5ET%0A"></p>
<p>which is <em>eigen decomposition</em> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D">, aka the <em>principal components</em> directions of <strong>X</strong>.</p>
<p>The first principal component (PC) direction <img src="https://latex.codecogs.com/png.latex?v_1"> has the largest variance, that is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AVar(%5Cmathbf%7Bz%7D_1)%20&amp;=%20Var(%5Cmathbf%7BX%7Dv_1)%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7BN%7D%20v_1%5ET%5Cmathbf%7BVD%7D%5E2%5Cmathbf%7BV%7D%5ETv_1%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7BN%7D%0A%20%20%20%5Cbegin%7Bpmatrix%7D%201&amp;0%20&amp;%20%5Cdots%20&amp;%200%20%5Cend%7Bpmatrix%7D%0A%20%20%20%5Cmathbf%7BD%7D%5E2%0A%20%20%20%5Cbegin%7Bpmatrix%7D%201%20%5C%5C%200%20%5C%5C%20%5Cvdots%20%5C%5C%200%20%5Cend%7Bpmatrix%7D%5C%5C%0A&amp;=%5Cfrac%7Bd_1%5E2%7D%7BN%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>and in fact <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_1%20=%20%5Cmathbf%7BX%7Dv_1%20=%20%5Cmathbf%7BUD%7D%5Cmathbf%7BV%7D%5E%7BT%7Dv_1=%20%5Cmathbf%7Bu%7D_1d_1"> and it is called the first PC. Ridge regression shrinks PC directions having small variance the most.</p>
<p><strong>Effective degrees of freedom</strong></p>
<p>In Figure 3.7 we have plotted the estimated prediction error versus the quantity: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Adf(%5Cgamma)%20&amp;=%20tr%5B%5Cmathbf%7BX%7D(%5Cmathbf%7BX%7D%5ET%5Cmathbf%7BX%7D+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D%5ET%5D%5C%5C%0A&amp;=%20tr(%5Cmathbf%7BH_%7B%5Cgamma%7D%7D)%5C%5C%0A&amp;=%20tr%5B%5Cmathbf%7BUD%7D(%5Cmathbf%7BD%7D%5E2+%5Cgamma%5Cmathbf%7BI%7D)%5E%7B-1%7D%5Cmathbf%7BD%7D%5Cmathbf%7BU%7D%5ET%5D%5C%5C%0A&amp;=%20%5Csum_%7Bj=1%7D%5Ep%20%5Ccfrac%7Bd_j%5E2%7D%7Bd_j%5E2+%5Cgamma%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>This monotone decreasing function of <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the <em>effective degrees of freedom</em> of the ridge regression fit. Note that: - <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%200%20%5Ctext%7B%20%20then%20%20%7D%20df(%5Cgamma)=p"> - <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Crightarrow%20%5Cinfty%20%5Ctext%7B%20%20then%20%20%7D%20df(%5Cgamma)%20%5Crightarrow%200"> - Although all p coefficients in a ridge fit will be non-zero, they are fit in a restricted fashion controlled by <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.1-ridge-regression.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.4.2 The Lasso</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.2-the-lasso.html</link>
  <description><![CDATA[ 





<p>The lasso is a shrinkage method, which is estimated by (3.51): <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Chat%7B%5Cbeta%7D%5E%7Blasso%7D%20=%20%5Cunderset%7B%5Cbeta%7D%7Bargmin%7D%5Csum_%7Bi=1%7D%5EN%20%5Cleft(%20y_i-%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%7Bx_%7Bij%7D%5Cbeta_j%7D%20%5Cright)%5E2%5C%5C%0A%5Ctext%7B%20subject%20to%20%7D%20%5Csum_%7Bj=1%7D%5Ep%7C%5Cbeta_j%7C%20%5Cle%20t%0A%5Cend%7Bequation%7D%0A"></p>
<p>we can re-parametrize the constant <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0"> by standartizing the predictors; the solution for <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0"> is <img src="https://latex.codecogs.com/png.latex?%5Coverline%7By%7D">.</p>
<p>We can also write the lasso in the equivalent <em>Lagragngian form</em> (3.52):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7Blasso%7D%20=%20%5Cunderset%7B%5Cbeta%7D%7Bargmin%7D%20%5Cleft%5C%7B%0A%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi=1%7D%5EN%20%5Cleft(%20y_i-%5Cbeta_0-%5Csum_%7Bj=1%7D%5Ep%7Bx_%7Bij%7D%5Cbeta_j%7D%20%5Cright)%5E2%0A+%5Clambda%5Csum_%7Bj=1%7D%5Ep%7C%5Cbeta_j%7C%0A%5Cright%5C%7D"></p>
<p>The lasso solution is nonlinear in the <img src="https://latex.codecogs.com/png.latex?y_i"> and it is a quadratic programming problem.</p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.2-the-lasso.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.4-discussion-subset-selection-ridge-regression-and-the-lasso.html</link>
  <description><![CDATA[ 





<p><strong>TODO</strong></p>



 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.4-discussion-subset-selection-ridge-regression-and-the-lasso.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.4.4 Least Angle Regression</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.4-least-angle-regression.html</link>
  <description><![CDATA[ 





<p>Least angle regression (LAR) uses a similar strategy to Forwarf stepwise regression, but only enters “as much” of a predictor as it deserves.</p>
<p><strong>Algorithm 3.2</strong></p>
<ol type="1">
<li><p>Standardize the predictors to have mean zero and unit norm. Start with the residual <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D%20=%20%5Cmathbf%7By%7D%20-%20%5Cmathbf%7B%5Coverline%7By%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1,...,%5Cbeta_p%20=%200"></p></li>
<li><p>Find the predictor <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j"> most correlated with <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D">.</p></li>
<li><p>Move <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j"> from 0 towards its least-squares coefficient <img src="https://latex.codecogs.com/png.latex?%5Clangle%20%5Cmathbf%7Bx%7D_j,%20%5Cmathbf%7Br%7D%20%5Crangle">, until some other competitor <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_k"> has as much correlation with the current residual as does <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j">.</p></li>
<li><p>Move <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta_k"> in the direction defined by their joint least squares coefficient of the current residual on <img src="https://latex.codecogs.com/png.latex?%5Clangle%20%5Cmathbf%7Bx%7D_j,%20%5Cmathbf%7Bx%7D_k%20%5Crangle">, until some other competitor <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_l"> has as much correlation with the current residual.</p></li>
<li><p>Continue in this way until all <img src="https://latex.codecogs.com/png.latex?p"> predictors have been entered. After min(N - 1, p) steps, we arrive at the full least-squares solution.</p></li>
</ol>
<p>Suppose at the beginning of the kth step:</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D_k"> is the active set of variables</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B%5Cmathcal%7BA%7D_k%7D"> be the coefficients</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_k=%5Cmathbf%7By%7D%20-%20%5Cmathbf%7BX%7D_%7B%5Cmathcal%7BA%7D_k%7D%5Cbeta_%7B%5Cmathcal%7BA%7D_k%7D"> is the current residual,</p></li>
</ul>
<p>then the direction for this step is (3.55):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cdelta_k%20=%20(%5Cmathbf%7BX%7D_%7B%5Cmathcal%7BA%7D_k%7D%5ET%5Cmathbf%7BX%7D_%7B%5Cmathcal%7BA%7D_k%7D)%5E%7B-1%7D%5Cmathbf%7BX%7D_%7B%5Cmathcal%7BA%7D_k%7D%5ET%5Cmathbf%7Br%7D_k"></p>
<p>The coefficient profile then evolves as <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B%5Cmathcal%7BA%7D_k%7D(%5Calpha)=%5Cbeta_%7B%5Cmathcal%7BA%7D_k%7D%20+%20%5Calpha%20%5Ccdot%20%5Cdelta_k"> and the fit vector evolves as <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_k(%5Calpha)=%5Chat%7Bf%7D_k%20+%20%5Calpha%20%5Ccdot%20%5Cmathbf%7Bu%7D_k"></p>
<div id="cell-5" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb1-4"></span>
<span id="cb1-5">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../data/prostate/prostate.data'</span>, delimiter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, index_col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-6">mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>)</span>
<span id="cb1-7">df_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lpsa'</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T'</span>]</span>
<span id="cb1-10">train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_y[mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T'</span>]</span>
<span id="cb1-11"></span>
<span id="cb1-12">train_x_centered <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_x.mean(axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-13">train_x_centered <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> np.linalg.norm(train_x_centered, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-14">train_y_centered <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_y.mean()</span></code></pre></div></div>
</div>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> lars(X, y):</span>
<span id="cb2-2">    n, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.shape</span>
<span id="cb2-3">    mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(y)</span>
<span id="cb2-4">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(p)</span>
<span id="cb2-5"></span>
<span id="cb2-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(p):</span>
<span id="cb2-7">        c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> (y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mu) </span>
<span id="cb2-8">        c_abs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(c)</span>
<span id="cb2-9">        c_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> c_abs.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>() </span>
<span id="cb2-10"></span>
<span id="cb2-11">        active <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.isclose(c_abs, c_max)</span>
<span id="cb2-12">        signs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(c[active] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-13">      </span>
<span id="cb2-14">        X_active <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> signs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, active]</span>
<span id="cb2-15"></span>
<span id="cb2-16">        G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_active.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> X_active</span>
<span id="cb2-17">        Ginv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.inv(G)</span>
<span id="cb2-18"></span>
<span id="cb2-19">        A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ginv.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb2-20"></span>
<span id="cb2-21">        w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Ginv.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-22">        u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_active <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> w</span>
<span id="cb2-23"></span>
<span id="cb2-24">        gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> c_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> A</span>
<span id="cb2-25"></span>
<span id="cb2-26">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span>(active):</span>
<span id="cb2-27">            a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> u</span>
<span id="cb2-28">            complement <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.invert(active)</span>
<span id="cb2-29">            cc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> c[complement]</span>
<span id="cb2-30">            ac <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> a[complement]</span>
<span id="cb2-31">            candidates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate([(c_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> cc) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ac),</span>
<span id="cb2-32">                                         (c_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> cc) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ac)])</span>
<span id="cb2-33">            gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> candidates[candidates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>()</span>
<span id="cb2-34">        mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> u</span>
<span id="cb2-35">        beta[active] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> signs</span>
<span id="cb2-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mu, beta</span></code></pre></div></div>
</div>
<div id="cell-7" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y_fit, beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lars(train_x_centered.as_matrix(), train_y_centered.as_matrix())</span>
<span id="cb3-2">train_error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean((y_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_y_centered) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Beta: '</span>, beta)</span>
<span id="cb3-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train error: '</span>, train_error)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Beta:  [10.06592433  6.58925225 -1.95834047  4.00665484  5.62572779 -1.65081245
 -0.20495795  3.9589639 ]
train error:  0.43919976805833433</code></pre>
</div>
</div>
<p><strong>Algorithm 3.2a</strong></p>
<p>4a. If a non-zero coefficient hits zero, drop its variable from the active set of variables and recompute the current joint least squares direction.</p>
<p>The LAR(lasso) algorithm is extremely efficient, requiring the same order of computation as that of a single least squares fit using the p predictors.</p>
<p><strong>Heuristic argument why LAR and Lasso are similar</strong></p>
<p>Suppose <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is the active set of variables at some stage. We can express as (3.56): <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)=%5Clambda%20%5Ccdot%20s_j,%20j%20%5Cin%20%5Cmathcal%7BA%7D"></p>
<p>also <img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7Bx%7D_j%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)%7C%20%5Cle%20%5Clambda,%20j%20%5Cnotin%20%5Cmathcal%7BA%7D">. Now consider the lasso criterian (3.57):</p>
<p><img src="https://latex.codecogs.com/png.latex?R(%5Cbeta)=%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta%7C%7C_2%5E2%20+%20%5Clambda%7C%7C%5Cbeta%7C%7C_1"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"> be the active set of variables in the solution for a given value of <img src="https://latex.codecogs.com/png.latex?%5Clambda">, and <img src="https://latex.codecogs.com/png.latex?R(%5Cbeta)"> is differentiable, and the stationarity conditions give (3.58):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)=%5Clambda%20%5Ccdot%20sign(%5Cbeta_j),%20j%20%5Cin%20%5Cmathcal%7BB%7D"></p>
<p>Comparing (3.56) and (3.58), we see that they are identical only if the sign of <img src="https://latex.codecogs.com/png.latex?%5Cbeta%7Bj%7D"> matches the sign of the inner product. That is why the LAR algorithm and lasso starts to differ when an active coefficient passes through zero; The stationary conditions for the non-active variable require that (3.59):</p>
<p><img src="https://latex.codecogs.com/png.latex?%7C%5Cmathbf%7Bx%7D_j%5ET(%5Cmathbf%7By%7D-%5Cmathbf%7BX%7D%5Cbeta)%7C%5Cle%20%5Clambda,%20j%20%5Cnotin%20%5Cmathcal%7BB%7D"></p>
<section id="degrees-of-freedom-formula-for-lar-and-lasso" class="level1">
<h1>Degrees-of-Freedom Formula for LAR and Lasso</h1>
<p>We define the degrees of freedom of the fitted vector <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Adf(%5Chat%7By%7D)=%5Cfrac%7B1%7D%7B%5Csigma%5E2%7D%5Csum_%7Bi=1%7D%5EN%20Cov(%5Chat%7By%7D_i,y_i)%0A"></p>
<p>This makes intuitive sense: the harder that we fit to the data, the larger this covariance and hence <img src="https://latex.codecogs.com/png.latex?df(%5Chat%7B%5Cmathbf%7By%7D%7D)">.</p>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.4.4-least-angle-regression.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
<item>
  <title>3.5 Methods Using Derived Input Directions</title>
  <link>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.5-methods-using-derived-input-directions.html</link>
  <description><![CDATA[ 





<p>In many situations, we have a large number of inputs, ofter very correlated. This methods in this section produce a small number of linear combinations <strong>Z</strong> of the original inputs <strong>X</strong>.</p>
<section id="principal-components-regression" class="level3">
<h3 class="anchored" data-anchor-id="principal-components-regression">3.5.1 Principal Components Regression</h3>
<p>Princinal component regression (PCR) forms the derived input columns <img src="https://latex.codecogs.com/png.latex?z_m=%5Cmathbf%7BX%7Dv_m"> and then regresses <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20%5Ctext%7B%20on%20%7D%20%5Cmathbf%7Bz%7D_1,...,%5Cmathbf%7Bz%7D_M"> for some M &lt;= p.&nbsp;Since the <strong>Z</strong> are orthogonal (3.61): <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cmathbf%7By%7D%7D_%7B(M)%7D%5E%7Bpcr%7D%20=%20%5Coverline%7By%7D%5Cmathbf%7B1%7D%20+%20%5Csum_%7Bm=1%7D%5EM%20%5Chat%7B%5Ctheta%7D_m%5Cmathbf%7Bz%7D_m%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_m%20=%20%5Clangle%20%5Cmathbf%7Bz%7D_m,%20%5Cmathbf%7By%7D%20%5Crangle%20/%20%5Clangle%20%5Cmathbf%7Bz%7D_m,%20%5Cmathbf%7Bz%7D_m%5Crangle">. Since the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_m"> are linear combinations of the original <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j">, we can express in terms of coefficients of the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_j"> (3.62):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D%5E%7Bpcr%7D(M)%20=%20%5Csum_%7Bm=1%7D%5EM%20%5Chat%7B%5Ctheta%7D_m%20v_m%0A"></p>
<p><strong>TODO</strong>: proof</p>
<ul>
<li><p>we first standardize the inputs.</p></li>
<li><p>Note that if M = p, we would get least squares estimates, since <strong>Z</strong>=<strong>UD</strong> span the column space of <strong>X</strong>.</p></li>
<li><p>PCR is very similar to ridge regression.</p></li>
</ul>
</section>
<section id="partial-least-squares" class="level1">
<h1>3.5.2 Partial Least Squares</h1>
<ul>
<li><p>Partial Least Squares (PLS) begins by computing <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cvarphi%7D_%7B1j%7D%20=%20%5Clangle%20%5Cmathbf%7Bx%7D_j,%20%5Cmathbf%7By%7D%20%5Crangle"> for each j.</p></li>
<li><p>Then we construct the derived input <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_1=%5Csum_j%20%5Chat%7B%5Cvarphi%7D_%7B1j%7D%20%5Cmathbf%7Bx%7D_j">, which the 1st partial least squares direction.</p></li>
<li><p>The outcome <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> is regressed on <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_1"> giving coefficient <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_1"></p></li>
<li><p>And then we orthogonalize <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_1,...,%20%5Cmathbf%7Bx%7D_p"> w.r.t <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_1">.</p></li>
<li><p>We continue this process until M &lt;= p directions have been obtained.</p></li>
</ul>
<p>Note: PLS produces orthogonal inputs(or directions) <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D_1,...,%5Cmathbf%7Bz%7D_M"> and if M = p, we would get least squares estimates.</p>
<div id="cell-4" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Algorithm 3.3 Partial Least Squares.</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb1-5"></span>
<span id="cb1-6">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../data/prostate/prostate.data'</span>, delimiter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, index_col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>)</span>
<span id="cb1-8">df_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lpsa'</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10">train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T'</span>]</span>
<span id="cb1-11">train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_y[mask_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T'</span>]</span>
<span id="cb1-12"></span>
<span id="cb1-13">train_x_centered <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_x.mean(axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-14">train_x_centered <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> np.linalg.norm(train_x_centered, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-15">train_y_centered <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_y.mean()</span></code></pre></div></div>
</div>
<div id="cell-5" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> PLS(X, y):</span>
<span id="cb2-2">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.copy()</span>
<span id="cb2-3">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.copy()</span>
<span id="cb2-4"></span>
<span id="cb2-5">    n, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.shape</span>
<span id="cb2-6">    y_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.mean() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.ones(n)</span>
<span id="cb2-7">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(X)</span>
<span id="cb2-8">    theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(p)</span>
<span id="cb2-9">    </span>
<span id="cb2-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(p):</span>
<span id="cb2-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(p):</span>
<span id="cb2-12">            phi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(X[:, j], y)</span>
<span id="cb2-13">            Z[:, m] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> phi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, j]</span>
<span id="cb2-14"></span>
<span id="cb2-15">        theta[m] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(Z[:, m], y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.dot(Z[:, m], Z[:, m])</span>
<span id="cb2-16">        y_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> theta[m] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Z[:, m]</span>
<span id="cb2-17">        </span>
<span id="cb2-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(p):</span>
<span id="cb2-19">            X[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> np.dot(Z[:, m], X[:, j]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.dot(Z[:, m], Z[:, m]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Z[:, m]</span>
<span id="cb2-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y_fit</span></code></pre></div></div>
</div>
<div id="cell-6" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PLS(train_x_centered.values, train_y_centered.values)</span>
<span id="cb3-2">train_error <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean((y_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_y_centered) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Train error: '</span>, train_error)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train error:  0.43919976805833433</code></pre>
</div>
</div>
<p>It can be shown that PLS seeks directions that have high variance and have high correlation with the response. The mth PLS direction <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cvarphi%7D_m"> solves:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0Amax_%5Calpha%20Corr%5E2(%5Cmathbf%7By%7D,%20%5Cmathbf%7BX%7D%5Calpha)Var(%5Cmathbf%7BX%7D%5Calpha)%5C%5C%0A%5Ctext%7B%20subject%20to%20%7D%20%5C%7C%20%5Calpha%20%5C%7C%20=%201,%20%5Calpha%5ET%5Cmathbf%7BS%7Dv_l%20=%200,%20l%20=%201,%20...,%20m%20-%201%0A%5Cend%7Bequation%7D%0A"></p>


</section>

 ]]></description>
  <guid>https://maitbayev.github.io/esl/the-elements-of-statistical-learning/chapter-03/3.5-methods-using-derived-input-directions.html</guid>
  <pubDate>Mon, 09 Feb 2026 23:44:52 GMT</pubDate>
</item>
</channel>
</rss>
