[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Engineer at Snap Inc, ex-Google."
  },
  {
    "objectID": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "href": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "title": "WIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes",
    "section": "",
    "text": "Notes for “High-Resolution Image Synthesis with Latent Diffusion Models”\nMy notes for the “High-Resolution Image Synthesis with Latent Diffusion Models” paper. Feel free to ask questions on my telegram channel"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html",
    "href": "posts/auto-encoding-variational-bayes/index.html",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "My notes for the “Auto-Encoding Variational Bayes” paper. Feel free to ask questions on my telegram channel\n\n\n\n\nLet us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable.\n\n\n\n\nWe use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#method",
    "href": "posts/auto-encoding-variational-bayes/index.html#method",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "Let us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable."
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "href": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "We use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/jensens-inequality/index.html",
    "href": "posts/jensens-inequality/index.html",
    "title": "WIP: Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is fundamental in many fields, including machine learning and statistics. For example, it is useful in the diffusion models paper for understanding the variational lower bound. In this post, I will give a simple geometric intuition for Jensen’s inequality.\nFeel free to leave feedback on my telegram channel.\n\n\n\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\n\n\ncode for fig_jensen_inequality\nimport itertools\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n\ndef alpha_profiles(n: int):\n    if n == 2:\n        space = np.linspace(0.01, 0.99, 100)\n        return np.column_stack((space, 1.0 - space))\n    space = np.linspace(0.01, 0.99, 15 - max(0, (n - 3) * 5))\n    space_prod = itertools.product(*[space for _ in range(n - 1)])\n    profiles = np.array(list(space_prod))\n    profiles = profiles[np.sum(profiles, axis=1) &lt; 1.0]\n    return np.concatenate([profiles, 1 - np.sum(profiles, axis=1).reshape(-1, 1)], axis=1)\n\n\ndef fig_jensen_inequality(f, x_range: np.array, x: np.array, show_hull_point_legend: bool = True):\n    points = np.column_stack([x, f(x)])\n    n = len(points)\n    steps = []\n    hull_points = []\n    titles = []\n    for index, alphas in enumerate(alpha_profiles(n)):\n        hp = np.average(points, weights=alphas, axis=0)\n        hull_points.append(hp)\n        title = \",\".join([\"\\\\lambda_\" + f\"{i + 1}={a:.2f}\" for i, a in enumerate(alphas)])\n        title = f\"${title}$\"\n        titles.append(title)\n        step = dict(name=index, label=index, method=\"update\",\n                    args=[{\n                        \"x\": [[hp[0]], [hp[0], hp[0]]],\n                        \"y\": [[hp[1]], [f(hp[0]), hp[1]]],\n                    }, {\"title\": title}, [2, 3]])\n        steps.append(step)\n    active_index = len(steps) // 2\n    sliders = [dict(active=len(steps) // 2, steps=steps)]\n    return go.Figure(data=[\n        go.Scatter(\n            name=\"f\", x=x_range, y=f(x_range), hoverinfo=\"none\"\n        ),\n        go.Scatter(\n            name=\"Convex Hull\", x=np.append(points[:, 0], points[0][0]),\n            y=np.append(points[:, 1], points[0][1]),\n            fillcolor=\"rgba(239, 85, 59, 0.2)\", fill=\"toself\", mode=\"lines\",\n            line=dict(width=3), hoverinfo=\"none\",\n            showlegend=points.shape[0] &gt; 2\n        ),\n        go.Scatter(\n            name=\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\",\n            x=[hull_points[active_index][0]],\n            y=[hull_points[active_index][1]],\n            mode=f\"markers{'+text' if show_hull_point_legend else ''}\",\n            text=[\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\"],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"black\"},\n            legendrank=1001,\n            showlegend=show_hull_point_legend,\n        ),\n        go.Scatter(\n            x=[hull_points[active_index][0], hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0]), hull_points[active_index][1]],\n            mode=\"lines\",\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            line={\"color\": \"black\", \"dash\": \"dot\", \"width\": 1},\n            showlegend=False\n        ),\n        go.Scatter(\n            name=\"$(x_i, f(x_i))$\",\n            x=points[:, 0], y=points[:, 1],\n            mode=\"markers+text\",\n            marker={\"size\": 20},\n            text=[f\"$(x_{i},f(x_{i}))$\" for i in range(1, n + 1)],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\"\n        ),\n\n    ], layout=go.Layout(\n        title=titles[active_index],\n        xaxis=dict(fixedrange=True),\n        yaxis=dict(fixedrange=True, scaleanchor=\"x\", scaleratio=1),\n        sliders=sliders,\n        legend=dict(\n            yanchor=\"top\",\n            xanchor=\"right\",\n            x=1,\n            y=1\n        ),\n        margin=dict(l=5, r=5, t=50, b=50)\n    ))\n\n\ndef sample_parabola(x):\n    return 0.15 * (x - 15) ** 2 + 15\n\n\n\n\n\n\nA function is a convex function when the line segment joining any two points on the graph of the function lies above or on the graph. In the simplest term, a convex function is shaped like a cup \\(\\cup\\) and a concave function is shaped like a cap \\(\\cap\\). If f is convex, then -f is concave.\nA visualization from Wikipedia:\n\n\ndisplay image from Wikipedia\nfrom IPython.display import Image\nImage(url='https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg', width=400)\n\n\n\n\n\n\n\nA function is called convex if the following holds:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nand concave when:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nThis definition formally describes the above visualization from Wikipedia, but where \\(\\lambda=t\\).\n\n\n\n\nfig_jensen_inequality(\n    f=sample_parabola,\n    x_range=np.linspace(0, 30, 100),\n    x=np.array([2, 22]),\n).show(renderer=\"iframe\")\n\n\n\n\nAn interactive visualization of a convex function \\(f(x)=0.15(x - 15)^2 + 15\\). We will use the same parabola during this post unless stated otherwise. You can use the slider to try different values of (\\(\\lambda_1\\), \\(\\lambda_2)\\), where \\(\\lambda_2=1-\\lambda_1\\).\nWe have a line segment that connects two points on the parabola: \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\). We can sample any point along the line segment by \\((\\lambda_1 x_1 + \\lambda_2 x_2, \\lambda_1 f(x_1) + \\lambda_2 f(x_2))\\). When \\(\\lambda_1=0\\), we get the second point; when \\(\\lambda_1=1\\) we get the first point; when \\(\\lambda_1=0.5\\), we get the middle point of the line segment; and so on… This point is visualized with a black point above. Let’s name this point as \\(A\\).\nThe point located at the intersection between the parabola and the dotted black line segment is described by: \\((\\lambda_1 x_1 + \\lambda_2 x_2, f(\\lambda_1 x_1 + \\lambda_2 x_2))\\). Let’s name it as \\(B\\).\nThen, the definition above is just asserting that \\(B_y \\le A_y\\) and we also have \\(A_x = B_x\\). Note that we are just showing a single line segment, but this statement should be true for any line segment joining two points on the graph.\n\n\n\n\nJensen’s inequality is a generalization of the convex function definition above for more than 2 points.\n\n\nAssume we have a real-valued convex function \\(f\\), \\(x_1, x_2, \\cdots, x_n\\) in its domain, and positive weights \\(w_1, w_2, \\cdots, w_n\\). Then we have:\n\\[\nf\\left(\\frac{w_1x_1+w_2x_2+\\cdots+w_nx_n}{w_1+w_2+\\cdots+w_n}\\right) \\le \\frac{w_1f(x_1)+w_2f(x_2)+\\cdots+w_nf(x_n)}{w_1+w_2+\\cdots+w_n}\n\\]\nThe equation is flipped for a concave function g:\n\\[\ng\\left(\\frac{w_1x_1+w_2x_2+\\cdots+w_nx_n}{w_1+w_2+\\cdots+w_n}\\right) \\ge \\frac{w_1g(x_1)+w_2g(x_2)+\\cdots+w_ng(x_n)}{w_1+w_2+\\cdots+w_n}\n\\]\nLet’s simplify the inequality by normalizing \\(w_1, w_2, \\cdots, w_n\\):\n\\[\n\\lambda_i=\\frac{w_i}{\\sum_{k=1}^n w_k} \\text{ where } i=1, 2, \\cdots, n\n\\]\nwhere it is easy to see that \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then we have:\n\\[\nf(\\sum_{i=1}^n \\lambda_i x_i) \\le \\sum_{i=1}^n \\lambda_i f(x_i)\n\\]\nNote that we have the same convex function definition when \\(n=2\\).\n\n\n\nA numerous proofs are already given by other posts. I encourage you to checkout the following resources:\n\nen.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\nbrilliant.org/wiki/jensens-inequality\nartofproblemsolving.com/wiki/…\nI will add more as I discover\n\nHere we describe a geometric intuition, which personally resonates more with me.\n\n\nLet’s start with \\(n=3\\) case, i.e.:\n\nfig_jensen_inequality(\n    f=sample_parabola,\n    x_range=np.linspace(0, 30, 100),\n    x=np.array([2, 13, 25]),\n).show(renderer=\"iframe\")\n\n\n\n\nA similar interactive visualization as before. You can use the slider to try different values of \\((\\lambda_1, \\lambda_2, \\lambda_3)\\) where \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\).\nWe have a triangle that connect the points: \\((x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3))\\). In the \\(n=2\\) case, we used \\(\\lambda_1\\) and \\(\\lambda_2\\) to sample any point along the line segment. In this case, it is very similar, but we can sample any point inside the triangle by using \\(\\lambda_1, \\lambda_2\\) and \\(\\lambda_3\\). When \\(\\lambda_1=1\\), we get the first point; when \\(\\lambda_3=1\\) we get the third point; when \\(\\lambda_1=1/3, \\lambda_2=1/3, \\lambda_3=1/3\\), we get the center of mass of the triangle; and so on… As usual, this point is visualized with a black point above.\nNote that (\\(\\lambda_1\\), \\(\\lambda_2\\) and \\(\\lambda_3\\)) describes the barycentric coordinate system."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#introduction",
    "href": "posts/jensens-inequality/index.html#introduction",
    "title": "WIP: Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is fundamental in many fields, including machine learning and statistics. For example, it is useful in the diffusion models paper for understanding the variational lower bound. In this post, I will give a simple geometric intuition for Jensen’s inequality.\nFeel free to leave feedback on my telegram channel."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#setup",
    "href": "posts/jensens-inequality/index.html#setup",
    "title": "WIP: Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "The post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\n\n\ncode for fig_jensen_inequality\nimport itertools\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n\ndef alpha_profiles(n: int):\n    if n == 2:\n        space = np.linspace(0.01, 0.99, 100)\n        return np.column_stack((space, 1.0 - space))\n    space = np.linspace(0.01, 0.99, 15 - max(0, (n - 3) * 5))\n    space_prod = itertools.product(*[space for _ in range(n - 1)])\n    profiles = np.array(list(space_prod))\n    profiles = profiles[np.sum(profiles, axis=1) &lt; 1.0]\n    return np.concatenate([profiles, 1 - np.sum(profiles, axis=1).reshape(-1, 1)], axis=1)\n\n\ndef fig_jensen_inequality(f, x_range: np.array, x: np.array, show_hull_point_legend: bool = True):\n    points = np.column_stack([x, f(x)])\n    n = len(points)\n    steps = []\n    hull_points = []\n    titles = []\n    for index, alphas in enumerate(alpha_profiles(n)):\n        hp = np.average(points, weights=alphas, axis=0)\n        hull_points.append(hp)\n        title = \",\".join([\"\\\\lambda_\" + f\"{i + 1}={a:.2f}\" for i, a in enumerate(alphas)])\n        title = f\"${title}$\"\n        titles.append(title)\n        step = dict(name=index, label=index, method=\"update\",\n                    args=[{\n                        \"x\": [[hp[0]], [hp[0], hp[0]]],\n                        \"y\": [[hp[1]], [f(hp[0]), hp[1]]],\n                    }, {\"title\": title}, [2, 3]])\n        steps.append(step)\n    active_index = len(steps) // 2\n    sliders = [dict(active=len(steps) // 2, steps=steps)]\n    return go.Figure(data=[\n        go.Scatter(\n            name=\"f\", x=x_range, y=f(x_range), hoverinfo=\"none\"\n        ),\n        go.Scatter(\n            name=\"Convex Hull\", x=np.append(points[:, 0], points[0][0]),\n            y=np.append(points[:, 1], points[0][1]),\n            fillcolor=\"rgba(239, 85, 59, 0.2)\", fill=\"toself\", mode=\"lines\",\n            line=dict(width=3), hoverinfo=\"none\",\n            showlegend=points.shape[0] &gt; 2\n        ),\n        go.Scatter(\n            name=\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\",\n            x=[hull_points[active_index][0]],\n            y=[hull_points[active_index][1]],\n            mode=f\"markers{'+text' if show_hull_point_legend else ''}\",\n            text=[\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\"],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"black\"},\n            legendrank=1001,\n            showlegend=show_hull_point_legend,\n        ),\n        go.Scatter(\n            x=[hull_points[active_index][0], hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0]), hull_points[active_index][1]],\n            mode=\"lines\",\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            line={\"color\": \"black\", \"dash\": \"dot\", \"width\": 1},\n            showlegend=False\n        ),\n        go.Scatter(\n            name=\"$(x_i, f(x_i))$\",\n            x=points[:, 0], y=points[:, 1],\n            mode=\"markers+text\",\n            marker={\"size\": 20},\n            text=[f\"$(x_{i},f(x_{i}))$\" for i in range(1, n + 1)],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\"\n        ),\n\n    ], layout=go.Layout(\n        title=titles[active_index],\n        xaxis=dict(fixedrange=True),\n        yaxis=dict(fixedrange=True, scaleanchor=\"x\", scaleratio=1),\n        sliders=sliders,\n        legend=dict(\n            yanchor=\"top\",\n            xanchor=\"right\",\n            x=1,\n            y=1\n        ),\n        margin=dict(l=5, r=5, t=50, b=50)\n    ))\n\n\ndef sample_parabola(x):\n    return 0.15 * (x - 15) ** 2 + 15"
  },
  {
    "objectID": "posts/jensens-inequality/index.html#convex-function",
    "href": "posts/jensens-inequality/index.html#convex-function",
    "title": "WIP: Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "A function is a convex function when the line segment joining any two points on the graph of the function lies above or on the graph. In the simplest term, a convex function is shaped like a cup \\(\\cup\\) and a concave function is shaped like a cap \\(\\cap\\). If f is convex, then -f is concave.\nA visualization from Wikipedia:\n\n\ndisplay image from Wikipedia\nfrom IPython.display import Image\nImage(url='https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg', width=400)\n\n\n\n\n\n\n\nA function is called convex if the following holds:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nand concave when:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nThis definition formally describes the above visualization from Wikipedia, but where \\(\\lambda=t\\).\n\n\n\n\nfig_jensen_inequality(\n    f=sample_parabola,\n    x_range=np.linspace(0, 30, 100),\n    x=np.array([2, 22]),\n).show(renderer=\"iframe\")\n\n\n\n\nAn interactive visualization of a convex function \\(f(x)=0.15(x - 15)^2 + 15\\). We will use the same parabola during this post unless stated otherwise. You can use the slider to try different values of (\\(\\lambda_1\\), \\(\\lambda_2)\\), where \\(\\lambda_2=1-\\lambda_1\\).\nWe have a line segment that connects two points on the parabola: \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\). We can sample any point along the line segment by \\((\\lambda_1 x_1 + \\lambda_2 x_2, \\lambda_1 f(x_1) + \\lambda_2 f(x_2))\\). When \\(\\lambda_1=0\\), we get the second point; when \\(\\lambda_1=1\\) we get the first point; when \\(\\lambda_1=0.5\\), we get the middle point of the line segment; and so on… This point is visualized with a black point above. Let’s name this point as \\(A\\).\nThe point located at the intersection between the parabola and the dotted black line segment is described by: \\((\\lambda_1 x_1 + \\lambda_2 x_2, f(\\lambda_1 x_1 + \\lambda_2 x_2))\\). Let’s name it as \\(B\\).\nThen, the definition above is just asserting that \\(B_y \\le A_y\\) and we also have \\(A_x = B_x\\). Note that we are just showing a single line segment, but this statement should be true for any line segment joining two points on the graph."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#jensens-inequality",
    "href": "posts/jensens-inequality/index.html#jensens-inequality",
    "title": "WIP: Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is a generalization of the convex function definition above for more than 2 points.\n\n\nAssume we have a real-valued convex function \\(f\\), \\(x_1, x_2, \\cdots, x_n\\) in its domain, and positive weights \\(w_1, w_2, \\cdots, w_n\\). Then we have:\n\\[\nf\\left(\\frac{w_1x_1+w_2x_2+\\cdots+w_nx_n}{w_1+w_2+\\cdots+w_n}\\right) \\le \\frac{w_1f(x_1)+w_2f(x_2)+\\cdots+w_nf(x_n)}{w_1+w_2+\\cdots+w_n}\n\\]\nThe equation is flipped for a concave function g:\n\\[\ng\\left(\\frac{w_1x_1+w_2x_2+\\cdots+w_nx_n}{w_1+w_2+\\cdots+w_n}\\right) \\ge \\frac{w_1g(x_1)+w_2g(x_2)+\\cdots+w_ng(x_n)}{w_1+w_2+\\cdots+w_n}\n\\]\nLet’s simplify the inequality by normalizing \\(w_1, w_2, \\cdots, w_n\\):\n\\[\n\\lambda_i=\\frac{w_i}{\\sum_{k=1}^n w_k} \\text{ where } i=1, 2, \\cdots, n\n\\]\nwhere it is easy to see that \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then we have:\n\\[\nf(\\sum_{i=1}^n \\lambda_i x_i) \\le \\sum_{i=1}^n \\lambda_i f(x_i)\n\\]\nNote that we have the same convex function definition when \\(n=2\\).\n\n\n\nA numerous proofs are already given by other posts. I encourage you to checkout the following resources:\n\nen.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\nbrilliant.org/wiki/jensens-inequality\nartofproblemsolving.com/wiki/…\nI will add more as I discover\n\nHere we describe a geometric intuition, which personally resonates more with me.\n\n\nLet’s start with \\(n=3\\) case, i.e.:\n\nfig_jensen_inequality(\n    f=sample_parabola,\n    x_range=np.linspace(0, 30, 100),\n    x=np.array([2, 13, 25]),\n).show(renderer=\"iframe\")\n\n\n\n\nA similar interactive visualization as before. You can use the slider to try different values of \\((\\lambda_1, \\lambda_2, \\lambda_3)\\) where \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\).\nWe have a triangle that connect the points: \\((x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3))\\). In the \\(n=2\\) case, we used \\(\\lambda_1\\) and \\(\\lambda_2\\) to sample any point along the line segment. In this case, it is very similar, but we can sample any point inside the triangle by using \\(\\lambda_1, \\lambda_2\\) and \\(\\lambda_3\\). When \\(\\lambda_1=1\\), we get the first point; when \\(\\lambda_3=1\\) we get the third point; when \\(\\lambda_1=1/3, \\lambda_2=1/3, \\lambda_3=1/3\\), we get the center of mass of the triangle; and so on… As usual, this point is visualized with a black point above.\nNote that (\\(\\lambda_1\\), \\(\\lambda_2\\) and \\(\\lambda_3\\)) describes the barycentric coordinate system."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\nFeel free to leave feedback on my telegram channel.\n\n\n\n\n\nThe lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the sum of absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the same equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying less important features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero.\n\n\n\n\n\n\nThe best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?\n\n\n\n\n\n\nWe will use a simple loss function that illustrates circle contours instead of elliptical ones.:\n\\[\nLoss(\\beta_1,\\beta_2 | c_x, c_y)=2{(\\beta_1 - c_x)}^2 + 2{(\\beta_2 - c_y)}^2 + 100\n\\]\nOnce we understand the intution for circles, it is easy to extend to other contours such as elliptical ones. We will use \\(c_x=15\\) and \\(c_y=5\\) most of the time.\n\n\ncode for loss setup and helper functions\nfrom enum import Enum\n\nimport mpl_toolkits.mplot3d.art3d as art3d\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else:\n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n\n\ndef argmin_within_constraint(reg: Reg, t: float):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    if reg == Reg.L1:\n        mask = np.abs(B0) + np.abs(B1) &lt;= t\n    else:\n        mask = B0 * B0 + B1 * B1 &lt;= t * t\n    index = np.argmin(Z[mask])\n    return B0[mask][index], B1[mask][index]\n\n\nbeta_range = -20, 20\ncx, cy = 15, 5\nvmax = 1000\n\n\n\n\ncode for plot3d\ndef base_fig3():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0, 500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n    # draw axes\n    ax.plot(beta_range, [0, 0], color='k')\n    ax.plot([0, 0], beta_range, color='k')\n    return fig, ax\n\n\ndef plot3d(reg: Reg, t=3):\n    fig, ax = base_fig3()\n\n    # surface\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n\n    # contours\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', zdir='z', offset=0, vmax=vmax)\n    \n    # minima within regularization shape\n    mx, my = argmin_within_constraint(reg, t)\n    ax.plot([mx], [my], marker='.', markersize=10, color='r')\n\n    # regularization contraints\n    reg_shape = make_reg_shape(reg, t, color=\"black\")\n    ax.add_patch(reg_shape)\n    art3d.pathpatch_2d_to_3d(reg_shape, z=0)\n\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\n\nLet’s visualize our loss \\(2{(\\beta_1 - 15)}^2 + 2{(\\beta_2 - 5)}^2 + 100\\) with the L1 constaint \\(t=5\\), i.e., \\(|\\beta_1| + |\\beta_2| \\le 5\\) in 3D:\n\nplot3d(Reg.L1, t=5)\n\n\n\n\n\n\n\n\nIn Lasso Regression, we’re looking for \\(\\beta_1\\) and \\(\\beta_2\\) within the diamond that has the lowest loss, which is marked with the red point in the figure above. The global minima without any constraint is marked with “x”.\nThe same visualization but with the L2 constraint t=5, i.e., \\(\\beta_1^2+\\beta_2^2 \\le 5^2\\):\n\nplot3d(Reg.L2, t=5)\n\n\n\n\n\n\n\n\nThe corresponding 2D visualizations that are similar to the ones given by the Elements of Statistical Learning book:\n\n\ncode for plot2d\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n\n\ndef ax2d_init(ax):\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n\ndef plot2d(regs: [Reg], t: float):\n    fig = plt.figure(figsize=(4 * len(regs), 4))\n    axes = fig.subplots(1, len(regs))\n    for ax in axes:\n        ax2d_init(ax)\n    for reg, ax in zip(regs, axes):\n        # draw the regularization safe region\n        ax.add_patch(make_reg_shape(reg=reg, t=t))\n        loss_contour(ax)\n        # draw minima within constraint\n        mx, my = argmin_within_constraint(reg, t)\n        ax.plot([mx], [my], marker='.', markersize=10, color='r')\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot2d([Reg.L1, Reg.L2], t=5)\n\n\n\n\n\n\n\n\n\n\n\nIn both cases (L1, L2), we’re looking for a contour that just touches the constraint region. For example, when we have the following case:\n\n\ncode for plot_reg_and_circle\ndef plot_reg_and_circle(reg: Reg, t: float, cx: float, cy: float, radius: float):\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    ax.add_patch(Circle(xy=(cx, cy), radius=radius, color='b', fill=False))\n    ax.add_patch(make_reg_shape(reg, t=t))\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_reg_and_circle(Reg.L1, t=7, cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nThen it is more optimal to reduce the constraint region (i.e., the diamond) until they touch in a single point as shown below. Note that a contour has the same loss along its points.\n\nplot_reg_and_circle(Reg.L1, t=(np.sqrt(50)-4) * np.sqrt(2), cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nWhen does the Lasso pick the corner of the diamond over any other point on the edges? To simplify our problem, let’s fix a specific circle among the contours: a circle with a fixed radius \\(r\\). Now, we want to come up with a formula that gives the probability of a random tangent circle with radius \\(r\\) touching our diamond at the corners versus at any other points.\nThe above problem becomes much simpler with a visualization:\n\n\ncode for html5_video_l1_tangent_circles\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t + np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop())\n    vertices.reverse()\n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        group = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner:\n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                group.append(corner + vec)\n            else:\n                group.append(v)\n        locations.append(np.array(group))\n    return locations\n\n\ndef l2_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    angles = np.linspace(0, 2 * np.pi, 200)\n    vertices = np.column_stack([np.cos(angles), np.sin(angles)]) * (t + radius) \n    return [vertices]\n\n\ndef animate_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    if reg == Reg.L1:\n        diamond = make_reg_shape(Reg.L1, t=t)\n        circle_locations = l1_tangent_circle_locations(t, radius)\n        plots = []\n        for i, locations in enumerate(circle_locations):\n            color = 'g' if i % 2 == 0 else 'b'\n            plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    else:\n        circle_locations = l2_tangent_circle_locations(t, radius)\n        plots = [ax.plot([], [], color='b', linewidth=2)[0]]\n    \n    circle = Circle(xy=(t + radius, 0), radius=radius, color='g', fill=False)\n    ax.add_patch(circle)\n    ax.add_patch(make_reg_shape(reg, t=t))\n\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\ndef html5_video_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    ani = animate_tangent_circle_trajectories(reg, t=t, radius=radius)\n    html = ani.to_html5_video()\n    plt.close()\n    return html\n\n\n\nfrom IPython.display import HTML\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe animation above illustrates all tangent circle trajectories. The circle is green when it touches the diamond at the corners, and it is blue when it touches at other points. The total length of all blue (touching at other points) trajectories is given by \\(4\\times \\sqrt{2}\\times t\\), while the total length of all green (touching at the corners) trajectories is given by \\(2\\pi \\times r\\), where t is the constraint and r is the radius of the circle. This means that it is likely to touch at the corners when t is sufficiently small or r is sufficiently high. We illustrate one of the such cases below:\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=3, radius=10))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIn Ridge (L2), we don’t have special points like corners, and all points along the disk have the same probability of touching a random tangent circle with radius \\(r\\):\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L2, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\nFeel free to leave feedback on my telegram channel."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the sum of absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the same equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying less important features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "We will use a simple loss function that illustrates circle contours instead of elliptical ones.:\n\\[\nLoss(\\beta_1,\\beta_2 | c_x, c_y)=2{(\\beta_1 - c_x)}^2 + 2{(\\beta_2 - c_y)}^2 + 100\n\\]\nOnce we understand the intution for circles, it is easy to extend to other contours such as elliptical ones. We will use \\(c_x=15\\) and \\(c_y=5\\) most of the time.\n\n\ncode for loss setup and helper functions\nfrom enum import Enum\n\nimport mpl_toolkits.mplot3d.art3d as art3d\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else:\n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n\n\ndef argmin_within_constraint(reg: Reg, t: float):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    if reg == Reg.L1:\n        mask = np.abs(B0) + np.abs(B1) &lt;= t\n    else:\n        mask = B0 * B0 + B1 * B1 &lt;= t * t\n    index = np.argmin(Z[mask])\n    return B0[mask][index], B1[mask][index]\n\n\nbeta_range = -20, 20\ncx, cy = 15, 5\nvmax = 1000\n\n\n\n\ncode for plot3d\ndef base_fig3():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0, 500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n    # draw axes\n    ax.plot(beta_range, [0, 0], color='k')\n    ax.plot([0, 0], beta_range, color='k')\n    return fig, ax\n\n\ndef plot3d(reg: Reg, t=3):\n    fig, ax = base_fig3()\n\n    # surface\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n\n    # contours\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', zdir='z', offset=0, vmax=vmax)\n    \n    # minima within regularization shape\n    mx, my = argmin_within_constraint(reg, t)\n    ax.plot([mx], [my], marker='.', markersize=10, color='r')\n\n    # regularization contraints\n    reg_shape = make_reg_shape(reg, t, color=\"black\")\n    ax.add_patch(reg_shape)\n    art3d.pathpatch_2d_to_3d(reg_shape, z=0)\n\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\n\nLet’s visualize our loss \\(2{(\\beta_1 - 15)}^2 + 2{(\\beta_2 - 5)}^2 + 100\\) with the L1 constaint \\(t=5\\), i.e., \\(|\\beta_1| + |\\beta_2| \\le 5\\) in 3D:\n\nplot3d(Reg.L1, t=5)\n\n\n\n\n\n\n\n\nIn Lasso Regression, we’re looking for \\(\\beta_1\\) and \\(\\beta_2\\) within the diamond that has the lowest loss, which is marked with the red point in the figure above. The global minima without any constraint is marked with “x”.\nThe same visualization but with the L2 constraint t=5, i.e., \\(\\beta_1^2+\\beta_2^2 \\le 5^2\\):\n\nplot3d(Reg.L2, t=5)\n\n\n\n\n\n\n\n\nThe corresponding 2D visualizations that are similar to the ones given by the Elements of Statistical Learning book:\n\n\ncode for plot2d\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n\n\ndef ax2d_init(ax):\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n\ndef plot2d(regs: [Reg], t: float):\n    fig = plt.figure(figsize=(4 * len(regs), 4))\n    axes = fig.subplots(1, len(regs))\n    for ax in axes:\n        ax2d_init(ax)\n    for reg, ax in zip(regs, axes):\n        # draw the regularization safe region\n        ax.add_patch(make_reg_shape(reg=reg, t=t))\n        loss_contour(ax)\n        # draw minima within constraint\n        mx, my = argmin_within_constraint(reg, t)\n        ax.plot([mx], [my], marker='.', markersize=10, color='r')\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot2d([Reg.L1, Reg.L2], t=5)\n\n\n\n\n\n\n\n\n\n\n\nIn both cases (L1, L2), we’re looking for a contour that just touches the constraint region. For example, when we have the following case:\n\n\ncode for plot_reg_and_circle\ndef plot_reg_and_circle(reg: Reg, t: float, cx: float, cy: float, radius: float):\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    ax.add_patch(Circle(xy=(cx, cy), radius=radius, color='b', fill=False))\n    ax.add_patch(make_reg_shape(reg, t=t))\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_reg_and_circle(Reg.L1, t=7, cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nThen it is more optimal to reduce the constraint region (i.e., the diamond) until they touch in a single point as shown below. Note that a contour has the same loss along its points.\n\nplot_reg_and_circle(Reg.L1, t=(np.sqrt(50)-4) * np.sqrt(2), cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nWhen does the Lasso pick the corner of the diamond over any other point on the edges? To simplify our problem, let’s fix a specific circle among the contours: a circle with a fixed radius \\(r\\). Now, we want to come up with a formula that gives the probability of a random tangent circle with radius \\(r\\) touching our diamond at the corners versus at any other points.\nThe above problem becomes much simpler with a visualization:\n\n\ncode for html5_video_l1_tangent_circles\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t + np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop())\n    vertices.reverse()\n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        group = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner:\n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                group.append(corner + vec)\n            else:\n                group.append(v)\n        locations.append(np.array(group))\n    return locations\n\n\ndef l2_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    angles = np.linspace(0, 2 * np.pi, 200)\n    vertices = np.column_stack([np.cos(angles), np.sin(angles)]) * (t + radius) \n    return [vertices]\n\n\ndef animate_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    if reg == Reg.L1:\n        diamond = make_reg_shape(Reg.L1, t=t)\n        circle_locations = l1_tangent_circle_locations(t, radius)\n        plots = []\n        for i, locations in enumerate(circle_locations):\n            color = 'g' if i % 2 == 0 else 'b'\n            plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    else:\n        circle_locations = l2_tangent_circle_locations(t, radius)\n        plots = [ax.plot([], [], color='b', linewidth=2)[0]]\n    \n    circle = Circle(xy=(t + radius, 0), radius=radius, color='g', fill=False)\n    ax.add_patch(circle)\n    ax.add_patch(make_reg_shape(reg, t=t))\n\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\ndef html5_video_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    ani = animate_tangent_circle_trajectories(reg, t=t, radius=radius)\n    html = ani.to_html5_video()\n    plt.close()\n    return html\n\n\n\nfrom IPython.display import HTML\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe animation above illustrates all tangent circle trajectories. The circle is green when it touches the diamond at the corners, and it is blue when it touches at other points. The total length of all blue (touching at other points) trajectories is given by \\(4\\times \\sqrt{2}\\times t\\), while the total length of all green (touching at the corners) trajectories is given by \\(2\\pi \\times r\\), where t is the constraint and r is the radius of the circle. This means that it is likely to touch at the corners when t is sufficiently small or r is sufficiently high. We illustrate one of the such cases below:\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=3, radius=10))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIn Ridge (L2), we don’t have special points like corners, and all points along the disk have the same probability of touching a random tangent circle with radius \\(r\\):\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L2, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#end",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#end",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper. Feel free to ask questions on my telegram channel\n\n\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates.\n\n\n\n\n\n\nThe forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "WIP: Geometric Intuition for Jensen’s Inequality\n\n\n\n\n\n\nbasics\n\n\nloss\n\n\nl1\n\n\nl2\n\n\nlasso\n\n\nridge\n\n\n\n\n\n\n\n\n\nJan 4, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\n\n\n\n\nbasics\n\n\nloss\n\n\nl1\n\n\nl2\n\n\nlasso\n\n\nridge\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\n\n\n\n\nstable\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes Notes\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\nautoencoder\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  }
]