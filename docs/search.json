[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper.\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noise with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and big enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder models."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noise with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and big enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "Denoising Diffusion Probabilistic Models\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  }
]