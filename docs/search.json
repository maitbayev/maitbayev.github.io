[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Engineer at Snap Inc, ex-Google."
  },
  {
    "objectID": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "href": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "title": "WIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes",
    "section": "",
    "text": "Notes for “High-Resolution Image Synthesis with Latent Diffusion Models”\nMy notes for the “High-Resolution Image Synthesis with Latent Diffusion Models” paper. Feel free to ask questions on my telegram channel"
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper. Feel free to ask questions on my telegram channel\n\n\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nProof 1: Using Jensen’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nNoteProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates.\n\n\n\n\n\n\nThe forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nProof 1: Using Jensen’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nNoteProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/linear-equation/index.html",
    "href": "posts/linear-equation/index.html",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "A linear equation is an equation in the form \\(a_1x_1+\\cdots+a_nx_n+b=0\\). In two dimensions, it’s an equation of a line \\(ax+by+c=0\\), while in three dimensions, it’s an equation of a plane \\(ax+by+cz+d=0\\).\nA linear equation is the foundation of linear models in machine learning, such as linear regression and logistic regression.\nIn this post, let’s understand the intuitions behind the linear equation, such as:\n\nWhy \\(ax+by+c\\) is the distance between the point \\((x, y)\\) and the line?\nWhat are the meanings of \\((a, b)\\) and \\(c\\)?\nHow are the dot product and linear equation related?\n\nMy previous post about the dot product is a required prerequisite to this post. You need to have a good intuition about the dot product.\n\n\n\nA short summary for people in hurry.\nThe distance from the point (x, y) to the line is: \\[\n\\left|\\frac{ax+by+c}{\\sqrt{a^2+b^2}}\\right|\n\\]\nin n-dimensions:\n\\[\n\\left|\\frac{a_1x_1+\\cdots+a_nx_n+b}{\\sqrt{a_1^1+\\cdots+a_n^2}}\\right|\n\\]\nThe vector \\(\\textbf{n}=[a, b]\\) is the orthogonal vector to the line \\(ax+by+c=0\\), and \\(\\textbf{n}=[a_1, a_2, \\cdots, a_n]\\) in n-dimensions.\nThe line equation can be expressed with the dot product as \\(\\textbf{n} \\cdot {[x, y]}+c=0\\) and \\(\\textbf{n} \\cdot \\textbf{x} + b = 0\\) in n-dimensions.\n\n\n\n\n\n\nLet’s start with a simpler equation of a line in the form of \\(ax+by=0\\), where we have \\(c=0\\). Does it remind you something ? Yes, the dot product between \\((a, b)\\) and \\((x, y)\\). The dot product \\((a, b) \\cdot (x, y)\\) is zero when the two vectors are orthogonal. Oh nice, this means that (a, b) is an orthogonal vector to our line. An orthogonal vector is also known as a normal vector of a line. Also, notice that the simpler line contains the origin since \\(a\\cdot0+b\\cdot0=0\\) holds true.\nLet’s visualize our setup:\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5, grid=false) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: grid,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board1() {\n  const board = init_board(\"board1\", /*extend=*/10, /*grid=*/true);\n  const pointN = board.create(\"point\", [3, 3], {\n    name: \"n\",\n    color: \"red\",\n    snapToGrid: true,\n  })\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n  })\n  const line = board.create(\"line\", [function() {\n    return 0;\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true\n  });\n  const lineText = board.create(\"text\", [-8, -8, function() {\n    return pointN.X() + \"x + \" + pointN.Y() + \"y = 0\";\n  }], {\n    fontSize: 19\n  });\n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board1\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board1()\n\n\n\n\n\n\nWe have the following items visualized:\n\nOur line \\(ax+by=0\\) is visualized with blue\n\nIt passes through the origin\n\nA normal vector of the line \\(\\textbf{n}=[a, b]\\) is visualized with red\nNote that multiple \\(\\textbf{n}\\) vectors can represent the same line\n\nFeel free to play (by moving) with the point \\(\\textbf{n}\\) and notice how the line equation is changing with respect to the normal vector.\nWe can multiply or divide the line equation by any real number \\(k\\) without changing the equation, in other words, \\(kax+kby+kc=0\\) and \\(ax+by+c=0\\) hold true to the same set of points. Let’s use this property and divide the equation by \\(\\sqrt{a^2+b^2}\\), since it gives us \\(a^2+b^2=1\\) and \\(\\|\\textbf{n}\\|=1\\).\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\nWe obtain a new normalized line equation with the coefficients: \\[\n\\begin{align}\na_{new}&=\\frac{a}{\\sqrt{a^2+b^2}}\\\\\nb_{new}&=\\frac{b}{\\sqrt{a^2+b^2}}\\\\\nc_{new}&=\\frac{c}{\\sqrt{a^2+b^2}}\n\\end{align}\n\\]\nNote that \\(a_{new}^2+b_{new}^2=1\\) holds true.\n\n\n\nFrom now on, we assume that \\(\\|\\textbf{n}\\|=1\\). Let’s check what we have:\n\nfunction make_board(name, props = {}) {\n  function rad(deg) {\n    return deg / 180 * Math.PI;\n  }\n  function vec_scale(v, s) {\n    return [v[0] * s, v[1] * s];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  const showPoint = props.showPoint || false\n  const board = init_board(name, /*extend=*/4.5);\n \n  const sliderAlpha = board.create(\"slider\", [[-1, -2.5], [1, -2.5], [0, 45, 360]], {\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const sliderC = board.create(\"slider\", [[-1, -3.5], [1, -3.5], [-4, 0, 4]], {\n    name: \"c\",\n    snapWidth: 0.1,\n    visible: props.showC || false\n  });\n  const pointN = board.create(\"point\", [function() {\n    return Math.cos(rad(sliderAlpha.Value()));\n  }, function() {\n    return Math.sin(rad(sliderAlpha.Value()));\n  }], {\n    visible: true,\n    name: \"n\",\n    color: \"#00000000\",\n    fixed: true,\n    highlight: false\n  });\n  const line = board.create(\"line\", [function() {\n    return sliderC.Value();\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true,\n    highlight: false\n  });\n  const pointP = board.create(\"point\", [3, 0], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPoint || false \n  });\n  const gliderP = board.create(\"glider\", [1, -1, line], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPointGlider || false,\n  });\n  const pointProj = board.create(\"point\", [function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[0];\n  }, function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[1];\n  }], {\n    visible: false\n  });\n  const lineProj = board.create(\"line\", [pointP, pointProj], {\n    color: \"gray\",\n    strokeWidth: 1,\n    fixed: true,\n    dash: 2,\n    straightFirst:false, \n    straightLast:false,\n    visible: showPoint \n  });\n  const normalAxis = board.create('axis', [[0, 0], pointN], {\n    visible: props.showNormalAxis || false,\n    ticks: {\n      majorHeight: 7,\n      minorHeight: 0,\n    },\n    needsRegularUpdate: true\n  });\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst: false, \n    straightLast: false, \n    lastArrow: true,\n    highlight: false\n  });\n  \n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board2\")\n\n\n\n\n\n\nWe introduced a slider that defines the angle of the normal vector \\(\\textbf{n}\\). We also restricted \\(\\textbf{n}\\) so that \\(\\|\\textbf{n}\\|=1\\). Now, our line is uniquely defined by \\(\\textbf{n}\\), which was not case before.\nLet’s introduce a point \\(\\textbf{p}=[x, y]\\) on the line:\n\nboard_div(\"board3\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board3\", {\n  showPointGlider: true,\n})\n\n\n\n\n\n\nTry moving the new green point! The point \\(\\textbf{p}=[x, y]\\) always lies on the line, in other words:\n\n\\(ax+by=0\\), where \\(\\textbf{n}=[a, b]\\)\nEquivalently \\(\\textbf{n} \\cdot \\textbf{p} = 0\\)\nThe angle defined by the points \\((\\textbf{n}, \\text{origin}, \\textbf{p})\\) is a right angle.\n\nWe have learned so far that \\(a\\) and \\(b\\) represent the normal vector \\(\\textbf{n}=[a, b]\\), and we can express the line equation with the dot product \\(\\textbf{n} \\cdot \\textbf{p} = 0\\).\nWhat is the meaning of the line equation when the point \\(\\textbf{p}\\) is outside of the line, i.e., when \\(ax+by \\neq 0\\)? Spoiler: the value is the signed distance from the point \\(\\textbf{p}\\) to our line. Let’s understand this intuitively.\nThe dot product \\(\\textbf{n} \\cdot \\textbf{p}\\) is a non-zero value when the point \\(\\textbf{p}\\) is not on the line. From the previous post we know that the dot product is the projection of the vector \\(\\textbf{p}\\) to the vector \\(\\textbf{n}\\). Imagine a 1D number line spanned by the vector \\(\\textbf{n}\\) like this:\n\nboard_div(\"board4\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board4\", {\n  showPoint: true,\n  showNormalAxis: true, \n})\n\n\n\n\n\n\nThe introduced 1D number line has ticks that represent the signed distances from our line. The projection of the point \\(\\textbf{p}\\) falls somewhere on that 1D number line (following the dotted line segment), which corresponds to the dot product. Remember that \\(ax+by\\) and \\(\\textbf{n} \\cdot \\textbf{p}\\) are the same. Play with the point \\(\\textbf{p}\\) (and with the slider) to understand when \\(ax+by\\) is zero, positive and negative. Can you see that \\(ax+by\\) (and \\(\\textbf{n} \\cdot \\textbf{p}\\)) represents the signed distance from the point \\(\\textbf{p}\\) to our line?\n\n\n\nBy this point, you need to understand the line equation of the form \\(ax+by=0\\). Now, let’s explore the line equation of the form \\(ax+by+c=0\\)!\nWe represented \\(ax+by\\) with the dot product \\(\\textbf{n} \\cdot \\textbf{p}\\), similarly \\(ax+by+c\\) can be represented with \\(\\textbf{n} \\cdot \\textbf{p}+c\\). First, we land on the 1D number line with \\(\\textbf{n} \\cdot \\textbf{p}\\), then move \\(c\\) steps forward on that 1D number line. It is easier explained with a visualization:\n\nboard_div(\"board5\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board5\", {\n  showPoint: true,\n  showNormalAxis: true,\n  showC: true,\n})\n\n\n\n\n\n\nPlay with the slider for \\(c\\), and notice the location of our line when \\(c=1\\) and \\(c=-1\\).\nCan you see that \\(ax+by+c\\) is still the signed distance from the point \\(\\textbf{p}\\) to our line? What does \\(c\\) represent?\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\nFirst, we land on the 1D number line with \\(c'=\\textbf{n} \\cdot \\textbf{p}\\). Then, let’s denote \\(ax+by+c\\) as \\(c'+c\\). The \\(c'+c\\) is zero when \\(c'=-c\\); \\(1\\) when \\(c'=-c+1\\); \\(-1\\) when \\(c'=-c-1\\); and so on.\n\\(|c|\\) is the distance from the origin to our line, c is the signed distance. The signed distance is positive in the direction of the normal vector, and negative in the opposite direction.\n\n\n\n\n\n\n\nI hope you enjoyed this post. Subscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/linear-equation/index.html#introduction",
    "href": "posts/linear-equation/index.html#introduction",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "A linear equation is an equation in the form \\(a_1x_1+\\cdots+a_nx_n+b=0\\). In two dimensions, it’s an equation of a line \\(ax+by+c=0\\), while in three dimensions, it’s an equation of a plane \\(ax+by+cz+d=0\\).\nA linear equation is the foundation of linear models in machine learning, such as linear regression and logistic regression.\nIn this post, let’s understand the intuitions behind the linear equation, such as:\n\nWhy \\(ax+by+c\\) is the distance between the point \\((x, y)\\) and the line?\nWhat are the meanings of \\((a, b)\\) and \\(c\\)?\nHow are the dot product and linear equation related?\n\nMy previous post about the dot product is a required prerequisite to this post. You need to have a good intuition about the dot product."
  },
  {
    "objectID": "posts/linear-equation/index.html#cheatsheet",
    "href": "posts/linear-equation/index.html#cheatsheet",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "A short summary for people in hurry.\nThe distance from the point (x, y) to the line is: \\[\n\\left|\\frac{ax+by+c}{\\sqrt{a^2+b^2}}\\right|\n\\]\nin n-dimensions:\n\\[\n\\left|\\frac{a_1x_1+\\cdots+a_nx_n+b}{\\sqrt{a_1^1+\\cdots+a_n^2}}\\right|\n\\]\nThe vector \\(\\textbf{n}=[a, b]\\) is the orthogonal vector to the line \\(ax+by+c=0\\), and \\(\\textbf{n}=[a_1, a_2, \\cdots, a_n]\\) in n-dimensions.\nThe line equation can be expressed with the dot product as \\(\\textbf{n} \\cdot {[x, y]}+c=0\\) and \\(\\textbf{n} \\cdot \\textbf{x} + b = 0\\) in n-dimensions."
  },
  {
    "objectID": "posts/linear-equation/index.html#equation-of-a-line",
    "href": "posts/linear-equation/index.html#equation-of-a-line",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "Let’s start with a simpler equation of a line in the form of \\(ax+by=0\\), where we have \\(c=0\\). Does it remind you something ? Yes, the dot product between \\((a, b)\\) and \\((x, y)\\). The dot product \\((a, b) \\cdot (x, y)\\) is zero when the two vectors are orthogonal. Oh nice, this means that (a, b) is an orthogonal vector to our line. An orthogonal vector is also known as a normal vector of a line. Also, notice that the simpler line contains the origin since \\(a\\cdot0+b\\cdot0=0\\) holds true.\nLet’s visualize our setup:\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5, grid=false) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: grid,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board1() {\n  const board = init_board(\"board1\", /*extend=*/10, /*grid=*/true);\n  const pointN = board.create(\"point\", [3, 3], {\n    name: \"n\",\n    color: \"red\",\n    snapToGrid: true,\n  })\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n  })\n  const line = board.create(\"line\", [function() {\n    return 0;\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true\n  });\n  const lineText = board.create(\"text\", [-8, -8, function() {\n    return pointN.X() + \"x + \" + pointN.Y() + \"y = 0\";\n  }], {\n    fontSize: 19\n  });\n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board1\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board1()\n\n\n\n\n\n\nWe have the following items visualized:\n\nOur line \\(ax+by=0\\) is visualized with blue\n\nIt passes through the origin\n\nA normal vector of the line \\(\\textbf{n}=[a, b]\\) is visualized with red\nNote that multiple \\(\\textbf{n}\\) vectors can represent the same line\n\nFeel free to play (by moving) with the point \\(\\textbf{n}\\) and notice how the line equation is changing with respect to the normal vector.\nWe can multiply or divide the line equation by any real number \\(k\\) without changing the equation, in other words, \\(kax+kby+kc=0\\) and \\(ax+by+c=0\\) hold true to the same set of points. Let’s use this property and divide the equation by \\(\\sqrt{a^2+b^2}\\), since it gives us \\(a^2+b^2=1\\) and \\(\\|\\textbf{n}\\|=1\\).\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\nWe obtain a new normalized line equation with the coefficients: \\[\n\\begin{align}\na_{new}&=\\frac{a}{\\sqrt{a^2+b^2}}\\\\\nb_{new}&=\\frac{b}{\\sqrt{a^2+b^2}}\\\\\nc_{new}&=\\frac{c}{\\sqrt{a^2+b^2}}\n\\end{align}\n\\]\nNote that \\(a_{new}^2+b_{new}^2=1\\) holds true.\n\n\n\nFrom now on, we assume that \\(\\|\\textbf{n}\\|=1\\). Let’s check what we have:\n\nfunction make_board(name, props = {}) {\n  function rad(deg) {\n    return deg / 180 * Math.PI;\n  }\n  function vec_scale(v, s) {\n    return [v[0] * s, v[1] * s];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  const showPoint = props.showPoint || false\n  const board = init_board(name, /*extend=*/4.5);\n \n  const sliderAlpha = board.create(\"slider\", [[-1, -2.5], [1, -2.5], [0, 45, 360]], {\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const sliderC = board.create(\"slider\", [[-1, -3.5], [1, -3.5], [-4, 0, 4]], {\n    name: \"c\",\n    snapWidth: 0.1,\n    visible: props.showC || false\n  });\n  const pointN = board.create(\"point\", [function() {\n    return Math.cos(rad(sliderAlpha.Value()));\n  }, function() {\n    return Math.sin(rad(sliderAlpha.Value()));\n  }], {\n    visible: true,\n    name: \"n\",\n    color: \"#00000000\",\n    fixed: true,\n    highlight: false\n  });\n  const line = board.create(\"line\", [function() {\n    return sliderC.Value();\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true,\n    highlight: false\n  });\n  const pointP = board.create(\"point\", [3, 0], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPoint || false \n  });\n  const gliderP = board.create(\"glider\", [1, -1, line], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPointGlider || false,\n  });\n  const pointProj = board.create(\"point\", [function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[0];\n  }, function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[1];\n  }], {\n    visible: false\n  });\n  const lineProj = board.create(\"line\", [pointP, pointProj], {\n    color: \"gray\",\n    strokeWidth: 1,\n    fixed: true,\n    dash: 2,\n    straightFirst:false, \n    straightLast:false,\n    visible: showPoint \n  });\n  const normalAxis = board.create('axis', [[0, 0], pointN], {\n    visible: props.showNormalAxis || false,\n    ticks: {\n      majorHeight: 7,\n      minorHeight: 0,\n    },\n    needsRegularUpdate: true\n  });\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst: false, \n    straightLast: false, \n    lastArrow: true,\n    highlight: false\n  });\n  \n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board2\")\n\n\n\n\n\n\nWe introduced a slider that defines the angle of the normal vector \\(\\textbf{n}\\). We also restricted \\(\\textbf{n}\\) so that \\(\\|\\textbf{n}\\|=1\\). Now, our line is uniquely defined by \\(\\textbf{n}\\), which was not case before.\nLet’s introduce a point \\(\\textbf{p}=[x, y]\\) on the line:\n\nboard_div(\"board3\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board3\", {\n  showPointGlider: true,\n})\n\n\n\n\n\n\nTry moving the new green point! The point \\(\\textbf{p}=[x, y]\\) always lies on the line, in other words:\n\n\\(ax+by=0\\), where \\(\\textbf{n}=[a, b]\\)\nEquivalently \\(\\textbf{n} \\cdot \\textbf{p} = 0\\)\nThe angle defined by the points \\((\\textbf{n}, \\text{origin}, \\textbf{p})\\) is a right angle.\n\nWe have learned so far that \\(a\\) and \\(b\\) represent the normal vector \\(\\textbf{n}=[a, b]\\), and we can express the line equation with the dot product \\(\\textbf{n} \\cdot \\textbf{p} = 0\\).\nWhat is the meaning of the line equation when the point \\(\\textbf{p}\\) is outside of the line, i.e., when \\(ax+by \\neq 0\\)? Spoiler: the value is the signed distance from the point \\(\\textbf{p}\\) to our line. Let’s understand this intuitively.\nThe dot product \\(\\textbf{n} \\cdot \\textbf{p}\\) is a non-zero value when the point \\(\\textbf{p}\\) is not on the line. From the previous post we know that the dot product is the projection of the vector \\(\\textbf{p}\\) to the vector \\(\\textbf{n}\\). Imagine a 1D number line spanned by the vector \\(\\textbf{n}\\) like this:\n\nboard_div(\"board4\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board4\", {\n  showPoint: true,\n  showNormalAxis: true, \n})\n\n\n\n\n\n\nThe introduced 1D number line has ticks that represent the signed distances from our line. The projection of the point \\(\\textbf{p}\\) falls somewhere on that 1D number line (following the dotted line segment), which corresponds to the dot product. Remember that \\(ax+by\\) and \\(\\textbf{n} \\cdot \\textbf{p}\\) are the same. Play with the point \\(\\textbf{p}\\) (and with the slider) to understand when \\(ax+by\\) is zero, positive and negative. Can you see that \\(ax+by\\) (and \\(\\textbf{n} \\cdot \\textbf{p}\\)) represents the signed distance from the point \\(\\textbf{p}\\) to our line?\n\n\n\nBy this point, you need to understand the line equation of the form \\(ax+by=0\\). Now, let’s explore the line equation of the form \\(ax+by+c=0\\)!\nWe represented \\(ax+by\\) with the dot product \\(\\textbf{n} \\cdot \\textbf{p}\\), similarly \\(ax+by+c\\) can be represented with \\(\\textbf{n} \\cdot \\textbf{p}+c\\). First, we land on the 1D number line with \\(\\textbf{n} \\cdot \\textbf{p}\\), then move \\(c\\) steps forward on that 1D number line. It is easier explained with a visualization:\n\nboard_div(\"board5\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board5\", {\n  showPoint: true,\n  showNormalAxis: true,\n  showC: true,\n})\n\n\n\n\n\n\nPlay with the slider for \\(c\\), and notice the location of our line when \\(c=1\\) and \\(c=-1\\).\nCan you see that \\(ax+by+c\\) is still the signed distance from the point \\(\\textbf{p}\\) to our line? What does \\(c\\) represent?\n\n\n\n\n\n\nNoteExplanation\n\n\n\n\n\nFirst, we land on the 1D number line with \\(c'=\\textbf{n} \\cdot \\textbf{p}\\). Then, let’s denote \\(ax+by+c\\) as \\(c'+c\\). The \\(c'+c\\) is zero when \\(c'=-c\\); \\(1\\) when \\(c'=-c+1\\); \\(-1\\) when \\(c'=-c-1\\); and so on.\n\\(|c|\\) is the distance from the origin to our line, c is the signed distance. The signed distance is positive in the direction of the normal vector, and negative in the opposite direction."
  },
  {
    "objectID": "posts/linear-equation/index.html#the-end",
    "href": "posts/linear-equation/index.html#the-end",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "I hope you enjoyed this post. Subscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/random-two-vectors/index.html",
    "href": "posts/random-two-vectors/index.html",
    "title": "Why are two random vectors near orthogonal in high dimensions?",
    "section": "",
    "text": "High-dimensional embedding vectors are fundamental building blocks in Machine Learning, particularly in transformers or word2vec. Typically, two vectors that are semantically similar point in roughly the same direction; if they are entirely dissimilar, they point in opposite directions; and if they’re nearly orthogonal, they are unrelated.\nWe usually think in two or three dimensions, but there are some unintuitive properties that only apply in higher dimensions. For example, two random vectors are expected to be near orthogonal in high dimensions. Intuitively, it makes sense for word2vec, as we expect that two words are unrelated in most instances.\nIn this post, I will explain why two random vectors are expected to be nearly orthogonal in high dimensions.\nSubscribe to get a notification about future posts.\n\n\n\n\nfunction makeCircle2D(vecU, vecV, height=200) {\n  const svg = d3.create('svg')\n    .attr('viewBox', [0, 0, width, height]);\n  \n  function appendArrow(svg, v, color, id) {\n    svg\n      .append('defs')\n      .append('marker')\n      .attr('id', id)\n      .attr('viewBox', [0, 0, 20, 20])\n      .attr('refX', 20)\n      .attr('refY', 10)\n      .attr('markerWidth', 10)\n      .attr('markerHeight', 10)\n      .attr('orient', 'auto-start-reverse')\n      .append('path')\n      .attr('d', d3.line()([[0, 0], [0, 20], [20, 10]]))\n      .attr('fill', color);\n    svg\n      .append('path')\n      .attr('d', d3.line()(\n        [[width / 2, height / 2], [width / 2 + v[0], height / 2 + v[1]]]\n      ))\n      .attr('stroke', color)\n      .attr('stroke-width', '2px')\n      .attr('marker-end', 'url(#' + id + ')')\n      .attr('fill', 'none');\n  }\n  const radius = Math.min(width, height) / 2 - 4;\n  vecU = [vecU[0] * radius, vecU[1] * radius];\n  vecV = [vecV[0] * radius, vecV[1] * radius];\n  svg\n    .append('circle')\n    .attr('cx', '50%')\n    .attr('cy', '50%')\n    .attr('r', radius)\n    .style('stroke', 'black')\n    .style('stroke-width', '2px')\n    .style('fill', 'none');\n\n  appendArrow(svg, vecU, \"black\", \"arrow-0\");\n  appendArrow(svg, vecV, \"red\", \"arrow-1\");\n  return svg.node();\n}\n\n\n\n\n\n\nThis problem is equivalent to selecting two random vectors u and v on a circle of radius 1 and computing their dot product.\n\nmakeCircle2D([0, 1], [1 / Math.sqrt(2), 1 / Math.sqrt(2)])\n\n\n\n\n\n\nAs a reminder the dot product of u and v is:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_x v_x + u_y v_y = \\cos (\\alpha)\n\\]\nThe dot product is zero when u and v are orthogonal, and near zero when they are nearly orthogonal.\nThe dot product is invariant under rotations, which means that we can rotate both vectors in the same way such that the vector u is [0, 1]:\n\nmakeCircle2D([0, -1], [-1 / Math.sqrt(2), -1 / Math.sqrt(2)])\n\n\n\n\n\n\nThis simplifies the dot product to \\(\\mathbf{u} \\cdot \\mathbf{v} = \\cos (\\alpha) = v_y\\). The probability of being near orthogonal is pretty low in two dimensions, but this helps us in framing our problem for higher dimensions.\nIt turns out that the average value of \\(\\cos^2(\\alpha)\\) or \\(v_y^2\\) is exactly \\(\\frac{1}{2}\\). There are numerous analytical proofs, but the simplest intuition is derived from \\(v_x^2+v_y^2=1\\). We have a total budget of 1, which we distribute between \\(v_x^2\\) and \\(v_y^2\\); thus, on average, \\(v_y^2\\) receives half of the budget.\nWe only have two orthogonal vectors to u, and they are:\n\nhtml`\n&lt;div style=\"display: flex; justify-content: center;\"&gt;\n  &lt;div style=\"width: 50%;\"&gt;\n    ${makeCircle2D([0, -1], [-1, 0], 400)}\n  &lt;/div&gt;\n  &lt;div style=\"width: 50%;\"&gt;\n    ${makeCircle2D([0, -1], [1, 0], 400)}\n  &lt;/div&gt;\n&lt;/div&gt;\n`\n\n\n\n\n\n\n\n\n\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.145.0/build/three.min.js\");\n  await require(\"three@0.145.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  return THREE;\n}\n\n\n\n\n\n\n\nfunction makeSphere(wireframe = false, opacity = 1) {\n  const material = new THREE.MeshBasicMaterial({\n    wireframe: wireframe, \n    opacity: opacity, \n    color: 0x555555, \n    transparent: true  \n  });\n  const geometry = new THREE.SphereGeometry(1, 32, 32 ); \n  return new THREE.Mesh(geometry, material);\n}\n\n\n\n\n\n\n\nfunction makeSphereSlice(wireframe = true, opacity = 1) {\n  const material = new THREE.MeshBasicMaterial({\n    wireframe: wireframe, \n    opacity: opacity, \n    color: 0xff0000, \n    transparent: true,\n    side: THREE.DoubleSide, \n    depthTest: false, \n  });\n  const geometry = new THREE.SphereGeometry(\n    1.01, 32, 32,\n    0, Math.PI * 2.0,\n    Math.PI / 2 - Math.PI / 32, Math.PI / 16\n  ); \n  return new THREE.Mesh(geometry, material);\n}\n\n\n\n\n\n\n\nfunction makeCircle3D(color, radius=1) {\n  const geometry = new THREE.CircleGeometry(radius, 32); \n  // radius^2+z^2=1\n  // z=sqrt(1-radius^2)\n  const material = new THREE.MeshStandardMaterial({ \n    color: color,\n    side: THREE.DoubleSide,\n    opacity: 0.3,\n    transparent: true,\n    depthTest: false, \n  }); \n  const circle = new THREE.Mesh( geometry, material );\n  circle.position.y = Math.sqrt(1 - radius * radius);\n  circle.lookAt(0, 1, 0);\n  return circle;\n}\n\n\n\n\n\n\n\nfunction makeVector(vec, color) {\n  function customArrow( fx, fy, fz, ix, iy, iz, thickness, color) {\n    const ARROW_BODY = new THREE.CylinderGeometry( 1, 1, 1, 12 )\n                            .rotateX( Math.PI/2)\n                            .translate( 0, 0, 0.5 );\n\n    const ARROW_HEAD = new THREE.ConeGeometry( 1, 1, 12 )\n                            .rotateX( Math.PI/2)\n                            .translate( 0, 0, -0.5 );\n    var material = new THREE.MeshLambertMaterial( {color: color} );\n    \n    var length = Math.sqrt( (ix-fx)**2 + (iy-fy)**2 + (iz-fz)**2 );\n    \n    var body = new THREE.Mesh( ARROW_BODY, material );\n    body.scale.set( thickness, thickness, length-10*thickness );\n      \n    var head = new THREE.Mesh( ARROW_HEAD, material );\n    head.position.set( 0, 0, length );\n    head.scale.set( 3*thickness, 3*thickness, 10*thickness );\n    \n    var arrow = new THREE.Group( );\n    arrow.position.set( ix, iy, iz );\n    arrow.lookAt( fx, fy, fz ); \n    arrow.add( body, head );\n    return arrow;\n  }\n  return customArrow(vec[0], vec[1], vec[2], 0, 0, 0, 0.02, color);\n}\n\n\n\n\n\n\n\nfunction makeScene(vec) {\n  const scene = new THREE.Scene();\n  scene.background = new THREE.Color(0xffffff);\n  scene.add(new THREE.AxesHelper(1.5));\n  scene.add(makeSphere(/*wireframe: */false, /*opacity: */0.1));\n  scene.add(makeSphere(/*wireframe: */true, /*opacity: */1));\n  const light = new THREE.AmbientLight(0xFFFFFF,  /*intensity=*/1);\n  scene.add(light);\n  scene.add(makeVector([0, 1, 0], 0x000000));\n  scene.add(makeVector(vec, 0xff0000));\n  return scene;\n}\n\n\n\n\n\n\n\nfunction makeRendererElement(height, scene) {\n  function makeCamera() {\n    const fov = 45;\n    const aspect = width / height;\n    const near = 1;\n    const far = 1000;\n    var camera = new THREE.PerspectiveCamera(fov, aspect, near, far);\n    camera.position.z = 3;\n    return camera;\n  }\n  const camera = makeCamera();\n  const renderer = new THREE.WebGLRenderer({antialias: true});\n  const controls = new THREE.OrbitControls(camera, renderer.domElement);\n  controls.autoRotate = false;\n  controls.enableZoom = false;\n  function animate() {\n    requestAnimationFrame( animate );\n    controls.update();\n    renderer.render( scene, camera );\n  }\n  animate();\n  invalidation.then(() =&gt; (controls.dispose(), renderer.dispose()));\n  renderer.setSize(width, height);\n  renderer.setPixelRatio(devicePixelRatio);\n  renderer.render(scene, camera)\n  // controls.addEventListener(\"change\", () =&gt; renderer.render(scene, camera));\n  return renderer.domElement\n}\n\n\n\n\n\n\nIn three dimensions, we take two random vectors u and v on a unit sphere and rotate them together so that the vector u becomes the north pole ([0, 0, 1]):\n\n{ \n  const scene = makeScene([1 / Math.sqrt(3), 1 / Math.sqrt(3), 1 / Math.sqrt(3)]);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nTip: The spheres in this post are interactive.\nThe dot product of \\(\\mathbf{u}=[0, 0, 1]\\) and \\(\\mathbf{v}=[v_x, v_y, v_z]\\) is:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\cos (\\alpha) = v_z\n\\]\nIn 2D, we only had two vectors that are orthogonal to u. In 3D, we have an entire subspace of vectors that are orthogonal to u:\n\n{ \n  const scene = makeScene([1 / Math.sqrt(2), 0, 1 / Math.sqrt(2)]);\n  scene.add(makeCircle3D(0xff0000));\n  scene.rotateX(Math.PI / 10);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nIn other words, any vector in the red circle above is orthogonal to u. Moreover, this circle is the largest one that can be found on the sphere. For example, compare with a smaller circle which spans non-orthogonal vectors:\n\n{ \n  const vec = [0.7, Math.sqrt(1 - 0.7 * 0.7), 0];\n  const scene = makeScene(vec);\n  scene.add(makeCircle3D(0xff0000, 0.7));\n  scene.rotateX(Math.PI / 8);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nThe larger the circle from which we select the vector v is, the closer it is to being orthogonal:\n\n{ \n  const vec = [1 / Math.sqrt(2), 0, 1 / Math.sqrt(2)];\n  const scene = makeScene(vec);\n  scene.add(makeSphereSlice(false, 0.3));\n  // scene.add(makeCircle3D(0xff0000, 1));\n  scene.rotateX(Math.PI / 8);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nFor example, we can consider all vectors within the thin red stripe to be nearly orthogonal.\nThe average value of \\(\\cos^2 (\\alpha)\\) or \\(v_z^2\\) is \\(\\dfrac{1}{3}\\). Similar to the 2D case, this comes from the fact that \\(v_x^2+v_y^2+v_z^2=1\\). With a total budget of 1 split among \\(v_x^2\\), \\(v_y^2\\) and \\(v_z^2\\), each gets, on average, one-third of it.\n\n\n\nVisualizing an N-dimensional sphere is challenging. However, we have all the tools needed to develop an intuition.\nSimilar to the 2D and 3D cases, we can fix the first vector as \\(\\mathbf{u}=[0, 0, \\dots, 1]\\). The second vector \\(\\mathbf{v}=[v_1, v_2, \\cdots, v_n]\\) is randomly chosen from an N-dimensional unit sphere. Their dot product is then given by:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\cos (\\alpha) = v_n\n\\]\nThe average value of \\(\\cos^2 (\\alpha)\\) or \\(v_n^2\\) is \\(\\dfrac{1}{n}\\). Since \\(v_1^2+v_2^2 + \\cdot + v_n^2=1\\) and the total value of 1 is divided equally among n components, giving \\(v_n^2\\) a share of \\(\\frac{1}{n}\\). As n becomes large enough, the average value of \\(cos^2(\\alpha)\\) approaches zero.\n\n\n\n\n\n\nNoteMore Formal Proof\n\n\n\n\n\nIn my opinion the above explanation is enough to understand the intuition. However, let’s also present a formal proof for the expected value of \\(v_n^2\\).\nWe’re looking for \\(\\mathbb{E}\\left[v_n^2\\right]\\). And we also have:\n\\[\n\\begin{align}\n\\mathbb{E}[\\sum_{i=1}^n v_i^2] &=\\sum_{i=1}^n \\mathbb{E}[v_i^2] \\\\\n&= n\\mathbb{E}[v_n^2] \\\\\n&= 1\n\\end{align}\n\\]\nHence, \\(\\mathbb{E}[v_n^2]=\\frac{1}{n}\\). Here we used the property that all components of v have identical expected values and the linearity of expectation.\n\n\n\n\n\n\nI hope you enjoyed this post.\nSubscribe to get a notification about future posts.\n\nhtml`\n    &lt;canvas id=\"canvas\" style=\"width:100%; height: 360px;\"&gt;\n    &lt;/canvas&gt;\n`\n\n\n\n\n\n\n\npkg = {\n  const file = await FileAttachment(\"./pkg/demo.js\").url();\n  const module = await import(file);\n  module.default = await module.default();\n  // module.set_panic_hook();\n  return module;\n}"
  },
  {
    "objectID": "posts/random-two-vectors/index.html#introduction",
    "href": "posts/random-two-vectors/index.html#introduction",
    "title": "Why are two random vectors near orthogonal in high dimensions?",
    "section": "",
    "text": "High-dimensional embedding vectors are fundamental building blocks in Machine Learning, particularly in transformers or word2vec. Typically, two vectors that are semantically similar point in roughly the same direction; if they are entirely dissimilar, they point in opposite directions; and if they’re nearly orthogonal, they are unrelated.\nWe usually think in two or three dimensions, but there are some unintuitive properties that only apply in higher dimensions. For example, two random vectors are expected to be near orthogonal in high dimensions. Intuitively, it makes sense for word2vec, as we expect that two words are unrelated in most instances.\nIn this post, I will explain why two random vectors are expected to be nearly orthogonal in high dimensions.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/random-two-vectors/index.html#two-dimensions",
    "href": "posts/random-two-vectors/index.html#two-dimensions",
    "title": "Why are two random vectors near orthogonal in high dimensions?",
    "section": "",
    "text": "function makeCircle2D(vecU, vecV, height=200) {\n  const svg = d3.create('svg')\n    .attr('viewBox', [0, 0, width, height]);\n  \n  function appendArrow(svg, v, color, id) {\n    svg\n      .append('defs')\n      .append('marker')\n      .attr('id', id)\n      .attr('viewBox', [0, 0, 20, 20])\n      .attr('refX', 20)\n      .attr('refY', 10)\n      .attr('markerWidth', 10)\n      .attr('markerHeight', 10)\n      .attr('orient', 'auto-start-reverse')\n      .append('path')\n      .attr('d', d3.line()([[0, 0], [0, 20], [20, 10]]))\n      .attr('fill', color);\n    svg\n      .append('path')\n      .attr('d', d3.line()(\n        [[width / 2, height / 2], [width / 2 + v[0], height / 2 + v[1]]]\n      ))\n      .attr('stroke', color)\n      .attr('stroke-width', '2px')\n      .attr('marker-end', 'url(#' + id + ')')\n      .attr('fill', 'none');\n  }\n  const radius = Math.min(width, height) / 2 - 4;\n  vecU = [vecU[0] * radius, vecU[1] * radius];\n  vecV = [vecV[0] * radius, vecV[1] * radius];\n  svg\n    .append('circle')\n    .attr('cx', '50%')\n    .attr('cy', '50%')\n    .attr('r', radius)\n    .style('stroke', 'black')\n    .style('stroke-width', '2px')\n    .style('fill', 'none');\n\n  appendArrow(svg, vecU, \"black\", \"arrow-0\");\n  appendArrow(svg, vecV, \"red\", \"arrow-1\");\n  return svg.node();\n}\n\n\n\n\n\n\nThis problem is equivalent to selecting two random vectors u and v on a circle of radius 1 and computing their dot product.\n\nmakeCircle2D([0, 1], [1 / Math.sqrt(2), 1 / Math.sqrt(2)])\n\n\n\n\n\n\nAs a reminder the dot product of u and v is:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_x v_x + u_y v_y = \\cos (\\alpha)\n\\]\nThe dot product is zero when u and v are orthogonal, and near zero when they are nearly orthogonal.\nThe dot product is invariant under rotations, which means that we can rotate both vectors in the same way such that the vector u is [0, 1]:\n\nmakeCircle2D([0, -1], [-1 / Math.sqrt(2), -1 / Math.sqrt(2)])\n\n\n\n\n\n\nThis simplifies the dot product to \\(\\mathbf{u} \\cdot \\mathbf{v} = \\cos (\\alpha) = v_y\\). The probability of being near orthogonal is pretty low in two dimensions, but this helps us in framing our problem for higher dimensions.\nIt turns out that the average value of \\(\\cos^2(\\alpha)\\) or \\(v_y^2\\) is exactly \\(\\frac{1}{2}\\). There are numerous analytical proofs, but the simplest intuition is derived from \\(v_x^2+v_y^2=1\\). We have a total budget of 1, which we distribute between \\(v_x^2\\) and \\(v_y^2\\); thus, on average, \\(v_y^2\\) receives half of the budget.\nWe only have two orthogonal vectors to u, and they are:\n\nhtml`\n&lt;div style=\"display: flex; justify-content: center;\"&gt;\n  &lt;div style=\"width: 50%;\"&gt;\n    ${makeCircle2D([0, -1], [-1, 0], 400)}\n  &lt;/div&gt;\n  &lt;div style=\"width: 50%;\"&gt;\n    ${makeCircle2D([0, -1], [1, 0], 400)}\n  &lt;/div&gt;\n&lt;/div&gt;\n`"
  },
  {
    "objectID": "posts/random-two-vectors/index.html#three-dimensions",
    "href": "posts/random-two-vectors/index.html#three-dimensions",
    "title": "Why are two random vectors near orthogonal in high dimensions?",
    "section": "",
    "text": "THREE = {\n  const THREE = window.THREE = await require(\"three@0.145.0/build/three.min.js\");\n  await require(\"three@0.145.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  return THREE;\n}\n\n\n\n\n\n\n\nfunction makeSphere(wireframe = false, opacity = 1) {\n  const material = new THREE.MeshBasicMaterial({\n    wireframe: wireframe, \n    opacity: opacity, \n    color: 0x555555, \n    transparent: true  \n  });\n  const geometry = new THREE.SphereGeometry(1, 32, 32 ); \n  return new THREE.Mesh(geometry, material);\n}\n\n\n\n\n\n\n\nfunction makeSphereSlice(wireframe = true, opacity = 1) {\n  const material = new THREE.MeshBasicMaterial({\n    wireframe: wireframe, \n    opacity: opacity, \n    color: 0xff0000, \n    transparent: true,\n    side: THREE.DoubleSide, \n    depthTest: false, \n  });\n  const geometry = new THREE.SphereGeometry(\n    1.01, 32, 32,\n    0, Math.PI * 2.0,\n    Math.PI / 2 - Math.PI / 32, Math.PI / 16\n  ); \n  return new THREE.Mesh(geometry, material);\n}\n\n\n\n\n\n\n\nfunction makeCircle3D(color, radius=1) {\n  const geometry = new THREE.CircleGeometry(radius, 32); \n  // radius^2+z^2=1\n  // z=sqrt(1-radius^2)\n  const material = new THREE.MeshStandardMaterial({ \n    color: color,\n    side: THREE.DoubleSide,\n    opacity: 0.3,\n    transparent: true,\n    depthTest: false, \n  }); \n  const circle = new THREE.Mesh( geometry, material );\n  circle.position.y = Math.sqrt(1 - radius * radius);\n  circle.lookAt(0, 1, 0);\n  return circle;\n}\n\n\n\n\n\n\n\nfunction makeVector(vec, color) {\n  function customArrow( fx, fy, fz, ix, iy, iz, thickness, color) {\n    const ARROW_BODY = new THREE.CylinderGeometry( 1, 1, 1, 12 )\n                            .rotateX( Math.PI/2)\n                            .translate( 0, 0, 0.5 );\n\n    const ARROW_HEAD = new THREE.ConeGeometry( 1, 1, 12 )\n                            .rotateX( Math.PI/2)\n                            .translate( 0, 0, -0.5 );\n    var material = new THREE.MeshLambertMaterial( {color: color} );\n    \n    var length = Math.sqrt( (ix-fx)**2 + (iy-fy)**2 + (iz-fz)**2 );\n    \n    var body = new THREE.Mesh( ARROW_BODY, material );\n    body.scale.set( thickness, thickness, length-10*thickness );\n      \n    var head = new THREE.Mesh( ARROW_HEAD, material );\n    head.position.set( 0, 0, length );\n    head.scale.set( 3*thickness, 3*thickness, 10*thickness );\n    \n    var arrow = new THREE.Group( );\n    arrow.position.set( ix, iy, iz );\n    arrow.lookAt( fx, fy, fz ); \n    arrow.add( body, head );\n    return arrow;\n  }\n  return customArrow(vec[0], vec[1], vec[2], 0, 0, 0, 0.02, color);\n}\n\n\n\n\n\n\n\nfunction makeScene(vec) {\n  const scene = new THREE.Scene();\n  scene.background = new THREE.Color(0xffffff);\n  scene.add(new THREE.AxesHelper(1.5));\n  scene.add(makeSphere(/*wireframe: */false, /*opacity: */0.1));\n  scene.add(makeSphere(/*wireframe: */true, /*opacity: */1));\n  const light = new THREE.AmbientLight(0xFFFFFF,  /*intensity=*/1);\n  scene.add(light);\n  scene.add(makeVector([0, 1, 0], 0x000000));\n  scene.add(makeVector(vec, 0xff0000));\n  return scene;\n}\n\n\n\n\n\n\n\nfunction makeRendererElement(height, scene) {\n  function makeCamera() {\n    const fov = 45;\n    const aspect = width / height;\n    const near = 1;\n    const far = 1000;\n    var camera = new THREE.PerspectiveCamera(fov, aspect, near, far);\n    camera.position.z = 3;\n    return camera;\n  }\n  const camera = makeCamera();\n  const renderer = new THREE.WebGLRenderer({antialias: true});\n  const controls = new THREE.OrbitControls(camera, renderer.domElement);\n  controls.autoRotate = false;\n  controls.enableZoom = false;\n  function animate() {\n    requestAnimationFrame( animate );\n    controls.update();\n    renderer.render( scene, camera );\n  }\n  animate();\n  invalidation.then(() =&gt; (controls.dispose(), renderer.dispose()));\n  renderer.setSize(width, height);\n  renderer.setPixelRatio(devicePixelRatio);\n  renderer.render(scene, camera)\n  // controls.addEventListener(\"change\", () =&gt; renderer.render(scene, camera));\n  return renderer.domElement\n}\n\n\n\n\n\n\nIn three dimensions, we take two random vectors u and v on a unit sphere and rotate them together so that the vector u becomes the north pole ([0, 0, 1]):\n\n{ \n  const scene = makeScene([1 / Math.sqrt(3), 1 / Math.sqrt(3), 1 / Math.sqrt(3)]);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nTip: The spheres in this post are interactive.\nThe dot product of \\(\\mathbf{u}=[0, 0, 1]\\) and \\(\\mathbf{v}=[v_x, v_y, v_z]\\) is:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\cos (\\alpha) = v_z\n\\]\nIn 2D, we only had two vectors that are orthogonal to u. In 3D, we have an entire subspace of vectors that are orthogonal to u:\n\n{ \n  const scene = makeScene([1 / Math.sqrt(2), 0, 1 / Math.sqrt(2)]);\n  scene.add(makeCircle3D(0xff0000));\n  scene.rotateX(Math.PI / 10);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nIn other words, any vector in the red circle above is orthogonal to u. Moreover, this circle is the largest one that can be found on the sphere. For example, compare with a smaller circle which spans non-orthogonal vectors:\n\n{ \n  const vec = [0.7, Math.sqrt(1 - 0.7 * 0.7), 0];\n  const scene = makeScene(vec);\n  scene.add(makeCircle3D(0xff0000, 0.7));\n  scene.rotateX(Math.PI / 8);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nThe larger the circle from which we select the vector v is, the closer it is to being orthogonal:\n\n{ \n  const vec = [1 / Math.sqrt(2), 0, 1 / Math.sqrt(2)];\n  const scene = makeScene(vec);\n  scene.add(makeSphereSlice(false, 0.3));\n  // scene.add(makeCircle3D(0xff0000, 1));\n  scene.rotateX(Math.PI / 8);\n  return makeRendererElement(300, scene);\n}\n\n\n\n\n\n\nFor example, we can consider all vectors within the thin red stripe to be nearly orthogonal.\nThe average value of \\(\\cos^2 (\\alpha)\\) or \\(v_z^2\\) is \\(\\dfrac{1}{3}\\). Similar to the 2D case, this comes from the fact that \\(v_x^2+v_y^2+v_z^2=1\\). With a total budget of 1 split among \\(v_x^2\\), \\(v_y^2\\) and \\(v_z^2\\), each gets, on average, one-third of it."
  },
  {
    "objectID": "posts/random-two-vectors/index.html#n-dimensions",
    "href": "posts/random-two-vectors/index.html#n-dimensions",
    "title": "Why are two random vectors near orthogonal in high dimensions?",
    "section": "",
    "text": "Visualizing an N-dimensional sphere is challenging. However, we have all the tools needed to develop an intuition.\nSimilar to the 2D and 3D cases, we can fix the first vector as \\(\\mathbf{u}=[0, 0, \\dots, 1]\\). The second vector \\(\\mathbf{v}=[v_1, v_2, \\cdots, v_n]\\) is randomly chosen from an N-dimensional unit sphere. Their dot product is then given by:\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = \\cos (\\alpha) = v_n\n\\]\nThe average value of \\(\\cos^2 (\\alpha)\\) or \\(v_n^2\\) is \\(\\dfrac{1}{n}\\). Since \\(v_1^2+v_2^2 + \\cdot + v_n^2=1\\) and the total value of 1 is divided equally among n components, giving \\(v_n^2\\) a share of \\(\\frac{1}{n}\\). As n becomes large enough, the average value of \\(cos^2(\\alpha)\\) approaches zero.\n\n\n\n\n\n\nNoteMore Formal Proof\n\n\n\n\n\nIn my opinion the above explanation is enough to understand the intuition. However, let’s also present a formal proof for the expected value of \\(v_n^2\\).\nWe’re looking for \\(\\mathbb{E}\\left[v_n^2\\right]\\). And we also have:\n\\[\n\\begin{align}\n\\mathbb{E}[\\sum_{i=1}^n v_i^2] &=\\sum_{i=1}^n \\mathbb{E}[v_i^2] \\\\\n&= n\\mathbb{E}[v_n^2] \\\\\n&= 1\n\\end{align}\n\\]\nHence, \\(\\mathbb{E}[v_n^2]=\\frac{1}{n}\\). Here we used the property that all components of v have identical expected values and the linearity of expectation."
  },
  {
    "objectID": "posts/random-two-vectors/index.html#the-end",
    "href": "posts/random-two-vectors/index.html#the-end",
    "title": "Why are two random vectors near orthogonal in high dimensions?",
    "section": "",
    "text": "I hope you enjoyed this post.\nSubscribe to get a notification about future posts.\n\nhtml`\n    &lt;canvas id=\"canvas\" style=\"width:100%; height: 360px;\"&gt;\n    &lt;/canvas&gt;\n`\n\n\n\n\n\n\n\npkg = {\n  const file = await FileAttachment(\"./pkg/demo.js\").url();\n  const module = await import(file);\n  module.default = await module.default();\n  // module.set_panic_hook();\n  return module;\n}"
  },
  {
    "objectID": "posts/jensens-inequality/index.html",
    "href": "posts/jensens-inequality/index.html",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is fundamental in many fields, including machine learning and statistics. For example, it is useful in the diffusion models paper for understanding the variational lower bound. In this post, I will give a simple geometric intuition for Jensen’s inequality.\nSubscribe to get a notification about future posts.\nFeel free to leave feedback on my telegram channel.\n\n\n\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\n\n\ncode for fig_jensen_inequality\nimport itertools\nfrom typing import Optional\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n\ndef alpha_profiles(n: int):\n    if n == 2:\n        space = np.linspace(0.01, 0.99, 100)\n        return np.column_stack((space, 1.0 - space))\n    space = np.linspace(0.01, 0.99, 15 - max(0, (n - 3) * 5))\n    space_prod = itertools.product(*[space for _ in range(n - 1)])\n    profiles = np.array(list(space_prod))\n    profiles = profiles[np.sum(profiles, axis=1) &lt; 1.0]\n    return np.concatenate([profiles, 1 - np.sum(profiles, axis=1).reshape(-1, 1)], axis=1)\n\n\ndef fig_jensen_inequality(f, x_range: list, x: np.array, y_range: Optional[list] = None):\n    x_linspace = np.linspace(x_range[0], x_range[1], 100)\n    points = np.column_stack([x, f(x)])\n    n = len(points)\n    steps = []\n    hull_points = []\n    titles = []\n    for index, alphas in enumerate(alpha_profiles(n)):\n        hp = np.average(points, weights=alphas, axis=0)\n        hull_points.append(hp)\n        title = \",\".join([\"\\\\lambda_\" + f\"{i + 1}={a:.2f}\" for i, a in enumerate(alphas)])\n        title = f\"${title}$\"\n        titles.append(title)\n        step = dict(name=index, label=index, method=\"update\",\n                    args=[{\n                        \"x\": [[hp[0], hp[0]], [hp[0]], [hp[0]]],\n                        \"y\": [[f(hp[0]), hp[1]], [hp[1]], [f(hp[0])]],\n                    }, {\"title\": title}, [2, 3, 4]])\n        steps.append(step)\n    active_index = len(steps) // 2\n    sliders = [dict(active=len(steps) // 2, steps=steps)]\n    return go.Figure(data=[\n        go.Scatter(\n            name=\"f\", x=x_linspace, y=f(x_linspace), hoverinfo=\"none\"\n        ),\n        go.Scatter(\n            name=\"Convex Hull\", x=np.append(points[:, 0], points[0][0]),\n            y=np.append(points[:, 1], points[0][1]),\n            fillcolor=\"rgba(239, 85, 59, 0.2)\", fill=\"toself\", mode=\"lines\",\n            line=dict(width=3), hoverinfo=\"none\",\n            showlegend=False\n        ),\n        go.Scatter(\n            x=[hull_points[active_index][0], hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0]), hull_points[active_index][1]],\n            mode=\"lines\",\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            line={\"color\": \"black\", \"dash\": \"dot\", \"width\": 1},\n            showlegend=False\n        ),\n        go.Scatter(\n            name=\"A\",\n            x=[hull_points[active_index][0]],\n            y=[hull_points[active_index][1]],\n            mode=f\"markers+text\",\n            text=[\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\"],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"black\"},\n        ),\n        go.Scatter(\n            name=\"B\",\n            x=[hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0])],\n            mode=f\"markers\",\n            text=[\"B\"],\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"#00CC96\"},\n        ),\n        go.Scatter(\n            name=\"$(x_i, f(x_i))$\",\n            x=points[:, 0], y=points[:, 1],\n            mode=\"markers+text\",\n            marker={\"size\": 20, \"color\": \"#ffa15a\"},\n            line={\"color\": \"rgba(239, 85, 59, 0.2)\"},\n            text=[f\"$(x_{i},f(x_{i}))$\" for i in range(1, n + 1)],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            showlegend=False,\n        ),\n    ], layout=go.Layout(\n        title=titles[active_index],\n        xaxis=dict(fixedrange=True, range=x_range),\n        yaxis=dict(fixedrange=True, scaleanchor=\"x\", scaleratio=1, range=y_range),\n        sliders=sliders,\n        legend=dict(\n            yanchor=\"top\",\n            xanchor=\"right\",\n            x=1,\n            y=1\n        ),\n        margin=dict(l=5, r=5, t=60, b=20)\n    ))\n\n\ndef sample_parabola(x):\n    return 0.15 * (x - 15) ** 2 + 15\n\n\n\n\n\ncode for show_sample_jensen_inequality\ndef show_sample_jensen_inequality(x: list):\n    fig = fig_jensen_inequality(\n        f=sample_parabola,\n        x_range=[0, 30],\n        x=np.array(x),\n        y_range=[12, 48]\n    )\n    return fig.show(renderer=\"iframe\")\n\n\n\n\n\nA function is a convex function when the line segment joining any two points on the function graph lies above or on the graph. In the simplest term, a convex function is shaped like \\(\\cup\\) and a concave function is shaped like \\(\\cap\\). If f is convex, then -f is concave.\nA visualization from Wikipedia:\n\n\ndisplay image from Wikipedia\nfrom IPython.display import Image\nImage(url='https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg', width=400)\n\n\n\n\n\n\n\nA function \\(f: X \\rightarrow \\mathbb{R}\\) is called convex if the following holds:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nand concave when:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nfor all \\(0 \\le \\lambda \\le 1\\) and for all \\(x_1, x_2 \\in X\\).\nWe will give geometric intuition for this definition in the next section.\n\n\n\n\nshow_sample_jensen_inequality(x=[2, 22])\n\n\n\n\nAn interactive visualization of the convex function: \\(f(x)=0.15(x - 15)^2 + 15\\). We will use the same parabola during this post unless stated otherwise. You can use the slider to try different values of (\\(\\lambda_1\\), \\(\\lambda_2)\\), where \\(\\lambda_1=\\lambda\\) (from the definition) and \\(\\lambda_2=1-\\lambda\\).\nWe have a line segment that joins \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\). We can sample any point along the line segment with \\((\\lambda_1 x_1 + \\lambda_2 x_2, \\lambda_1 f(x_1) + \\lambda_2 f(x_2))\\) where \\(0 \\le \\lambda_1 \\le 1\\) and \\(\\lambda_2 = 1 - \\lambda_1\\). For example:\n\nWhen \\(\\lambda_1=1\\), we get the first point\nWhen \\(\\lambda_1=0\\), we get the second point\nAnd when \\(\\lambda_1=0.5\\), we get the middle point of the line segment\nand so on… Try the slider above and notice how \\(\\lambda_1\\) and \\(\\lambda_2\\) are changing!\n\nThis point is visualized with a black point above. Let’s name it as A.\nThe light green point where the function graph intersects with the dotted line segment is represented by: \\((\\lambda_1 x_1 + \\lambda_2 x_2, f(\\lambda_1 x_1 + \\lambda_2 x_2))\\). Let’s name it as B.\nThen, the definition above is just asserting that \\(B_y \\le A_y\\) and we also have \\(A_x = B_x\\). Note that we are only showing a single line segment, but this statement should be true for all similar line segments and for all \\(0 \\le \\lambda_1 \\le 1\\).\n\n\n\n\nJensen’s inequality is a generalization of the above convex function definition for more than 2 points.\n\n\nAssume we have a convex function \\(f\\) and \\(x_1, x_2, \\cdots, x_n\\) in \\(f\\)’s domain, and also positive weights \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\) where \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then Jensen’s inequality can be stated as:\n\\[\nf(\\sum_{i=1}^n \\lambda_i x_i) \\le \\sum_{i=1}^n \\lambda_i f(x_i)\n\\]\n\n\n\n\n\n\nNoteConcave Function\n\n\n\n\n\nThe equation is flipped for a concave function g:\n\\[\ng(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\sum_{i=1}^n \\lambda_i g(x_i)\n\\]\n\n\n\nNote that we arrive at the same definition for convex function when \\(n=2\\).\n\n\n\nA numerous proofs are already available by other posts. I encourage you to checkout the following resources:\n\nen.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\nbrilliant.org/wiki/jensens-inequality\nartofproblemsolving.com/wiki/…\nPhysics intuition (1993)\n\nHere I describe a geometric intuition, which resonates more with me.\n\n\nLet’s start with a triangle, i.e., \\(n=3\\):\n\nshow_sample_jensen_inequality(x=[2, 12, 27])\n\n\n\n\nAs before, you can use the slider to try different values of \\((\\lambda_1, \\lambda_2, \\lambda_3)\\) where \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\).\nWe have a triangle that connects the points: \\((x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3))\\).\nIn the \\(n=2\\) case, we used \\(\\lambda_1\\) and \\(\\lambda_2\\) to sample a point along the line segment. In this case, it is similar, but we can sample any point inside or on the boundaries of the triangle with:\n\\[\n\\left(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, \\lambda_1f(x_1)+\\lambda_2f(x_2)+\\lambda_3f(x_3)\\right)\n\\]\nwhere \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\). For example:\n\nWhen \\(\\lambda_i=1\\) where \\(i \\in \\{1, 2, 3\\}\\), we get the point \\((x_i, f(x_i))\\)\nWhen \\(\\lambda_1=\\lambda_2=\\lambda_3=\\frac{1}{3}\\), we get the center of mass of the triangle\n\nThe black point (named A) in the visualization represents this point.\nNote that (\\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\lambda_3\\)) describes the barycentric coordinate system. You don’t need to know it in this post, just sharing in case you’re already familiar with it.\nThe light green point where the parabola meets the dotted line segment is represented by:\n\\[\n(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, f(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3))\n\\]\nIf we name this point as B, then it is not difficult to see that Jensen’s inequality is the same as \\(B_y \\le A_y\\).\n\n\n\nIt is easy to generalize for \\(n&gt;3\\). I am adding it here for the sake of completeness:\n\nshow_sample_jensen_inequality(x=[2, 13, 22, 25])\n\n\n\n\nIn the general case, \\((\\sum_{i=1}^n \\lambda_ix_i, \\sum_{i=1}^n \\lambda_if(x_i))\\) describes a point inside or on the boundary of the convex hull enclosing the points: \\((x_1, f(x_1)), (x_2, f(x_2)), \\cdots, (x_n, f(x_n))\\). The convex hull is always above or on the convex function graph, which is why Jensen’s inequality holds true.\nA few closing notes:\n\nThe convex hull may have any number of points, including n → ∞\nWe closely approximate the convex function in some interval with the convex hull as n approaches infinity\nThe convexity definitions for functions and polygons are the same once we have enough points, i.e., n → ∞\nJensen’s inequality is useful in a probability theory setting, since \\(\\sum_{i=1}^n \\lambda_i = 1\\), including the continuous form with n → ∞.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAM–GM inequality\n\n\n\n\n\nThe arithmetic mean-geometric mean inequality (AM-GM inequality) states that: \\[\n\\frac{x_1+x_2+\\cdots+x_n}{n} \\ge \\sqrt[n]{x_1x_2\\cdots x_n}\n\\]\nLet’s prove with Jensen’s inequality by rewriting the above with \\(\\lambda_1=\\lambda_2=\\cdots=\\lambda_n=\\frac{1}{n}\\):\n\\[\n\\sum_{i=1}^n \\lambda_i x_i \\ge \\prod_{i=1}^n x_i^{\\lambda_i}\n\\]\nSince log is a concave and monotonic function, we can apply log to both sides.\n\\[\n\\log(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\log(\\prod_{i=1}^n x_i^{\\lambda_i}) = \\sum_{i=1}^n \\lambda_i \\log(x_i)\n\\]\nThe above equation is valid due to Jensen’s inequality. Note that the same proof works for the weighted version since the proof does not rely on the fact that \\(\\lambda_i=\\frac{1}{n}\\) for all \\(i=1,2,\\cdots,n\\).\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#introduction",
    "href": "posts/jensens-inequality/index.html#introduction",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is fundamental in many fields, including machine learning and statistics. For example, it is useful in the diffusion models paper for understanding the variational lower bound. In this post, I will give a simple geometric intuition for Jensen’s inequality.\nSubscribe to get a notification about future posts.\nFeel free to leave feedback on my telegram channel."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#setup",
    "href": "posts/jensens-inequality/index.html#setup",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "The post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\n\n\ncode for fig_jensen_inequality\nimport itertools\nfrom typing import Optional\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n\ndef alpha_profiles(n: int):\n    if n == 2:\n        space = np.linspace(0.01, 0.99, 100)\n        return np.column_stack((space, 1.0 - space))\n    space = np.linspace(0.01, 0.99, 15 - max(0, (n - 3) * 5))\n    space_prod = itertools.product(*[space for _ in range(n - 1)])\n    profiles = np.array(list(space_prod))\n    profiles = profiles[np.sum(profiles, axis=1) &lt; 1.0]\n    return np.concatenate([profiles, 1 - np.sum(profiles, axis=1).reshape(-1, 1)], axis=1)\n\n\ndef fig_jensen_inequality(f, x_range: list, x: np.array, y_range: Optional[list] = None):\n    x_linspace = np.linspace(x_range[0], x_range[1], 100)\n    points = np.column_stack([x, f(x)])\n    n = len(points)\n    steps = []\n    hull_points = []\n    titles = []\n    for index, alphas in enumerate(alpha_profiles(n)):\n        hp = np.average(points, weights=alphas, axis=0)\n        hull_points.append(hp)\n        title = \",\".join([\"\\\\lambda_\" + f\"{i + 1}={a:.2f}\" for i, a in enumerate(alphas)])\n        title = f\"${title}$\"\n        titles.append(title)\n        step = dict(name=index, label=index, method=\"update\",\n                    args=[{\n                        \"x\": [[hp[0], hp[0]], [hp[0]], [hp[0]]],\n                        \"y\": [[f(hp[0]), hp[1]], [hp[1]], [f(hp[0])]],\n                    }, {\"title\": title}, [2, 3, 4]])\n        steps.append(step)\n    active_index = len(steps) // 2\n    sliders = [dict(active=len(steps) // 2, steps=steps)]\n    return go.Figure(data=[\n        go.Scatter(\n            name=\"f\", x=x_linspace, y=f(x_linspace), hoverinfo=\"none\"\n        ),\n        go.Scatter(\n            name=\"Convex Hull\", x=np.append(points[:, 0], points[0][0]),\n            y=np.append(points[:, 1], points[0][1]),\n            fillcolor=\"rgba(239, 85, 59, 0.2)\", fill=\"toself\", mode=\"lines\",\n            line=dict(width=3), hoverinfo=\"none\",\n            showlegend=False\n        ),\n        go.Scatter(\n            x=[hull_points[active_index][0], hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0]), hull_points[active_index][1]],\n            mode=\"lines\",\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            line={\"color\": \"black\", \"dash\": \"dot\", \"width\": 1},\n            showlegend=False\n        ),\n        go.Scatter(\n            name=\"A\",\n            x=[hull_points[active_index][0]],\n            y=[hull_points[active_index][1]],\n            mode=f\"markers+text\",\n            text=[\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\"],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"black\"},\n        ),\n        go.Scatter(\n            name=\"B\",\n            x=[hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0])],\n            mode=f\"markers\",\n            text=[\"B\"],\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"#00CC96\"},\n        ),\n        go.Scatter(\n            name=\"$(x_i, f(x_i))$\",\n            x=points[:, 0], y=points[:, 1],\n            mode=\"markers+text\",\n            marker={\"size\": 20, \"color\": \"#ffa15a\"},\n            line={\"color\": \"rgba(239, 85, 59, 0.2)\"},\n            text=[f\"$(x_{i},f(x_{i}))$\" for i in range(1, n + 1)],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            showlegend=False,\n        ),\n    ], layout=go.Layout(\n        title=titles[active_index],\n        xaxis=dict(fixedrange=True, range=x_range),\n        yaxis=dict(fixedrange=True, scaleanchor=\"x\", scaleratio=1, range=y_range),\n        sliders=sliders,\n        legend=dict(\n            yanchor=\"top\",\n            xanchor=\"right\",\n            x=1,\n            y=1\n        ),\n        margin=dict(l=5, r=5, t=60, b=20)\n    ))\n\n\ndef sample_parabola(x):\n    return 0.15 * (x - 15) ** 2 + 15\n\n\n\n\n\ncode for show_sample_jensen_inequality\ndef show_sample_jensen_inequality(x: list):\n    fig = fig_jensen_inequality(\n        f=sample_parabola,\n        x_range=[0, 30],\n        x=np.array(x),\n        y_range=[12, 48]\n    )\n    return fig.show(renderer=\"iframe\")"
  },
  {
    "objectID": "posts/jensens-inequality/index.html#convex-function",
    "href": "posts/jensens-inequality/index.html#convex-function",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "A function is a convex function when the line segment joining any two points on the function graph lies above or on the graph. In the simplest term, a convex function is shaped like \\(\\cup\\) and a concave function is shaped like \\(\\cap\\). If f is convex, then -f is concave.\nA visualization from Wikipedia:\n\n\ndisplay image from Wikipedia\nfrom IPython.display import Image\nImage(url='https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg', width=400)\n\n\n\n\n\n\n\nA function \\(f: X \\rightarrow \\mathbb{R}\\) is called convex if the following holds:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nand concave when:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nfor all \\(0 \\le \\lambda \\le 1\\) and for all \\(x_1, x_2 \\in X\\).\nWe will give geometric intuition for this definition in the next section.\n\n\n\n\nshow_sample_jensen_inequality(x=[2, 22])\n\n\n\n\nAn interactive visualization of the convex function: \\(f(x)=0.15(x - 15)^2 + 15\\). We will use the same parabola during this post unless stated otherwise. You can use the slider to try different values of (\\(\\lambda_1\\), \\(\\lambda_2)\\), where \\(\\lambda_1=\\lambda\\) (from the definition) and \\(\\lambda_2=1-\\lambda\\).\nWe have a line segment that joins \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\). We can sample any point along the line segment with \\((\\lambda_1 x_1 + \\lambda_2 x_2, \\lambda_1 f(x_1) + \\lambda_2 f(x_2))\\) where \\(0 \\le \\lambda_1 \\le 1\\) and \\(\\lambda_2 = 1 - \\lambda_1\\). For example:\n\nWhen \\(\\lambda_1=1\\), we get the first point\nWhen \\(\\lambda_1=0\\), we get the second point\nAnd when \\(\\lambda_1=0.5\\), we get the middle point of the line segment\nand so on… Try the slider above and notice how \\(\\lambda_1\\) and \\(\\lambda_2\\) are changing!\n\nThis point is visualized with a black point above. Let’s name it as A.\nThe light green point where the function graph intersects with the dotted line segment is represented by: \\((\\lambda_1 x_1 + \\lambda_2 x_2, f(\\lambda_1 x_1 + \\lambda_2 x_2))\\). Let’s name it as B.\nThen, the definition above is just asserting that \\(B_y \\le A_y\\) and we also have \\(A_x = B_x\\). Note that we are only showing a single line segment, but this statement should be true for all similar line segments and for all \\(0 \\le \\lambda_1 \\le 1\\)."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#jensens-inequality",
    "href": "posts/jensens-inequality/index.html#jensens-inequality",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is a generalization of the above convex function definition for more than 2 points.\n\n\nAssume we have a convex function \\(f\\) and \\(x_1, x_2, \\cdots, x_n\\) in \\(f\\)’s domain, and also positive weights \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\) where \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then Jensen’s inequality can be stated as:\n\\[\nf(\\sum_{i=1}^n \\lambda_i x_i) \\le \\sum_{i=1}^n \\lambda_i f(x_i)\n\\]\n\n\n\n\n\n\nNoteConcave Function\n\n\n\n\n\nThe equation is flipped for a concave function g:\n\\[\ng(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\sum_{i=1}^n \\lambda_i g(x_i)\n\\]\n\n\n\nNote that we arrive at the same definition for convex function when \\(n=2\\).\n\n\n\nA numerous proofs are already available by other posts. I encourage you to checkout the following resources:\n\nen.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\nbrilliant.org/wiki/jensens-inequality\nartofproblemsolving.com/wiki/…\nPhysics intuition (1993)\n\nHere I describe a geometric intuition, which resonates more with me.\n\n\nLet’s start with a triangle, i.e., \\(n=3\\):\n\nshow_sample_jensen_inequality(x=[2, 12, 27])\n\n\n\n\nAs before, you can use the slider to try different values of \\((\\lambda_1, \\lambda_2, \\lambda_3)\\) where \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\).\nWe have a triangle that connects the points: \\((x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3))\\).\nIn the \\(n=2\\) case, we used \\(\\lambda_1\\) and \\(\\lambda_2\\) to sample a point along the line segment. In this case, it is similar, but we can sample any point inside or on the boundaries of the triangle with:\n\\[\n\\left(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, \\lambda_1f(x_1)+\\lambda_2f(x_2)+\\lambda_3f(x_3)\\right)\n\\]\nwhere \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\). For example:\n\nWhen \\(\\lambda_i=1\\) where \\(i \\in \\{1, 2, 3\\}\\), we get the point \\((x_i, f(x_i))\\)\nWhen \\(\\lambda_1=\\lambda_2=\\lambda_3=\\frac{1}{3}\\), we get the center of mass of the triangle\n\nThe black point (named A) in the visualization represents this point.\nNote that (\\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\lambda_3\\)) describes the barycentric coordinate system. You don’t need to know it in this post, just sharing in case you’re already familiar with it.\nThe light green point where the parabola meets the dotted line segment is represented by:\n\\[\n(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, f(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3))\n\\]\nIf we name this point as B, then it is not difficult to see that Jensen’s inequality is the same as \\(B_y \\le A_y\\).\n\n\n\nIt is easy to generalize for \\(n&gt;3\\). I am adding it here for the sake of completeness:\n\nshow_sample_jensen_inequality(x=[2, 13, 22, 25])\n\n\n\n\nIn the general case, \\((\\sum_{i=1}^n \\lambda_ix_i, \\sum_{i=1}^n \\lambda_if(x_i))\\) describes a point inside or on the boundary of the convex hull enclosing the points: \\((x_1, f(x_1)), (x_2, f(x_2)), \\cdots, (x_n, f(x_n))\\). The convex hull is always above or on the convex function graph, which is why Jensen’s inequality holds true.\nA few closing notes:\n\nThe convex hull may have any number of points, including n → ∞\nWe closely approximate the convex function in some interval with the convex hull as n approaches infinity\nThe convexity definitions for functions and polygons are the same once we have enough points, i.e., n → ∞\nJensen’s inequality is useful in a probability theory setting, since \\(\\sum_{i=1}^n \\lambda_i = 1\\), including the continuous form with n → ∞."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#applications",
    "href": "posts/jensens-inequality/index.html#applications",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "NoteAM–GM inequality\n\n\n\n\n\nThe arithmetic mean-geometric mean inequality (AM-GM inequality) states that: \\[\n\\frac{x_1+x_2+\\cdots+x_n}{n} \\ge \\sqrt[n]{x_1x_2\\cdots x_n}\n\\]\nLet’s prove with Jensen’s inequality by rewriting the above with \\(\\lambda_1=\\lambda_2=\\cdots=\\lambda_n=\\frac{1}{n}\\):\n\\[\n\\sum_{i=1}^n \\lambda_i x_i \\ge \\prod_{i=1}^n x_i^{\\lambda_i}\n\\]\nSince log is a concave and monotonic function, we can apply log to both sides.\n\\[\n\\log(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\log(\\prod_{i=1}^n x_i^{\\lambda_i}) = \\sum_{i=1}^n \\lambda_i \\log(x_i)\n\\]\nThe above equation is valid due to Jensen’s inequality. Note that the same proof works for the weighted version since the proof does not rely on the fact that \\(\\lambda_i=\\frac{1}{n}\\) for all \\(i=1,2,\\cdots,n\\)."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#the-end",
    "href": "posts/jensens-inequality/index.html#the-end",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/roc-auc-implementation/index.html",
    "href": "posts/roc-auc-implementation/index.html",
    "title": "Implementation of ROC AUC Score",
    "section": "",
    "text": "This post is a continuation of the ROC and AUC Interpretation. Please make sure that you understand that post before reading this one.\nIn this post, we will implement a ROC AUC Score in Python with \\(O(n\\log n)\\) runtime complexity.\nSubscribe to get a notification about future posts.\n\n\n\nSimilar to the previous post we have:\n\nA dataset with positive and negative items\nAn ML model that predicts a probability score from 0 to 1, representing the probability that the input belongs to the positive class\n\nWe want to compute the ROC AUC score of our model predictions. The algorithm that we are going to implement is explained more easily with a visualization (press the play button):\n\n\n\n\n\n\n\n\nThis is a slightly modified visualization from the other post. A few notes from the animation video:\n\nThe ROC score is the sum of the areas of trapezoids formed by two adjacent points on the ROC curve\nSome trapezoids have zero area\nWe process the dataset items in order of their probability scores, from the highest to the lowest\n\n\n\n\nLet’s setup our environment:\n\nimport numpy as np\n\nnp.random.seed(0)\nn = 100\ntarget = np.random.randint(0, 2, n)\npredicted = np.random.rand(n)\n\nWe randomly generated targets and predicted probability scores. Let’s check the result of sklearn.metrics.roc_auc_score:\n\nimport sklearn\nsklearn.metrics.roc_auc_score(target, predicted)\n\nnp.float64(0.4277597402597403)\n\n\nOur implementation should have the same score.\n\n\nFirst, let’s implement a helper function that finds the area of the trapezoid defined by two points \\((x_0, y_0)\\) and \\((x_1, y_1)\\).\n\n\n\nArea of trapezoid\n\n\nTo achieve this, we can add the area of the rectangle and the area of the right triangle, which is:\n\\[\n\\begin{align}\n\\text{Area}&=(x_1-x_0) \\times y0+\\frac{1}{2}(x_1-x_0) \\times (y_1-y_0)\\\\\n&= \\frac{1}{2}(x_1-x_0) \\times (2y_0+y_1 - y_0)\\\\\n&= \\frac{1}{2}(x_1-x_0) \\times (y_0 + y_1)\\\\\n\\end{align}\n\\]\nLet’s implement the formula in Python:\n\ndef trapezoid_area(p0, p1):\n    return (p1[0] - p0[0]) * (p0[1] + p1[1]) / 2.0\n\n\n\n\nNow our main implementation:\n\ndef roc_auc_score(target, predicted):\n    n = target.shape[0]\n    num_positive = np.sum(target == 1)\n    num_negative = n - num_positive \n    # argsort in reverse order\n1    order = np.argsort(predicted)[::-1]\n    last = [0, 0]\n    num_true_positive = 0\n    num_false_positive = 0\n    score = 0\n2    for index in range(n):\n        # Make sure that the new threshold is unique\n        if index == 0 or predicted[order[index]] != predicted[order[index - 1]]:\n            # True positive rate\n            tpr = num_true_positive / num_positive\n            # False positive rate\n            fpr = num_false_positive / num_negative\n            # New point on the ROC curve\n3            cur = [fpr, tpr]\n            \n4            score += trapezoid_area(last, cur)\n            last = cur\n        \n        if target[order[index]] == 1:\n5            num_true_positive += 1\n        else:\n6            num_false_positive += 1\n    score += trapezoid_area(last, [1, 1])\n\n    return score \n\n\n1\n\nSort items by their predicted scores, from largest to smallest\n\n2\n\nProcess the sorted items one by one in a loop:\n\n3\n\nForm the current point on the ROC curve by: \\((\\frac{\\text{num\\_false\\_positive}}{\\text{num\\_negative}}, \\frac{\\text{num\\_true\\_positive}}{\\text{num\\_positive}})\\)\n\n4\n\nAdd the trapezoid area formed by the previous point and the current one\n\n5\n\nIf the current item is positive, then increase num_true_positive by one\n\n6\n\nIf the current item is negative, then increase num_false_positive by one\n\n\n\n\nLet’s verify the result:\n\nroc_auc_score(target, predicted)\n\nnp.float64(0.4277597402597403)\n\n\nNice, we got exactly the same result as sklearn.\n\n\n\n\nI hope you enjoyed this post.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/roc-auc-implementation/index.html#introduction",
    "href": "posts/roc-auc-implementation/index.html#introduction",
    "title": "Implementation of ROC AUC Score",
    "section": "",
    "text": "This post is a continuation of the ROC and AUC Interpretation. Please make sure that you understand that post before reading this one.\nIn this post, we will implement a ROC AUC Score in Python with \\(O(n\\log n)\\) runtime complexity.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/roc-auc-implementation/index.html#explanation",
    "href": "posts/roc-auc-implementation/index.html#explanation",
    "title": "Implementation of ROC AUC Score",
    "section": "",
    "text": "Similar to the previous post we have:\n\nA dataset with positive and negative items\nAn ML model that predicts a probability score from 0 to 1, representing the probability that the input belongs to the positive class\n\nWe want to compute the ROC AUC score of our model predictions. The algorithm that we are going to implement is explained more easily with a visualization (press the play button):\n\n\n\n\n\n\n\n\nThis is a slightly modified visualization from the other post. A few notes from the animation video:\n\nThe ROC score is the sum of the areas of trapezoids formed by two adjacent points on the ROC curve\nSome trapezoids have zero area\nWe process the dataset items in order of their probability scores, from the highest to the lowest"
  },
  {
    "objectID": "posts/roc-auc-implementation/index.html#implementation",
    "href": "posts/roc-auc-implementation/index.html#implementation",
    "title": "Implementation of ROC AUC Score",
    "section": "",
    "text": "Let’s setup our environment:\n\nimport numpy as np\n\nnp.random.seed(0)\nn = 100\ntarget = np.random.randint(0, 2, n)\npredicted = np.random.rand(n)\n\nWe randomly generated targets and predicted probability scores. Let’s check the result of sklearn.metrics.roc_auc_score:\n\nimport sklearn\nsklearn.metrics.roc_auc_score(target, predicted)\n\nnp.float64(0.4277597402597403)\n\n\nOur implementation should have the same score.\n\n\nFirst, let’s implement a helper function that finds the area of the trapezoid defined by two points \\((x_0, y_0)\\) and \\((x_1, y_1)\\).\n\n\n\nArea of trapezoid\n\n\nTo achieve this, we can add the area of the rectangle and the area of the right triangle, which is:\n\\[\n\\begin{align}\n\\text{Area}&=(x_1-x_0) \\times y0+\\frac{1}{2}(x_1-x_0) \\times (y_1-y_0)\\\\\n&= \\frac{1}{2}(x_1-x_0) \\times (2y_0+y_1 - y_0)\\\\\n&= \\frac{1}{2}(x_1-x_0) \\times (y_0 + y_1)\\\\\n\\end{align}\n\\]\nLet’s implement the formula in Python:\n\ndef trapezoid_area(p0, p1):\n    return (p1[0] - p0[0]) * (p0[1] + p1[1]) / 2.0\n\n\n\n\nNow our main implementation:\n\ndef roc_auc_score(target, predicted):\n    n = target.shape[0]\n    num_positive = np.sum(target == 1)\n    num_negative = n - num_positive \n    # argsort in reverse order\n1    order = np.argsort(predicted)[::-1]\n    last = [0, 0]\n    num_true_positive = 0\n    num_false_positive = 0\n    score = 0\n2    for index in range(n):\n        # Make sure that the new threshold is unique\n        if index == 0 or predicted[order[index]] != predicted[order[index - 1]]:\n            # True positive rate\n            tpr = num_true_positive / num_positive\n            # False positive rate\n            fpr = num_false_positive / num_negative\n            # New point on the ROC curve\n3            cur = [fpr, tpr]\n            \n4            score += trapezoid_area(last, cur)\n            last = cur\n        \n        if target[order[index]] == 1:\n5            num_true_positive += 1\n        else:\n6            num_false_positive += 1\n    score += trapezoid_area(last, [1, 1])\n\n    return score \n\n\n1\n\nSort items by their predicted scores, from largest to smallest\n\n2\n\nProcess the sorted items one by one in a loop:\n\n3\n\nForm the current point on the ROC curve by: \\((\\frac{\\text{num\\_false\\_positive}}{\\text{num\\_negative}}, \\frac{\\text{num\\_true\\_positive}}{\\text{num\\_positive}})\\)\n\n4\n\nAdd the trapezoid area formed by the previous point and the current one\n\n5\n\nIf the current item is positive, then increase num_true_positive by one\n\n6\n\nIf the current item is negative, then increase num_false_positive by one\n\n\n\n\nLet’s verify the result:\n\nroc_auc_score(target, predicted)\n\nnp.float64(0.4277597402597403)\n\n\nNice, we got exactly the same result as sklearn."
  },
  {
    "objectID": "posts/roc-auc-implementation/index.html#the-end",
    "href": "posts/roc-auc-implementation/index.html#the-end",
    "title": "Implementation of ROC AUC Score",
    "section": "",
    "text": "I hope you enjoyed this post.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\nFeel free to leave feedback on my telegram channel.\nSubscribe to get a notification about future posts.\n\n\n\n\n\nThe lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the sum of absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the same equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying less important features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero.\n\n\n\n\n\n\nThe best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?\n\n\n\n\n\n\nWe will use a simple loss function that illustrates circle contours instead of elliptical ones.:\n\\[\nLoss(\\beta_1,\\beta_2 | c_x, c_y)=2{(\\beta_1 - c_x)}^2 + 2{(\\beta_2 - c_y)}^2 + 100\n\\]\nOnce we understand the intution for circles, it is easy to extend to other contours such as elliptical ones. We will use \\(c_x=15\\) and \\(c_y=5\\) most of the time.\n\n\ncode for loss setup and helper functions\nfrom enum import Enum\n\nimport mpl_toolkits.mplot3d.art3d as art3d\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else:\n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n\n\ndef argmin_within_constraint(reg: Reg, t: float):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    if reg == Reg.L1:\n        mask = np.abs(B0) + np.abs(B1) &lt;= t\n    else:\n        mask = B0 * B0 + B1 * B1 &lt;= t * t\n    index = np.argmin(Z[mask])\n    return B0[mask][index], B1[mask][index]\n\n\nbeta_range = -20, 20\ncx, cy = 15, 5\nvmax = 1000\n\n\n\n\ncode for plot3d\ndef base_fig3():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0, 500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n    # draw axes\n    ax.plot(beta_range, [0, 0], color='k')\n    ax.plot([0, 0], beta_range, color='k')\n    return fig, ax\n\n\ndef plot3d(reg: Reg, t=3):\n    fig, ax = base_fig3()\n\n    # surface\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n\n    # contours\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', zdir='z', offset=0, vmax=vmax)\n    \n    # minima within regularization shape\n    mx, my = argmin_within_constraint(reg, t)\n    ax.plot([mx], [my], marker='.', markersize=10, color='r')\n\n    # regularization contraints\n    reg_shape = make_reg_shape(reg, t, color=\"black\")\n    ax.add_patch(reg_shape)\n    art3d.pathpatch_2d_to_3d(reg_shape, z=0)\n\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\n\nLet’s visualize our loss \\(2{(\\beta_1 - 15)}^2 + 2{(\\beta_2 - 5)}^2 + 100\\) with the L1 constaint \\(t=5\\), i.e., \\(|\\beta_1| + |\\beta_2| \\le 5\\) in 3D:\n\nplot3d(Reg.L1, t=5)\n\n\n\n\n\n\n\n\nIn Lasso Regression, we’re looking for \\(\\beta_1\\) and \\(\\beta_2\\) within the diamond that has the lowest loss, which is marked with the red point in the figure above. The global minima without any constraint is marked with “x”.\nThe same visualization but with the L2 constraint t=5, i.e., \\(\\beta_1^2+\\beta_2^2 \\le 5^2\\):\n\nplot3d(Reg.L2, t=5)\n\n\n\n\n\n\n\n\nThe corresponding 2D visualizations that are similar to the ones given by the Elements of Statistical Learning book:\n\n\ncode for plot2d\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n\n\ndef ax2d_init(ax):\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n\ndef plot2d(regs: [Reg], t: float):\n    fig = plt.figure(figsize=(4 * len(regs), 4))\n    axes = fig.subplots(1, len(regs))\n    for ax in axes:\n        ax2d_init(ax)\n    for reg, ax in zip(regs, axes):\n        # draw the regularization safe region\n        ax.add_patch(make_reg_shape(reg=reg, t=t))\n        loss_contour(ax)\n        # draw minima within constraint\n        mx, my = argmin_within_constraint(reg, t)\n        ax.plot([mx], [my], marker='.', markersize=10, color='r')\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot2d([Reg.L1, Reg.L2], t=5)\n\n\n\n\n\n\n\n\n\n\n\nIn both cases (L1, L2), we’re looking for a contour that just touches the constraint region. For example, when we have the following case:\n\n\ncode for plot_reg_and_circle\ndef plot_reg_and_circle(reg: Reg, t: float, cx: float, cy: float, radius: float):\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    ax.add_patch(Circle(xy=(cx, cy), radius=radius, color='b', fill=False))\n    ax.add_patch(make_reg_shape(reg, t=t))\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_reg_and_circle(Reg.L1, t=7, cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nThen it is more optimal to reduce the constraint region (i.e., the diamond) until they touch in a single point as shown below. Note that a contour has the same loss along its points.\n\nplot_reg_and_circle(Reg.L1, t=(np.sqrt(50)-4) * np.sqrt(2), cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nWhen does the Lasso pick the corner of the diamond over any other point on the edges? To simplify our problem, let’s fix a specific circle among the contours: a circle with a fixed radius \\(r\\). Now, we want to come up with a formula that gives the probability of a random tangent circle with radius \\(r\\) touching our diamond at the corners versus at any other points.\nThe above problem becomes much simpler with a visualization:\n\n\ncode for html5_video_l1_tangent_circles\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t + np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop())\n    vertices.reverse()\n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        group = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner:\n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                group.append(corner + vec)\n            else:\n                group.append(v)\n        locations.append(np.array(group))\n    return locations\n\n\ndef l2_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    angles = np.linspace(0, 2 * np.pi, 200)\n    vertices = np.column_stack([np.cos(angles), np.sin(angles)]) * (t + radius) \n    return [vertices]\n\n\ndef animate_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    if reg == Reg.L1:\n        diamond = make_reg_shape(Reg.L1, t=t)\n        circle_locations = l1_tangent_circle_locations(t, radius)\n        plots = []\n        for i, locations in enumerate(circle_locations):\n            color = 'g' if i % 2 == 0 else 'b'\n            plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    else:\n        circle_locations = l2_tangent_circle_locations(t, radius)\n        plots = [ax.plot([], [], color='b', linewidth=2)[0]]\n    \n    circle = Circle(xy=(t + radius, 0), radius=radius, color='g', fill=False)\n    ax.add_patch(circle)\n    ax.add_patch(make_reg_shape(reg, t=t))\n\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\ndef html5_video_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    ani = animate_tangent_circle_trajectories(reg, t=t, radius=radius)\n    html = ani.to_html5_video()\n    plt.close()\n    return html\n\n\n\nfrom IPython.display import HTML\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe animation above illustrates all tangent circle trajectories. The circle is green when it touches the diamond at the corners, and it is blue when it touches at other points. The total length of all blue (touching at other points) trajectories is given by \\(4\\times \\sqrt{2}\\times t\\), while the total length of all green (touching at the corners) trajectories is given by \\(2\\pi \\times r\\), where t is the constraint and r is the radius of the circle. This means that it is likely to touch at the corners when t is sufficiently small or r is sufficiently high. We illustrate one of the such cases below:\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=3, radius=10))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIn Ridge (L2), we don’t have special points like corners, and all points along the disk have the same probability of touching a random tangent circle with radius \\(r\\):\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L2, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\nFeel free to leave feedback on my telegram channel.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the sum of absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the same equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying less important features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "We will use a simple loss function that illustrates circle contours instead of elliptical ones.:\n\\[\nLoss(\\beta_1,\\beta_2 | c_x, c_y)=2{(\\beta_1 - c_x)}^2 + 2{(\\beta_2 - c_y)}^2 + 100\n\\]\nOnce we understand the intution for circles, it is easy to extend to other contours such as elliptical ones. We will use \\(c_x=15\\) and \\(c_y=5\\) most of the time.\n\n\ncode for loss setup and helper functions\nfrom enum import Enum\n\nimport mpl_toolkits.mplot3d.art3d as art3d\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else:\n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n\n\ndef argmin_within_constraint(reg: Reg, t: float):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    if reg == Reg.L1:\n        mask = np.abs(B0) + np.abs(B1) &lt;= t\n    else:\n        mask = B0 * B0 + B1 * B1 &lt;= t * t\n    index = np.argmin(Z[mask])\n    return B0[mask][index], B1[mask][index]\n\n\nbeta_range = -20, 20\ncx, cy = 15, 5\nvmax = 1000\n\n\n\n\ncode for plot3d\ndef base_fig3():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0, 500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n    # draw axes\n    ax.plot(beta_range, [0, 0], color='k')\n    ax.plot([0, 0], beta_range, color='k')\n    return fig, ax\n\n\ndef plot3d(reg: Reg, t=3):\n    fig, ax = base_fig3()\n\n    # surface\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n\n    # contours\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', zdir='z', offset=0, vmax=vmax)\n    \n    # minima within regularization shape\n    mx, my = argmin_within_constraint(reg, t)\n    ax.plot([mx], [my], marker='.', markersize=10, color='r')\n\n    # regularization contraints\n    reg_shape = make_reg_shape(reg, t, color=\"black\")\n    ax.add_patch(reg_shape)\n    art3d.pathpatch_2d_to_3d(reg_shape, z=0)\n\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\n\nLet’s visualize our loss \\(2{(\\beta_1 - 15)}^2 + 2{(\\beta_2 - 5)}^2 + 100\\) with the L1 constaint \\(t=5\\), i.e., \\(|\\beta_1| + |\\beta_2| \\le 5\\) in 3D:\n\nplot3d(Reg.L1, t=5)\n\n\n\n\n\n\n\n\nIn Lasso Regression, we’re looking for \\(\\beta_1\\) and \\(\\beta_2\\) within the diamond that has the lowest loss, which is marked with the red point in the figure above. The global minima without any constraint is marked with “x”.\nThe same visualization but with the L2 constraint t=5, i.e., \\(\\beta_1^2+\\beta_2^2 \\le 5^2\\):\n\nplot3d(Reg.L2, t=5)\n\n\n\n\n\n\n\n\nThe corresponding 2D visualizations that are similar to the ones given by the Elements of Statistical Learning book:\n\n\ncode for plot2d\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n\n\ndef ax2d_init(ax):\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n\ndef plot2d(regs: [Reg], t: float):\n    fig = plt.figure(figsize=(4 * len(regs), 4))\n    axes = fig.subplots(1, len(regs))\n    for ax in axes:\n        ax2d_init(ax)\n    for reg, ax in zip(regs, axes):\n        # draw the regularization safe region\n        ax.add_patch(make_reg_shape(reg=reg, t=t))\n        loss_contour(ax)\n        # draw minima within constraint\n        mx, my = argmin_within_constraint(reg, t)\n        ax.plot([mx], [my], marker='.', markersize=10, color='r')\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot2d([Reg.L1, Reg.L2], t=5)\n\n\n\n\n\n\n\n\n\n\n\nIn both cases (L1, L2), we’re looking for a contour that just touches the constraint region. For example, when we have the following case:\n\n\ncode for plot_reg_and_circle\ndef plot_reg_and_circle(reg: Reg, t: float, cx: float, cy: float, radius: float):\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    ax.add_patch(Circle(xy=(cx, cy), radius=radius, color='b', fill=False))\n    ax.add_patch(make_reg_shape(reg, t=t))\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_reg_and_circle(Reg.L1, t=7, cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nThen it is more optimal to reduce the constraint region (i.e., the diamond) until they touch in a single point as shown below. Note that a contour has the same loss along its points.\n\nplot_reg_and_circle(Reg.L1, t=(np.sqrt(50)-4) * np.sqrt(2), cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nWhen does the Lasso pick the corner of the diamond over any other point on the edges? To simplify our problem, let’s fix a specific circle among the contours: a circle with a fixed radius \\(r\\). Now, we want to come up with a formula that gives the probability of a random tangent circle with radius \\(r\\) touching our diamond at the corners versus at any other points.\nThe above problem becomes much simpler with a visualization:\n\n\ncode for html5_video_l1_tangent_circles\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t + np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop())\n    vertices.reverse()\n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        group = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner:\n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                group.append(corner + vec)\n            else:\n                group.append(v)\n        locations.append(np.array(group))\n    return locations\n\n\ndef l2_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    angles = np.linspace(0, 2 * np.pi, 200)\n    vertices = np.column_stack([np.cos(angles), np.sin(angles)]) * (t + radius) \n    return [vertices]\n\n\ndef animate_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    if reg == Reg.L1:\n        diamond = make_reg_shape(Reg.L1, t=t)\n        circle_locations = l1_tangent_circle_locations(t, radius)\n        plots = []\n        for i, locations in enumerate(circle_locations):\n            color = 'g' if i % 2 == 0 else 'b'\n            plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    else:\n        circle_locations = l2_tangent_circle_locations(t, radius)\n        plots = [ax.plot([], [], color='b', linewidth=2)[0]]\n    \n    circle = Circle(xy=(t + radius, 0), radius=radius, color='g', fill=False)\n    ax.add_patch(circle)\n    ax.add_patch(make_reg_shape(reg, t=t))\n\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\ndef html5_video_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    ani = animate_tangent_circle_trajectories(reg, t=t, radius=radius)\n    html = ani.to_html5_video()\n    plt.close()\n    return html\n\n\n\nfrom IPython.display import HTML\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe animation above illustrates all tangent circle trajectories. The circle is green when it touches the diamond at the corners, and it is blue when it touches at other points. The total length of all blue (touching at other points) trajectories is given by \\(4\\times \\sqrt{2}\\times t\\), while the total length of all green (touching at the corners) trajectories is given by \\(2\\pi \\times r\\), where t is the constraint and r is the radius of the circle. This means that it is likely to touch at the corners when t is sufficiently small or r is sufficiently high. We illustrate one of the such cases below:\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=3, radius=10))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIn Ridge (L2), we don’t have special points like corners, and all points along the disk have the same probability of touching a random tangent circle with radius \\(r\\):\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L2, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#end",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#end",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/dot-product/index.html",
    "href": "posts/dot-product/index.html",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "The dot product (or scalar product) is a simple yet powerful operation that is used in many places in Machine Learning and other fields. In this post, I will explain the geometric intuition behind the dot product. You need to have a basic grasp of trigonometry and vector algebra to follow this post.\nI will need this explanation for my future posts.\nFeel free to ask questions on my telegram channel\n\n\n\nMore beginner friendly explanations are available in the following resources:\n\nen.wikipedia.org/wiki/Dot_product\nbrilliant.org/wiki/dot-product-definition\nbetterexplained.com/articles/vector-calculus-understanding-the-dot-product\n\nFeel free to checkout the above or other resources first.\nAssume we have two vectors \\(\\textbf{a}=[a_1, a_2, \\cdots, a_n]\\) and \\(\\textbf{b} = [b_1, b_2, \\cdots, b_n]\\), then there are two definitions of the dot product: algebraic and geometric.\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\sum_{i=1}^n{a_ib_i} = a_1b_1+a_2b_2+\\cdots+a_nb_n\n\\]\n\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is: \\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\theta\\) is the angle between a and b and \\(\\|\\textbf{a}\\|\\) is the magnitude of a vector a.\n\n\n\n\n\n\nNoteGeometric properties\n\n\n\n\n\nThe geometric definition gives us a few useful properties:\n\nThe dot product is zero when a and b are orthogonal, since \\(\\cos(90 \\degree) = 0\\)\nThe dot product is positive for acute angles and negative for obtuse, e.g., \\(\\cos(45\\degree)\\) or \\(\\cos(89\\degree)\\) are positive but \\(\\cos(180\\degree), \\cos(91\\degree)\\) are negative.\nWe can find the angle between vectors by \\(\\theta = \\arccos(\\frac{\\textbf{a} \\cdot \\textbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|})\\). A picture from Wikipedia:\n\n\n\n\nSource Wikipedia\n\n\n\n\n\n\n\n\nWait, how are the two dot product definitions the same? How is \\(cos( \\theta)\\) related to such a straightforward sum of components? A few questions I asked when I first encountered the dot product. I accepted the fact and moved on with my life until today. I will try to understand myself and also explain by interactive visualizations.\n\n\n\n\nLet’s first simplify the problem as much as possible. Once we build up an intuition for simpler cases, we can come back and try to understand the general cases.\n\n\nFirst, we will only consider the \\(n=2\\) case. Assume that we have 2D vectors: \\(\\textbf{a}=[a_x, a_y]\\) and \\(\\textbf{b}=[b_x, b_y]\\) with the dot product:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\|\\textbf{a}\\|=\\sqrt{a_x^2+a_y^2}\\) is the length of the vector and \\(\\theta\\) is the angle between the vectors.\nSecond, scaling either \\(\\textbf{a}\\) or \\(\\textbf{b}\\) by a real number \\(k\\) does not invalidate the dot product equivalence. Let’s say we scale \\(\\textbf{b}\\) by \\(k\\), then:\n\\[\n\\textbf{a} \\cdot (k \\textbf{b}) = a_x(kb_x)+a_y(kb_y)=\\|\\textbf{a}\\|k\\|\\textbf{b}\\|\\cos \\theta\n\\]\nWith the above, we can simplify further by constraining \\(\\|\\textbf{b}\\| = 1\\). If the equivalence holds for \\(\\|\\textbf{b}\\| = 1\\), then it holds for any \\(\\|k\\textbf{b}\\|\\).\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: false,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board(\n  name, showSlider=true, sliderValue=0.0, \n  showP=true, showPLabel=true\n) {\n  function vec_length(a)  {\n    return Math.sqrt(a[0] * a[0] + a[1] * a[1]);\n  }\n  function vec_unit(a) {\n    const len = vec_length(a);\n    return vec_scale(a, 1 / len);\n  }\n  function vec_scale(a, scalar) {\n    return [a[0] * scalar, a[1] * scalar];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  function vec_cross(a, b) {\n    return a[0] * b[1] - a[1] * b[0];\n  }\n  function vec_mid(a, b) {\n    return vec_scale(vec_add(a, b), 0.5);\n  }\n  function vec_rot90(v) {\n    return [-v[1], v[0]];\n  }\n  function vec_add(a, b) {\n    return [a[0] + b[0], a[1] + b[1]];\n  }\n\n  function calcP(a, b) {\n    const unit_b = vec_unit(b);\n    return vec_scale(unit_b, vec_dot(a, unit_b));\n  }\n\n  function calcUnder(p, a, delta=0.05) {\n    var n = vec_scale(vec_rot90(p), delta / vec_length(p));\n    if (vec_cross(a, p) &lt; 0) {\n      n = vec_scale(n, -1);\n    }\n    return [n, vec_add(p, n)];\n  }\n\n  const a = [2.5, 2];\n  const b = [1, 0];\n  const p = [a[0], 0];\n  var board = init_board(name, 3.5);\n  const pointa = board.create(\"point\", a, {\n    fixed: false,\n    name: \"a\",\n    color: \"blue\"\n  })\n  const pointb = board.create(\"point\", b, {\n    fixed: true,\n    name: \"b\",\n    color: \"red\"\n  });\n  const pointp = board.create(\"point\", [function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[0];\n  }, function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[1];\n  }], {\n    visible: showP,\n    fixed: true,\n    name: \"p\",\n    color: \"green\",\n    size: 5,\n  });\n  const lineGreen = board.create(\"line\", [function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[0];\n  }, function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[1];\n  }], {\n    visible: showP,\n    lastArrow:true,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    strokeColor: \"green\",\n    strokeWidth: 2.5,\n    label:{offsets:[-1,1]}\n  });\n  const linea = board.create(\"line\", [[0, 0], pointa], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"blue\",\n    strokeWidth: 2\n  });\n  const lineb = board.create(\"line\", [[0, 0], pointb], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"red\",\n    strokeWidth: 2\n  });\n\n  const lineap = board.create(\"line\", [pointa, pointp], {\n    visible: showP,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    dash: 2,\n    strokeColor: \"black\",\n    strokeWidth: 1\n  })\n  const rightAngle = board.create(\"nonreflexangle\", [pointa, pointp, pointb], {\n    visible: showP,\n    name: \"\",\n    strokeColor: \"black\",\n    strokeWidth: 0.7,\n    fillColor: \"#00000000\",\n    radius: 0.3\n  });\n  const pointo = board.create(\"point\", [0, 0], {\n    visible: false,\n    fixed: true,\n    name: \"o\",\n  });\n  const alphaAngle = board.create(\"angle\", [pointb, pointo, pointa], {\n    name: \"θ\"\n  });\n \n  const slider = board.create('slider', [[-1, -2], [1, -2], [0, sliderValue, 360]], {\n    visible: showSlider,\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const text = board.create('text', [function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[0];\n  }, function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[1];\n  }, '||a||cos(θ)'], {\n    visible: showP && showPLabel,\n    anchorX: \"middle\",\n    anchorY: \"middle\",\n    // fontSize: 15,\n  })\n  const rot = board.create('transform', [function(){return slider.Value() / 180 * Math.PI;}, [0, 0]], {type:'rotate'});\n  rot.bindTo([pointb, pointa]);\n  return board;\n}\n\n\n\n\n\n\nA visualization for the setup we have so far:\n\nboard_div(\"board1\", 300)\n\n\n\n\n\n\n\nboard1 = make_board(\"board1\", /*showSlider=*/false, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe have the following items shown above:\n\nThe vector \\(\\textbf{a}\\): it is interactive and visualized with blue; feel free to move the point \\(\\textbf{a}\\) around.\nThe vector \\(\\textbf{b}\\): it is visualized with red and \\(\\|\\textbf{b}\\|=1\\).\n\\(\\theta\\): the angle from \\(\\textbf{b}\\) to \\(\\textbf{a}\\)\n\nThe geometric definition implies that the dot product is invariant under rotations. So, if we rotate the both vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) by the same angle \\(\\alpha\\), the dot product won’t change. It is not obvious from the algebraic definition, but a detailed proof is provided in the next section. Let’s trust this property for now until the next section, and simplify our problem for the third time.\nWe can rotate the both vectors such that \\(\\textbf{b}\\) is aligned with the x-axis. Click the button below for the illustration:\n\nviewof mybutton = {\n  const form = Inputs.button(\"Play to Rotate\", {value: null, reduce: () =&gt; {\n    var slider = board2.elementsByName[\"alpha\"];\n    function updateRotate() {\n      if (slider.Value() == 0) {\n        return;\n      }\n      slider.setValue(Math.max(0, slider.Value() - 1));\n      board2.update();\n      setTimeout(updateRotate, 12);\n    }\n    if (slider.Value() == 0) {\n      slider.setValue(60);\n    }\n    updateRotate();\n  }});\n  const scope = DOM.uid().id;\n  const cls = form.classList[0];\n  form.classList.add(scope);\n  form.append(html`&lt;style&gt;\n    .${cls} &gt; button { color: white }\n    &lt;/style&gt;\n  `);\n  return form;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", 300)\n\n\n\n\n\n\n\nboard2 = make_board(\"board2\", /*showSlider=*/true, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe arrived at a simplifed version of the problem: a vector \\(\\textbf{a}=[a_x, a_y]\\) and a fixed vector \\(\\textbf{b}=[1, 0]\\). Let’s check if the definitions agree now. The algebraic dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=a_x\n\\]\nand the geometric is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =  \\|\\textbf{a}\\|\\cos \\theta\n\\]\nnow, we have \\(\\textbf{a} \\cdot \\textbf{b} = a_x =  \\|\\textbf{a}\\|\\cos \\theta\\). This is correct due to the polar coordinates. Alternatively, we can easily derive from the cosine definition. The proof is left as an exercise to the reader. You can use the visualization below to convince yourself:\n\nboard_div(\"board3\", 400)\n\n\n\n\n\n\n\nboard3 = make_board(\"board3\", /*showSlider=*/false, /*sliderValue=*/0, /*showP=*/true)\n\n\n\n\n\n\nWe introduced a new vector \\(\\textbf{p}\\) in the above visualization:\n\nIt is visualized in green and visually represents the dot product\nIt has coordinates \\(\\textbf{p}=[\\|\\textbf{a}\\|\\cos \\theta, 0]=[a_x, 0]\\):\n\nWhere \\(p_x\\) is the dot product as we derived above\n\nThe vector \\(\\textbf{p}\\) is collinear with the vector \\(\\textbf{b}\\)\nSince the vectors \\(\\textbf{p}\\) and \\(\\textbf{b}\\) lie along the same line:\n\nWe can express \\(\\textbf{p}\\) using vector arithmetic: \\(\\textbf{p}=(\\|\\textbf{a}\\|\\cos \\theta)\\textbf{b}\\)\n\\(\\|\\textbf{a}\\|\\cos \\theta\\) is a real number, which means how many \\(\\textbf{b}\\)`s are required to reach the vector \\(\\textbf{p}\\)\nYou can visualize a 1D number line spanned by the vector \\(\\textbf{b}\\), then \\(\\|\\textbf{a}\\|\\cos \\theta\\) is the location of \\(\\textbf{p}\\) in that 1D coordinate system.\n\n\\(p_x\\) can be 0, positive or negative. When negative, the vector \\(\\textbf{p}\\) is opposite to the vector \\(\\textbf{b}\\)\nThe vector \\(\\textbf{p}\\) is actually the projection of the vector \\(\\textbf{a}\\) into \\(\\textbf{b}\\).\nPlay with the point \\(\\textbf{a}\\) to gain more intuition!\n\nTry different values of \\(\\textbf{a}\\) by moving it in the visualization, and verify the following questions:\n\nWhen the dot product is zero?\nWhen the dot product is negative ?\nWhen the dot product is positive ?\nWhat values of \\(\\textbf{a}\\) result in the same dot product?\n\nThe simplified form of the dot product is quite useful for intuition. I visualize this version in my mind when I forget some details of the dot product. At this point, you should be comfortable with the dot product of the simplified form: \\(\\textbf{a} \\cdot [1, 0]\\).\n\n\n\nFirst, let’s visualize the rotation:\n\nboard_div(\"board4\", 500)\n\n\n\n\n\n\n\nboard4 = make_board(\"board4\", /*showSlider=*/true, /*sliderValue=*/10, /*showP=*/true,  /*showP=*/false)\n\n\n\n\n\n\nThere is a slider which represents the angle \\(\\alpha\\) (in degrees) by which both vectors are rotated. Feel free to play with the slider! You can also interact with the point \\(\\textbf{a}\\).\nIt is easy to see why the dot product is invariant under rotations from the geometric definition. The geometric definition relies only on the lengths of the vectors, and the lengths don’t change when rotated.\nHowever, it is not immediately clear from the algebraic definition. To prove it, we derive an alternative formula for the algebraic dot product:\n\\[\n\\begin{align}\n\\|\\textbf{a}-\\textbf{b}\\|^2 &= (a_x-b_x)^2 + (a_y-b_y)^2 \\\\\n&= a_x^2 + b_x^2 - 2a_xb_x + a_y^2 + b_y^2 - 2a_yb_y \\\\\n&= (a_x^2 + a_y^2) + (b_x^2 + b_y^2) - 2(a_xb_x + a_yb_y) \\\\\n&= \\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - 2 \\textbf{a} \\cdot \\textbf{b}\n\\end{align}\n\\]\nwhich gives us:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= a_xb_x + a_yb_y \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nThe dot product above is derived purely from the lengths. Since the lengths remain unchanged under rotations, the algebraic dot product remains unchanged as well, which completes our proof.\nNote that we derived the law of cosine, which is quite cool!\n\n\n\nIt’s not hard to extend the 2D version of the proof to n-dimensional vectors, since we didn’t rely on any properties unique to 2D. The shortest way is:\n\nNote that the dot product of n-dimensional vectors is invariant under rotations. The same proof as in the rotational invariance\nRotate the vectors such that they lie in the xy-plane\nNow, we reduced n-dimensional vectors to 2D vectors, which we already know how to prove\n\nI will provide more alternative proofs below, mostly for myself. They’re optional and collapsed, feel free to read them.\n\n\n\n\n\n\nNoteAlternative Proof 1\n\n\n\n\n\nWe only scaled the vector \\(\\textbf{b}\\) but we can also scale the vector \\(\\textbf{a}\\), so that \\(\\|\\textbf{a}\\|=1\\). Also switch to the polar coordinates: \\(\\textbf{a}=[\\cos \\alpha, \\sin \\alpha]\\) and \\(\\textbf{b}=[\\cos \\beta, \\sin \\beta]\\).\nThen, the geometric dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos(\\alpha - \\beta)\n\\]\nThe algebraic is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\nWe arrive at the cosine subtraction rule: \\[\n\\cos(\\alpha-\\beta)=\\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\n\n\n\n\n\n\n\n\n\nNoteAlternative Proof 2\n\n\n\n\n\nWe will extend the formula from the rotational invariance section further. We derived the following dot product formula:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= \\sum_{i=1}^n a_ib_i \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nFrom the law of cosine, we also have:\n\\[\n\\|\\textbf{a}-\\textbf{b}\\|=\\|\\textbf{a}\\|^2+\\|\\textbf{b}\\|^2-2\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nif we rearrange and combine the above two equations: \\[\n\\begin{align}\n\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =& \\textbf{a} \\cdot \\textbf{b}= \\sum_{i=1}^n a_ib_i  \\\\\n=& \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nNoteAlternative Proof 3\n\n\n\n\n\nWe can use the Pythagorean theorem:\n\\[\n\\begin{align}\n\\|\\textbf{a}\\|^2&=\\|\\textbf{p}\\|^2 + \\|\\textbf{a}-\\textbf{p}\\|^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x-p_x)^2 + (a_y-p_y)^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x^2+a_y^2) + (p_x^2+p_y^2) - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2\\textbf{a}\\cdot\\textbf{p} \\\\\n\\end{align}\n\\]\nThis gives us the following:\n\\[\na_xp_x+a_yp_y=\\textbf{a}\\cdot\\textbf{p}=\\|\\textbf{p}\\|^2\n\\]\nif we substitute \\(\\textbf{p}\\) by \\((\\|\\textbf{a}\\|\\cos\\theta) \\textbf{b}\\):\n\\[\n\\|\\textbf{a}\\|\\cos\\theta(a_xb_x+a_yb_y)=\\|\\textbf{a}\\|\\cos\\theta(\\textbf{a}\\cdot \\textbf{b})=(\\|\\textbf{a}\\|\\cos\\theta)^2\n\\]\nwhich gives back the both geometric and algebraic dot product.\n\n\n\n\n\n\n\n\n\nNoteAlternative Proof 4\n\n\n\n\n\nYou can also find more proofs in proofwiki.\n\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/dot-product/index.html#introduction",
    "href": "posts/dot-product/index.html#introduction",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "The dot product (or scalar product) is a simple yet powerful operation that is used in many places in Machine Learning and other fields. In this post, I will explain the geometric intuition behind the dot product. You need to have a basic grasp of trigonometry and vector algebra to follow this post.\nI will need this explanation for my future posts.\nFeel free to ask questions on my telegram channel"
  },
  {
    "objectID": "posts/dot-product/index.html#recap",
    "href": "posts/dot-product/index.html#recap",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "More beginner friendly explanations are available in the following resources:\n\nen.wikipedia.org/wiki/Dot_product\nbrilliant.org/wiki/dot-product-definition\nbetterexplained.com/articles/vector-calculus-understanding-the-dot-product\n\nFeel free to checkout the above or other resources first.\nAssume we have two vectors \\(\\textbf{a}=[a_1, a_2, \\cdots, a_n]\\) and \\(\\textbf{b} = [b_1, b_2, \\cdots, b_n]\\), then there are two definitions of the dot product: algebraic and geometric.\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\sum_{i=1}^n{a_ib_i} = a_1b_1+a_2b_2+\\cdots+a_nb_n\n\\]\n\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is: \\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\theta\\) is the angle between a and b and \\(\\|\\textbf{a}\\|\\) is the magnitude of a vector a.\n\n\n\n\n\n\nNoteGeometric properties\n\n\n\n\n\nThe geometric definition gives us a few useful properties:\n\nThe dot product is zero when a and b are orthogonal, since \\(\\cos(90 \\degree) = 0\\)\nThe dot product is positive for acute angles and negative for obtuse, e.g., \\(\\cos(45\\degree)\\) or \\(\\cos(89\\degree)\\) are positive but \\(\\cos(180\\degree), \\cos(91\\degree)\\) are negative.\nWe can find the angle between vectors by \\(\\theta = \\arccos(\\frac{\\textbf{a} \\cdot \\textbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|})\\). A picture from Wikipedia:\n\n\n\n\nSource Wikipedia\n\n\n\n\n\n\n\n\nWait, how are the two dot product definitions the same? How is \\(cos( \\theta)\\) related to such a straightforward sum of components? A few questions I asked when I first encountered the dot product. I accepted the fact and moved on with my life until today. I will try to understand myself and also explain by interactive visualizations."
  },
  {
    "objectID": "posts/dot-product/index.html#explanation",
    "href": "posts/dot-product/index.html#explanation",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "Let’s first simplify the problem as much as possible. Once we build up an intuition for simpler cases, we can come back and try to understand the general cases.\n\n\nFirst, we will only consider the \\(n=2\\) case. Assume that we have 2D vectors: \\(\\textbf{a}=[a_x, a_y]\\) and \\(\\textbf{b}=[b_x, b_y]\\) with the dot product:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\|\\textbf{a}\\|=\\sqrt{a_x^2+a_y^2}\\) is the length of the vector and \\(\\theta\\) is the angle between the vectors.\nSecond, scaling either \\(\\textbf{a}\\) or \\(\\textbf{b}\\) by a real number \\(k\\) does not invalidate the dot product equivalence. Let’s say we scale \\(\\textbf{b}\\) by \\(k\\), then:\n\\[\n\\textbf{a} \\cdot (k \\textbf{b}) = a_x(kb_x)+a_y(kb_y)=\\|\\textbf{a}\\|k\\|\\textbf{b}\\|\\cos \\theta\n\\]\nWith the above, we can simplify further by constraining \\(\\|\\textbf{b}\\| = 1\\). If the equivalence holds for \\(\\|\\textbf{b}\\| = 1\\), then it holds for any \\(\\|k\\textbf{b}\\|\\).\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: false,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board(\n  name, showSlider=true, sliderValue=0.0, \n  showP=true, showPLabel=true\n) {\n  function vec_length(a)  {\n    return Math.sqrt(a[0] * a[0] + a[1] * a[1]);\n  }\n  function vec_unit(a) {\n    const len = vec_length(a);\n    return vec_scale(a, 1 / len);\n  }\n  function vec_scale(a, scalar) {\n    return [a[0] * scalar, a[1] * scalar];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  function vec_cross(a, b) {\n    return a[0] * b[1] - a[1] * b[0];\n  }\n  function vec_mid(a, b) {\n    return vec_scale(vec_add(a, b), 0.5);\n  }\n  function vec_rot90(v) {\n    return [-v[1], v[0]];\n  }\n  function vec_add(a, b) {\n    return [a[0] + b[0], a[1] + b[1]];\n  }\n\n  function calcP(a, b) {\n    const unit_b = vec_unit(b);\n    return vec_scale(unit_b, vec_dot(a, unit_b));\n  }\n\n  function calcUnder(p, a, delta=0.05) {\n    var n = vec_scale(vec_rot90(p), delta / vec_length(p));\n    if (vec_cross(a, p) &lt; 0) {\n      n = vec_scale(n, -1);\n    }\n    return [n, vec_add(p, n)];\n  }\n\n  const a = [2.5, 2];\n  const b = [1, 0];\n  const p = [a[0], 0];\n  var board = init_board(name, 3.5);\n  const pointa = board.create(\"point\", a, {\n    fixed: false,\n    name: \"a\",\n    color: \"blue\"\n  })\n  const pointb = board.create(\"point\", b, {\n    fixed: true,\n    name: \"b\",\n    color: \"red\"\n  });\n  const pointp = board.create(\"point\", [function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[0];\n  }, function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[1];\n  }], {\n    visible: showP,\n    fixed: true,\n    name: \"p\",\n    color: \"green\",\n    size: 5,\n  });\n  const lineGreen = board.create(\"line\", [function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[0];\n  }, function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[1];\n  }], {\n    visible: showP,\n    lastArrow:true,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    strokeColor: \"green\",\n    strokeWidth: 2.5,\n    label:{offsets:[-1,1]}\n  });\n  const linea = board.create(\"line\", [[0, 0], pointa], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"blue\",\n    strokeWidth: 2\n  });\n  const lineb = board.create(\"line\", [[0, 0], pointb], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"red\",\n    strokeWidth: 2\n  });\n\n  const lineap = board.create(\"line\", [pointa, pointp], {\n    visible: showP,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    dash: 2,\n    strokeColor: \"black\",\n    strokeWidth: 1\n  })\n  const rightAngle = board.create(\"nonreflexangle\", [pointa, pointp, pointb], {\n    visible: showP,\n    name: \"\",\n    strokeColor: \"black\",\n    strokeWidth: 0.7,\n    fillColor: \"#00000000\",\n    radius: 0.3\n  });\n  const pointo = board.create(\"point\", [0, 0], {\n    visible: false,\n    fixed: true,\n    name: \"o\",\n  });\n  const alphaAngle = board.create(\"angle\", [pointb, pointo, pointa], {\n    name: \"θ\"\n  });\n \n  const slider = board.create('slider', [[-1, -2], [1, -2], [0, sliderValue, 360]], {\n    visible: showSlider,\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const text = board.create('text', [function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[0];\n  }, function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[1];\n  }, '||a||cos(θ)'], {\n    visible: showP && showPLabel,\n    anchorX: \"middle\",\n    anchorY: \"middle\",\n    // fontSize: 15,\n  })\n  const rot = board.create('transform', [function(){return slider.Value() / 180 * Math.PI;}, [0, 0]], {type:'rotate'});\n  rot.bindTo([pointb, pointa]);\n  return board;\n}\n\n\n\n\n\n\nA visualization for the setup we have so far:\n\nboard_div(\"board1\", 300)\n\n\n\n\n\n\n\nboard1 = make_board(\"board1\", /*showSlider=*/false, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe have the following items shown above:\n\nThe vector \\(\\textbf{a}\\): it is interactive and visualized with blue; feel free to move the point \\(\\textbf{a}\\) around.\nThe vector \\(\\textbf{b}\\): it is visualized with red and \\(\\|\\textbf{b}\\|=1\\).\n\\(\\theta\\): the angle from \\(\\textbf{b}\\) to \\(\\textbf{a}\\)\n\nThe geometric definition implies that the dot product is invariant under rotations. So, if we rotate the both vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) by the same angle \\(\\alpha\\), the dot product won’t change. It is not obvious from the algebraic definition, but a detailed proof is provided in the next section. Let’s trust this property for now until the next section, and simplify our problem for the third time.\nWe can rotate the both vectors such that \\(\\textbf{b}\\) is aligned with the x-axis. Click the button below for the illustration:\n\nviewof mybutton = {\n  const form = Inputs.button(\"Play to Rotate\", {value: null, reduce: () =&gt; {\n    var slider = board2.elementsByName[\"alpha\"];\n    function updateRotate() {\n      if (slider.Value() == 0) {\n        return;\n      }\n      slider.setValue(Math.max(0, slider.Value() - 1));\n      board2.update();\n      setTimeout(updateRotate, 12);\n    }\n    if (slider.Value() == 0) {\n      slider.setValue(60);\n    }\n    updateRotate();\n  }});\n  const scope = DOM.uid().id;\n  const cls = form.classList[0];\n  form.classList.add(scope);\n  form.append(html`&lt;style&gt;\n    .${cls} &gt; button { color: white }\n    &lt;/style&gt;\n  `);\n  return form;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", 300)\n\n\n\n\n\n\n\nboard2 = make_board(\"board2\", /*showSlider=*/true, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe arrived at a simplifed version of the problem: a vector \\(\\textbf{a}=[a_x, a_y]\\) and a fixed vector \\(\\textbf{b}=[1, 0]\\). Let’s check if the definitions agree now. The algebraic dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=a_x\n\\]\nand the geometric is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =  \\|\\textbf{a}\\|\\cos \\theta\n\\]\nnow, we have \\(\\textbf{a} \\cdot \\textbf{b} = a_x =  \\|\\textbf{a}\\|\\cos \\theta\\). This is correct due to the polar coordinates. Alternatively, we can easily derive from the cosine definition. The proof is left as an exercise to the reader. You can use the visualization below to convince yourself:\n\nboard_div(\"board3\", 400)\n\n\n\n\n\n\n\nboard3 = make_board(\"board3\", /*showSlider=*/false, /*sliderValue=*/0, /*showP=*/true)\n\n\n\n\n\n\nWe introduced a new vector \\(\\textbf{p}\\) in the above visualization:\n\nIt is visualized in green and visually represents the dot product\nIt has coordinates \\(\\textbf{p}=[\\|\\textbf{a}\\|\\cos \\theta, 0]=[a_x, 0]\\):\n\nWhere \\(p_x\\) is the dot product as we derived above\n\nThe vector \\(\\textbf{p}\\) is collinear with the vector \\(\\textbf{b}\\)\nSince the vectors \\(\\textbf{p}\\) and \\(\\textbf{b}\\) lie along the same line:\n\nWe can express \\(\\textbf{p}\\) using vector arithmetic: \\(\\textbf{p}=(\\|\\textbf{a}\\|\\cos \\theta)\\textbf{b}\\)\n\\(\\|\\textbf{a}\\|\\cos \\theta\\) is a real number, which means how many \\(\\textbf{b}\\)`s are required to reach the vector \\(\\textbf{p}\\)\nYou can visualize a 1D number line spanned by the vector \\(\\textbf{b}\\), then \\(\\|\\textbf{a}\\|\\cos \\theta\\) is the location of \\(\\textbf{p}\\) in that 1D coordinate system.\n\n\\(p_x\\) can be 0, positive or negative. When negative, the vector \\(\\textbf{p}\\) is opposite to the vector \\(\\textbf{b}\\)\nThe vector \\(\\textbf{p}\\) is actually the projection of the vector \\(\\textbf{a}\\) into \\(\\textbf{b}\\).\nPlay with the point \\(\\textbf{a}\\) to gain more intuition!\n\nTry different values of \\(\\textbf{a}\\) by moving it in the visualization, and verify the following questions:\n\nWhen the dot product is zero?\nWhen the dot product is negative ?\nWhen the dot product is positive ?\nWhat values of \\(\\textbf{a}\\) result in the same dot product?\n\nThe simplified form of the dot product is quite useful for intuition. I visualize this version in my mind when I forget some details of the dot product. At this point, you should be comfortable with the dot product of the simplified form: \\(\\textbf{a} \\cdot [1, 0]\\).\n\n\n\nFirst, let’s visualize the rotation:\n\nboard_div(\"board4\", 500)\n\n\n\n\n\n\n\nboard4 = make_board(\"board4\", /*showSlider=*/true, /*sliderValue=*/10, /*showP=*/true,  /*showP=*/false)\n\n\n\n\n\n\nThere is a slider which represents the angle \\(\\alpha\\) (in degrees) by which both vectors are rotated. Feel free to play with the slider! You can also interact with the point \\(\\textbf{a}\\).\nIt is easy to see why the dot product is invariant under rotations from the geometric definition. The geometric definition relies only on the lengths of the vectors, and the lengths don’t change when rotated.\nHowever, it is not immediately clear from the algebraic definition. To prove it, we derive an alternative formula for the algebraic dot product:\n\\[\n\\begin{align}\n\\|\\textbf{a}-\\textbf{b}\\|^2 &= (a_x-b_x)^2 + (a_y-b_y)^2 \\\\\n&= a_x^2 + b_x^2 - 2a_xb_x + a_y^2 + b_y^2 - 2a_yb_y \\\\\n&= (a_x^2 + a_y^2) + (b_x^2 + b_y^2) - 2(a_xb_x + a_yb_y) \\\\\n&= \\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - 2 \\textbf{a} \\cdot \\textbf{b}\n\\end{align}\n\\]\nwhich gives us:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= a_xb_x + a_yb_y \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nThe dot product above is derived purely from the lengths. Since the lengths remain unchanged under rotations, the algebraic dot product remains unchanged as well, which completes our proof.\nNote that we derived the law of cosine, which is quite cool!\n\n\n\nIt’s not hard to extend the 2D version of the proof to n-dimensional vectors, since we didn’t rely on any properties unique to 2D. The shortest way is:\n\nNote that the dot product of n-dimensional vectors is invariant under rotations. The same proof as in the rotational invariance\nRotate the vectors such that they lie in the xy-plane\nNow, we reduced n-dimensional vectors to 2D vectors, which we already know how to prove\n\nI will provide more alternative proofs below, mostly for myself. They’re optional and collapsed, feel free to read them.\n\n\n\n\n\n\nNoteAlternative Proof 1\n\n\n\n\n\nWe only scaled the vector \\(\\textbf{b}\\) but we can also scale the vector \\(\\textbf{a}\\), so that \\(\\|\\textbf{a}\\|=1\\). Also switch to the polar coordinates: \\(\\textbf{a}=[\\cos \\alpha, \\sin \\alpha]\\) and \\(\\textbf{b}=[\\cos \\beta, \\sin \\beta]\\).\nThen, the geometric dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos(\\alpha - \\beta)\n\\]\nThe algebraic is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\nWe arrive at the cosine subtraction rule: \\[\n\\cos(\\alpha-\\beta)=\\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\n\n\n\n\n\n\n\n\n\nNoteAlternative Proof 2\n\n\n\n\n\nWe will extend the formula from the rotational invariance section further. We derived the following dot product formula:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= \\sum_{i=1}^n a_ib_i \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nFrom the law of cosine, we also have:\n\\[\n\\|\\textbf{a}-\\textbf{b}\\|=\\|\\textbf{a}\\|^2+\\|\\textbf{b}\\|^2-2\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nif we rearrange and combine the above two equations: \\[\n\\begin{align}\n\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =& \\textbf{a} \\cdot \\textbf{b}= \\sum_{i=1}^n a_ib_i  \\\\\n=& \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nNoteAlternative Proof 3\n\n\n\n\n\nWe can use the Pythagorean theorem:\n\\[\n\\begin{align}\n\\|\\textbf{a}\\|^2&=\\|\\textbf{p}\\|^2 + \\|\\textbf{a}-\\textbf{p}\\|^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x-p_x)^2 + (a_y-p_y)^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x^2+a_y^2) + (p_x^2+p_y^2) - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2\\textbf{a}\\cdot\\textbf{p} \\\\\n\\end{align}\n\\]\nThis gives us the following:\n\\[\na_xp_x+a_yp_y=\\textbf{a}\\cdot\\textbf{p}=\\|\\textbf{p}\\|^2\n\\]\nif we substitute \\(\\textbf{p}\\) by \\((\\|\\textbf{a}\\|\\cos\\theta) \\textbf{b}\\):\n\\[\n\\|\\textbf{a}\\|\\cos\\theta(a_xb_x+a_yb_y)=\\|\\textbf{a}\\|\\cos\\theta(\\textbf{a}\\cdot \\textbf{b})=(\\|\\textbf{a}\\|\\cos\\theta)^2\n\\]\nwhich gives back the both geometric and algebraic dot product.\n\n\n\n\n\n\n\n\n\nNoteAlternative Proof 4\n\n\n\n\n\nYou can also find more proofs in proofwiki."
  },
  {
    "objectID": "posts/dot-product/index.html#the-end",
    "href": "posts/dot-product/index.html#the-end",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html",
    "href": "posts/auto-encoding-variational-bayes/index.html",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "My notes for the “Auto-Encoding Variational Bayes” paper. Feel free to ask questions on my telegram channel\n\n\n\n\nLet us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable.\n\n\n\n\nWe use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#method",
    "href": "posts/auto-encoding-variational-bayes/index.html#method",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "Let us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable."
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "href": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "We use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/roc-auc/index.html",
    "href": "posts/roc-auc/index.html",
    "title": "ROC and AUC Interpretation",
    "section": "",
    "text": "A binary classification is a machine learning model that classifies input data into two classes. We need different metrics to train or evaluate the performance of ML models. The Area Under the Receiver Operating Characteristic Curve (ROC AUC) score is a popular metric for evaluating binary classification models. In this post, we will try to understand the intuition behind the ROC AUC with simple and interactive visualizations.\nSubscribe to get a notification about future posts.\n\n\n\nFeel free to skip this section if you are already familiar with the confusion matrix, true positive rate and false positive rate. Next Section\nI suggest you also checkout the following resources:\n\nevidentlyai.com/classification-metrics/… - quite detailed explaination\ndevelopers.google.com/machine-learning/… - beginner friendly explanation\nmadrury.github.io/jekyll/update/statistics/… - my favourite so far that gives a probabilistic intution of ROC AUC score\nwww.alexejgossmann.com/auc/ - more advanced explanation\n\nLet’s say we have two classes (“positive” and “negative”) and a machine learning model that predicts a probability score between 0 and 1. A probability of 0.9 signifes that the input is closer to “positive” than “negative”, a probability of 0.2 signifies that the input is closer to “negative”, and so on.\nHowever, our task is to produce a binary output: “positive” or “negative”. To achieve this, we choose a threshold. Any input with a predicted probability score above the threshold is classified as positive, while inputs with lower scores are classified as negative.\n\n\nThere are four outcomes of the predicted binary output, which can be nicely summarized with the following table, called a confusion matrix:\n\n\n\nconfusion matrix\n\n\nThe green row corresponds to the positive items in our dataset, while the red row corresponds to the negative items in the dataset. The columns correspond to the model predictions. The cells outlined with dark green are the items our model classified correctly, i.e., the accurate predictions of our model.\nNow, we can give a few relevant metrics based on the confusion matrix.\n\n\n\nAccuracy is the proportion of all accurate predictions among all items in the dataset. It is defined as:\n\\[\n\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}\n\\]\nAccuracy can be misleading, especially with imbalanced datasets. For example, if 99% of our dataset is positive, a model that always predicts positive will have an accuracy of 99%, but this doesn’t provide meaningful insight. Hence, we need other metrics.\n\n\n\nThe true positive rate or recall is the proportion of accurate predictions among positive items:\n\\[\n\\text{Recall or TPR} = \\frac{TP}{TP+FN}\n\\]\nThe recall only considers the green row (actual positives) from our confusion table, and completely ignores the red row.\n\n\n\nThe true negative rate or specificity is the proportion of accurate predictions among negative items:\n\\[\n\\text{Specificity or TNR} = \\frac{TN}{FP+TN}\n\\]\nThe specificity only considers the red row (actual negatives) from our confusion table.\n\n\n\nThe false positive rate is the proportion of inaccurate predictions among negative items:\n\\[\n\\text{FPR} = \\frac{FP}{FP+TN}\n\\]\nalternatively:\n\\[\n\\text{FPR}=1-\\text{TNR}\n\\]\nThe false positive rate is related to the true negative rate. However, we will be using FPR more than TNR in the next sections.\n\n\n\n\n\nstats = {\n    function truePositive(positives, threshold) {\n        var truePositive = 0;\n        for (var i in positives) {\n            if (positives[i] &gt; threshold) {\n                truePositive += 1;\n            }\n        }\n        return truePositive;\n    }\n\n    function trueNegative(negatives, threshold) {\n        var trueNegative = 0;\n        for (var i in negatives) {\n            if (negatives[i] &lt;= threshold) {\n                trueNegative += 1;\n            }\n        }\n        return trueNegative;\n    }\n\n    function falsePositive(negatives, threshold) {\n        return negatives.length - trueNegative(negatives, threshold);\n    }\n\n    function falseNegative(positives, threshold) {\n        return positives.length - truePositive(positives, threshold);\n    }\n\n    function recall(positives, threshold) {\n        return truePositive(positives, threshold) / positives.length;\n    }\n\n    function specifity(negatives, threshold) {\n        return trueNegative(negatives, threshold) / negatives.length;\n    }\n\n    function fallout(negatives, threshold) {\n        return 1.0 - specifity(negatives, threshold);\n    }\n\n    return {\n        truePositive: truePositive,\n        trueNegative: trueNegative,\n        falsePositive: falsePositive,\n        falseNegative: falseNegative,\n        recall: recall,\n        specifity: specifity,\n        fallout: fallout\n    }\n}\n\n\n\n\n\n\n\nfunction confusionMatrix(positives, negatives, threshold) {\n    this.positives = positives;\n    this.negatives = negatives;\n    this.threshold = threshold;\n    this.truePositive = stats.truePositive(positives, threshold);\n    this.trueNegative = stats.trueNegative(negatives, threshold);\n    this.falsePositive = stats.falsePositive(negatives, threshold);\n    this.falseNegative = stats.falseNegative(positives, threshold);\n    this.recall = stats.recall(positives, threshold);\n    this.fallout = stats.fallout(negatives, threshold);\n    return this;\n}\n\n\n\n\n\n\n\npositives = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0];\n\n\n\n\n\n\n\nnegatives = [0.1, 0.2, 0.3, 0.5, 0.8, 1.0];\n\n\n\n\n\n\n\nlightRed = \"#ff8886\"\n\n\n\n\n\n\n\nfunction customTable(data) {\n    const table = Inputs.table(data);\n    table.style.setProperty(\"font-size\", \"1.3em\");\n    return table \n}\n\n\n\n\n\n\n\nfunction plot1D(simple, threshold, fpr=false, reverse=false) {\n    var positiveDots = []\n    for (var i in positives) {\n        positiveDots.push({\n            x: positives[i], \n            y: 1,\n            r: positives[i],\n            strokeWidth: !simple && positives[i] &gt; threshold? 3 : 0,\n            stroke: \"green\",\n        })\n    }\n    var negativeDots = [];\n    for (var i in negatives) {\n        var strokeWidth = 0;\n        if (simple) {\n            strokeWidth = 0;\n        } else if (fpr && negatives[i] &gt; threshold) {\n            strokeWidth = 3;\n        } else if (!fpr && negatives[i] &lt;= threshold) {\n            strokeWidth = 3;\n        }\n        negativeDots.push({\n            x: negatives[i],\n            y: 0,\n            r: negatives[i],\n            strokeWidth: strokeWidth,\n            stroke: fpr ? \"#8b0000\" : \"green\"\n            //stroke: \"red\"\n        })\n    }\n    return Plot.plot({\n        r: {range: [0, 20]},\n        height: 100,\n        width: 600,\n        x: { label: \"probability\", reverse: reverse },\n        y: { axis: null },\n        marginRight: 25,\n        marginTop: 25,\n        marginBottom: 25,\n        marginLeft: 25,\n        marks: [\n            Plot.dot(positiveDots, {\n                x: \"x\", y: \"y\", r: \"r\", \n                stroke: \"stroke\", strokeWidth: \"strokeWidth\", \n                fill: \"lightgreen\"\n            }),\n            Plot.dot(negativeDots, {\n                x: \"x\", y: \"y\", r: \"r\", \n                stroke: \"stroke\", strokeWidth: \"strokeWidth\", \n                fill: lightRed\n            }),\n            Plot.line([\n                [Math.min(threshold, 1), 0], \n                [Math.min(threshold, 1), 1]\n            ], { \n                strokeWidth: 3, \n                strokeDasharray: \"2, 6\",\n                opacity: simple ? 0 : 1,\n            })\n        ]\n    })\n}\n\n\n\n\n\n\nLet’s setup a visualization for better understanding. Assume that we have:\n\nA dataset with positive and negative items\nAn ML model that predicts a probability score from 0 to 1, representing the probability that the input belongs to the positive class.\n\nThen, we can visualize our dataset and their probability predictions in the same visualization as below:\n\nplot1D(true, thresholdSlider)\n\n\n\n\n\n\nPositive items in the dataset are shown in green, and negative items are shown in red. The sizes of circles represent the predicted probability scores, with smaller circles representing scores close to 0 and larger circles representing scores close to 1. The items are ordered according to their probability scores, from smallest to largest.\nNext, we choose a threshold depending on the application of our ML model. But, for now, let’s visualize the threshold as well:\n\nplot1D(false, thresholdSlider)\n\n\n\n\n\n\n\nviewof thresholdSlider = html`\n&lt;input type=range min=0 max=1.1 step=0.01 value=0.5 style=\"width:90%\"/&gt;\n`\n\n\n\n\n\n\nThe circles with dark green outline represent items that are accurately classified, in other words, true positives and true negatives.\nWhy not calculate the true positive (TP), false positive (FP), false negative (FP) and true negative (TN) values from the confusion matrix:\n\n{\n    const confusion = confusionMatrix(positives, negatives, thresholdSlider);\n    const data = [{\n        \"Threshold\": Math.min(1, thresholdSlider), \n        \"TP\": confusion.truePositive,\n        \"FN\": confusion.falseNegative,\n        \"FP\": confusion.falsePositive,\n        \"TN\": confusion.trueNegative,\n    }]\n    return customTable(data);\n}\n\n\n\n\n\n\nTip: You can adjust the threshold using the slider above (give it a try), and the tables above and below will update accordingly.\nAnd the metrics as defined above:\n\n{\n    const confusion = confusionMatrix(positives, negatives, thresholdSlider);\n    const correctItems = confusion.truePositive + confusion.trueNegative;\n    const allItems = positives.length + negatives.length;\n    const data = [{\n        \"Accuracy\": correctItems + \"/\" + allItems,\n        \"TPR\": confusion.truePositive + \"/\" + positives.length,\n        \"FPR\": confusion.falsePositive + \"/\" + negatives.length,\n        \"TNR\": confusion.trueNegative + \"/\" + negatives.length,\n    }]\n    return customTable(data)\n}\n\n\n\n\n\n\nPlay with the threshold slider and make sure that you understand different metrics, especially the true positive rate and the false positive rate. Maybe try to reproduce the metrics youself first at different thresholds and compare with the table?\n\n\n\n\n\nChoosing a specific threshold can be difficult since it depends on the particular application. Luckily we have metrics that show the performance of an ML model at varying threshold values. One of them is a receiver operating characteristic curve or ROC curve.\nThe term “Receiver Operating Characteristic” originates from Word War II, where it was used in radar systems for detecting enemy objects.\nThe ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold values.\nLet’s bring back a slightly modified version of our visualization:\n\nplot1D(false, thresholdSlider2, /*fpr=*/true);\n\n\n\n\n\n\nEarlier, we have been visualizing the true negative rate. Now, we are visualizing the false positive rate, outlined with dark red.\n\nviewof thresholdSlider2 = html`\n&lt;input type=range min=0 max=1.1 step=0.01 value=0.5 style=\"width:90%\"/&gt;\n`\n\n\n\n\n\n\nThe true positive and the false positive rates accordingly:\n\n{\n    const confusion = confusionMatrix(positives, negatives, thresholdSlider2);\n    return customTable([{\n        \"Threshold\": Math.min(1, thresholdSlider2), \n        \"TPR\": confusion.truePositive + \"/\" + positives.length,\n        \"FPR\": confusion.falsePositive + \"/\" + negatives.length,\n       \n    }])\n}\n\n\n\n\n\n\nNow, we need to plot FPR (x-axis) against TPR (y-axis).\n\nplot_roc(null, thresholdSlider2)\n\n\n\n\n\n\nTip: You can adjust the slider above, then this visualization will update accordingly.\nPlay with the slider above and compare the values in the table and (x, y) values of the black point.\nThe new 2D visualization is similar to the previous 1D visualization, except the following differences:\n\nThe positives items (green) are visualized along the Y-axis\nThe negatives items (red) are visualized along the X-axis\nThe threshold is visualized by the black circle\n\n\nfunction plot_roc(pointSelection=null, pointThreshold=0) {\n    function roc_curve(positives, negatives) {\n        var thresholds = positives.concat(negatives);\n        thresholds.push(-0.1);\n        thresholds.push(1.1);\n        thresholds.sort();\n        thresholds.reverse();\n        var result = []\n        for (var i in thresholds) {\n            const threshold = thresholds[i]; \n            if (i != 0 && threshold == thresholds[i - 1]) {\n                continue;\n            }\n            const f = stats.fallout(negatives, threshold);\n            const r = stats.recall(positives, threshold);\n            const prevx = (i == 0 ? -1 : result[result.length - 1].x);\n            result.push({x: prevx &gt;= f ? prevx + 1e-8 : f, y: r})\n        }\n        return result;\n    }\n\n    var falloutDots = []\n    for (var i in negatives) {\n        const value = stats.fallout(negatives, negatives[i] - 1e-6);\n        falloutDots.push({\n            x: value, \n            y: 0.0,\n            r: negatives[i] \n        })\n    }\n    var recallDots = []\n    for (var i in positives) {\n        const value = stats.recall(positives, positives[i] - 1e-6);\n        recallDots.push({\n            x: 0.0, \n            y: value,\n            r: positives[i],\n        })\n    }\n\n    var aucPoints = [];\n    for (var i in falloutDots) {\n        for (var j in recallDots) {\n            if (falloutDots[i].r &lt;= recallDots[j].r) {\n                aucPoints.push([falloutDots[i].x, recallDots[j].y])\n            }\n        }\n    }\n    aucPoints.reverse();\n    var point = [0, 0];\n    var pointOpacity = 0;\n    if (pointThreshold != null) { \n        point = [\n            stats.fallout(negatives, pointThreshold), \n            stats.recall(positives, pointThreshold)\n        ];\n        pointOpacity = 1;\n    } \n    if (pointSelection != null) {\n        point = aucPoints[Math.floor(pointSelection / 101 * aucPoints.length)];\n        pointOpacity = 1;\n    }\n    return Plot.plot({\n        grid: true,\n        x: {label: \"False Positive Rate\"},\n        y: {label: \"True Positive Rate\"},\n        r: {range: [0, 20]},\n        aspectRatio: 1.0,\n        width: 400,\n        marks: [\n            Plot.line([[0, point[1]], point], {\n            strokeDasharray: \"1, 7\",\n            }),\n            Plot.line([[point[0], 0], point], {\n            strokeDasharray: \"1, 7\",\n            }),\n            Plot.areaY(roc_curve(positives, negatives), {\n                x: \"x\",\n                y: \"y\",\n                opacity: 0.15,\n            }),\n            Plot.line(roc_curve(positives, negatives), {\n                x: \"x\",\n                y: \"y\",\n                strokeWidth: 2\n            }),\n            Plot.dot(falloutDots, {x: \"x\", y: \"y\", r: \"r\", fill: lightRed}),\n            Plot.dot(recallDots, {x: \"x\", y: \"y\", r: \"r\", fill: \"lightgreen\"}),\n            Plot.line([[0, 0], [1, 1]], {\n                stroke: \"orange\",\n                opacity: 0.0,\n            }),\n            Plot.dot([{x: point[0], y: point[1], r: 0.15}], {\n                x: \"x\", y: \"y\", r: \"r\",\n                fill: \"black\",\n                opacity: pointOpacity\n            }),\n        ]\n    })\n}\n\n\n\n\n\n\n\n\n\nThe area under the ROC curve is called ROC AUC score. ROC AUC stands for “Receiver Operating Characteristic Area Under the Curve”.\nThe ROC AUC score is a single number that summarizes the ML model’s performance across all threshold values. Note that the ROC curve summarizes with a visual plot, whereas the ROC AUC score summarizes with a single number.\n\nplot_roc(null, null)\n\n\n\n\n\n\nThe area of the light gray region in the above visualization is the ROC AUC Score.\nThe ROC AUC score is 0.5 for a classifier that performs no better than random guessing and approaches 1.0 for a classifier with perfect performance.\n\n\n\nThe ROC AUC score has a very nice probabilistic interpretation:\n\nThe ROC AUC is the probability that the model will predict a higher probability score to a randomly selected positive item than a randomly selected negative item.\n\nIt is nicely explained with the visualization that we already have:\n\nplot_roc(pointSelection)\n\n\n\n\n\n\n\nviewof pointSelection = html`\n&lt;input type=range min=1 max=100 step=1 value=50 style=\"width:90%\"/&gt;\n`\n\n\n\n\n\n\nEvery point (e.g., the black dot above) under the ROC curve has a corresponding green circle (follow the dotted horizontal segment) and a corresponding red circle (follow the dotted vertical segment). Then, the mentioned green circle is always bigger or equal to the red circle.\nTip: Try the slider above and compare the matching green circle with the matching red circle.\nThis is the same interpretation as the probabilistic interpretation when the number of items in our dataset approaches infinity.\n\n\n\n\nI hope you enjoyed this post.\nIn the next post, we will work on an efficient Python implementation of the ROC AUC score based on probabilistic intuition.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/roc-auc/index.html#introduction",
    "href": "posts/roc-auc/index.html#introduction",
    "title": "ROC and AUC Interpretation",
    "section": "",
    "text": "A binary classification is a machine learning model that classifies input data into two classes. We need different metrics to train or evaluate the performance of ML models. The Area Under the Receiver Operating Characteristic Curve (ROC AUC) score is a popular metric for evaluating binary classification models. In this post, we will try to understand the intuition behind the ROC AUC with simple and interactive visualizations.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/roc-auc/index.html#recap",
    "href": "posts/roc-auc/index.html#recap",
    "title": "ROC and AUC Interpretation",
    "section": "",
    "text": "Feel free to skip this section if you are already familiar with the confusion matrix, true positive rate and false positive rate. Next Section\nI suggest you also checkout the following resources:\n\nevidentlyai.com/classification-metrics/… - quite detailed explaination\ndevelopers.google.com/machine-learning/… - beginner friendly explanation\nmadrury.github.io/jekyll/update/statistics/… - my favourite so far that gives a probabilistic intution of ROC AUC score\nwww.alexejgossmann.com/auc/ - more advanced explanation\n\nLet’s say we have two classes (“positive” and “negative”) and a machine learning model that predicts a probability score between 0 and 1. A probability of 0.9 signifes that the input is closer to “positive” than “negative”, a probability of 0.2 signifies that the input is closer to “negative”, and so on.\nHowever, our task is to produce a binary output: “positive” or “negative”. To achieve this, we choose a threshold. Any input with a predicted probability score above the threshold is classified as positive, while inputs with lower scores are classified as negative.\n\n\nThere are four outcomes of the predicted binary output, which can be nicely summarized with the following table, called a confusion matrix:\n\n\n\nconfusion matrix\n\n\nThe green row corresponds to the positive items in our dataset, while the red row corresponds to the negative items in the dataset. The columns correspond to the model predictions. The cells outlined with dark green are the items our model classified correctly, i.e., the accurate predictions of our model.\nNow, we can give a few relevant metrics based on the confusion matrix.\n\n\n\nAccuracy is the proportion of all accurate predictions among all items in the dataset. It is defined as:\n\\[\n\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}\n\\]\nAccuracy can be misleading, especially with imbalanced datasets. For example, if 99% of our dataset is positive, a model that always predicts positive will have an accuracy of 99%, but this doesn’t provide meaningful insight. Hence, we need other metrics.\n\n\n\nThe true positive rate or recall is the proportion of accurate predictions among positive items:\n\\[\n\\text{Recall or TPR} = \\frac{TP}{TP+FN}\n\\]\nThe recall only considers the green row (actual positives) from our confusion table, and completely ignores the red row.\n\n\n\nThe true negative rate or specificity is the proportion of accurate predictions among negative items:\n\\[\n\\text{Specificity or TNR} = \\frac{TN}{FP+TN}\n\\]\nThe specificity only considers the red row (actual negatives) from our confusion table.\n\n\n\nThe false positive rate is the proportion of inaccurate predictions among negative items:\n\\[\n\\text{FPR} = \\frac{FP}{FP+TN}\n\\]\nalternatively:\n\\[\n\\text{FPR}=1-\\text{TNR}\n\\]\nThe false positive rate is related to the true negative rate. However, we will be using FPR more than TNR in the next sections."
  },
  {
    "objectID": "posts/roc-auc/index.html#visualization",
    "href": "posts/roc-auc/index.html#visualization",
    "title": "ROC and AUC Interpretation",
    "section": "",
    "text": "stats = {\n    function truePositive(positives, threshold) {\n        var truePositive = 0;\n        for (var i in positives) {\n            if (positives[i] &gt; threshold) {\n                truePositive += 1;\n            }\n        }\n        return truePositive;\n    }\n\n    function trueNegative(negatives, threshold) {\n        var trueNegative = 0;\n        for (var i in negatives) {\n            if (negatives[i] &lt;= threshold) {\n                trueNegative += 1;\n            }\n        }\n        return trueNegative;\n    }\n\n    function falsePositive(negatives, threshold) {\n        return negatives.length - trueNegative(negatives, threshold);\n    }\n\n    function falseNegative(positives, threshold) {\n        return positives.length - truePositive(positives, threshold);\n    }\n\n    function recall(positives, threshold) {\n        return truePositive(positives, threshold) / positives.length;\n    }\n\n    function specifity(negatives, threshold) {\n        return trueNegative(negatives, threshold) / negatives.length;\n    }\n\n    function fallout(negatives, threshold) {\n        return 1.0 - specifity(negatives, threshold);\n    }\n\n    return {\n        truePositive: truePositive,\n        trueNegative: trueNegative,\n        falsePositive: falsePositive,\n        falseNegative: falseNegative,\n        recall: recall,\n        specifity: specifity,\n        fallout: fallout\n    }\n}\n\n\n\n\n\n\n\nfunction confusionMatrix(positives, negatives, threshold) {\n    this.positives = positives;\n    this.negatives = negatives;\n    this.threshold = threshold;\n    this.truePositive = stats.truePositive(positives, threshold);\n    this.trueNegative = stats.trueNegative(negatives, threshold);\n    this.falsePositive = stats.falsePositive(negatives, threshold);\n    this.falseNegative = stats.falseNegative(positives, threshold);\n    this.recall = stats.recall(positives, threshold);\n    this.fallout = stats.fallout(negatives, threshold);\n    return this;\n}\n\n\n\n\n\n\n\npositives = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0];\n\n\n\n\n\n\n\nnegatives = [0.1, 0.2, 0.3, 0.5, 0.8, 1.0];\n\n\n\n\n\n\n\nlightRed = \"#ff8886\"\n\n\n\n\n\n\n\nfunction customTable(data) {\n    const table = Inputs.table(data);\n    table.style.setProperty(\"font-size\", \"1.3em\");\n    return table \n}\n\n\n\n\n\n\n\nfunction plot1D(simple, threshold, fpr=false, reverse=false) {\n    var positiveDots = []\n    for (var i in positives) {\n        positiveDots.push({\n            x: positives[i], \n            y: 1,\n            r: positives[i],\n            strokeWidth: !simple && positives[i] &gt; threshold? 3 : 0,\n            stroke: \"green\",\n        })\n    }\n    var negativeDots = [];\n    for (var i in negatives) {\n        var strokeWidth = 0;\n        if (simple) {\n            strokeWidth = 0;\n        } else if (fpr && negatives[i] &gt; threshold) {\n            strokeWidth = 3;\n        } else if (!fpr && negatives[i] &lt;= threshold) {\n            strokeWidth = 3;\n        }\n        negativeDots.push({\n            x: negatives[i],\n            y: 0,\n            r: negatives[i],\n            strokeWidth: strokeWidth,\n            stroke: fpr ? \"#8b0000\" : \"green\"\n            //stroke: \"red\"\n        })\n    }\n    return Plot.plot({\n        r: {range: [0, 20]},\n        height: 100,\n        width: 600,\n        x: { label: \"probability\", reverse: reverse },\n        y: { axis: null },\n        marginRight: 25,\n        marginTop: 25,\n        marginBottom: 25,\n        marginLeft: 25,\n        marks: [\n            Plot.dot(positiveDots, {\n                x: \"x\", y: \"y\", r: \"r\", \n                stroke: \"stroke\", strokeWidth: \"strokeWidth\", \n                fill: \"lightgreen\"\n            }),\n            Plot.dot(negativeDots, {\n                x: \"x\", y: \"y\", r: \"r\", \n                stroke: \"stroke\", strokeWidth: \"strokeWidth\", \n                fill: lightRed\n            }),\n            Plot.line([\n                [Math.min(threshold, 1), 0], \n                [Math.min(threshold, 1), 1]\n            ], { \n                strokeWidth: 3, \n                strokeDasharray: \"2, 6\",\n                opacity: simple ? 0 : 1,\n            })\n        ]\n    })\n}\n\n\n\n\n\n\nLet’s setup a visualization for better understanding. Assume that we have:\n\nA dataset with positive and negative items\nAn ML model that predicts a probability score from 0 to 1, representing the probability that the input belongs to the positive class.\n\nThen, we can visualize our dataset and their probability predictions in the same visualization as below:\n\nplot1D(true, thresholdSlider)\n\n\n\n\n\n\nPositive items in the dataset are shown in green, and negative items are shown in red. The sizes of circles represent the predicted probability scores, with smaller circles representing scores close to 0 and larger circles representing scores close to 1. The items are ordered according to their probability scores, from smallest to largest.\nNext, we choose a threshold depending on the application of our ML model. But, for now, let’s visualize the threshold as well:\n\nplot1D(false, thresholdSlider)\n\n\n\n\n\n\n\nviewof thresholdSlider = html`\n&lt;input type=range min=0 max=1.1 step=0.01 value=0.5 style=\"width:90%\"/&gt;\n`\n\n\n\n\n\n\nThe circles with dark green outline represent items that are accurately classified, in other words, true positives and true negatives.\nWhy not calculate the true positive (TP), false positive (FP), false negative (FP) and true negative (TN) values from the confusion matrix:\n\n{\n    const confusion = confusionMatrix(positives, negatives, thresholdSlider);\n    const data = [{\n        \"Threshold\": Math.min(1, thresholdSlider), \n        \"TP\": confusion.truePositive,\n        \"FN\": confusion.falseNegative,\n        \"FP\": confusion.falsePositive,\n        \"TN\": confusion.trueNegative,\n    }]\n    return customTable(data);\n}\n\n\n\n\n\n\nTip: You can adjust the threshold using the slider above (give it a try), and the tables above and below will update accordingly.\nAnd the metrics as defined above:\n\n{\n    const confusion = confusionMatrix(positives, negatives, thresholdSlider);\n    const correctItems = confusion.truePositive + confusion.trueNegative;\n    const allItems = positives.length + negatives.length;\n    const data = [{\n        \"Accuracy\": correctItems + \"/\" + allItems,\n        \"TPR\": confusion.truePositive + \"/\" + positives.length,\n        \"FPR\": confusion.falsePositive + \"/\" + negatives.length,\n        \"TNR\": confusion.trueNegative + \"/\" + negatives.length,\n    }]\n    return customTable(data)\n}\n\n\n\n\n\n\nPlay with the threshold slider and make sure that you understand different metrics, especially the true positive rate and the false positive rate. Maybe try to reproduce the metrics youself first at different thresholds and compare with the table?"
  },
  {
    "objectID": "posts/roc-auc/index.html#roc-and-auc",
    "href": "posts/roc-auc/index.html#roc-and-auc",
    "title": "ROC and AUC Interpretation",
    "section": "",
    "text": "Choosing a specific threshold can be difficult since it depends on the particular application. Luckily we have metrics that show the performance of an ML model at varying threshold values. One of them is a receiver operating characteristic curve or ROC curve.\nThe term “Receiver Operating Characteristic” originates from Word War II, where it was used in radar systems for detecting enemy objects.\nThe ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold values.\nLet’s bring back a slightly modified version of our visualization:\n\nplot1D(false, thresholdSlider2, /*fpr=*/true);\n\n\n\n\n\n\nEarlier, we have been visualizing the true negative rate. Now, we are visualizing the false positive rate, outlined with dark red.\n\nviewof thresholdSlider2 = html`\n&lt;input type=range min=0 max=1.1 step=0.01 value=0.5 style=\"width:90%\"/&gt;\n`\n\n\n\n\n\n\nThe true positive and the false positive rates accordingly:\n\n{\n    const confusion = confusionMatrix(positives, negatives, thresholdSlider2);\n    return customTable([{\n        \"Threshold\": Math.min(1, thresholdSlider2), \n        \"TPR\": confusion.truePositive + \"/\" + positives.length,\n        \"FPR\": confusion.falsePositive + \"/\" + negatives.length,\n       \n    }])\n}\n\n\n\n\n\n\nNow, we need to plot FPR (x-axis) against TPR (y-axis).\n\nplot_roc(null, thresholdSlider2)\n\n\n\n\n\n\nTip: You can adjust the slider above, then this visualization will update accordingly.\nPlay with the slider above and compare the values in the table and (x, y) values of the black point.\nThe new 2D visualization is similar to the previous 1D visualization, except the following differences:\n\nThe positives items (green) are visualized along the Y-axis\nThe negatives items (red) are visualized along the X-axis\nThe threshold is visualized by the black circle\n\n\nfunction plot_roc(pointSelection=null, pointThreshold=0) {\n    function roc_curve(positives, negatives) {\n        var thresholds = positives.concat(negatives);\n        thresholds.push(-0.1);\n        thresholds.push(1.1);\n        thresholds.sort();\n        thresholds.reverse();\n        var result = []\n        for (var i in thresholds) {\n            const threshold = thresholds[i]; \n            if (i != 0 && threshold == thresholds[i - 1]) {\n                continue;\n            }\n            const f = stats.fallout(negatives, threshold);\n            const r = stats.recall(positives, threshold);\n            const prevx = (i == 0 ? -1 : result[result.length - 1].x);\n            result.push({x: prevx &gt;= f ? prevx + 1e-8 : f, y: r})\n        }\n        return result;\n    }\n\n    var falloutDots = []\n    for (var i in negatives) {\n        const value = stats.fallout(negatives, negatives[i] - 1e-6);\n        falloutDots.push({\n            x: value, \n            y: 0.0,\n            r: negatives[i] \n        })\n    }\n    var recallDots = []\n    for (var i in positives) {\n        const value = stats.recall(positives, positives[i] - 1e-6);\n        recallDots.push({\n            x: 0.0, \n            y: value,\n            r: positives[i],\n        })\n    }\n\n    var aucPoints = [];\n    for (var i in falloutDots) {\n        for (var j in recallDots) {\n            if (falloutDots[i].r &lt;= recallDots[j].r) {\n                aucPoints.push([falloutDots[i].x, recallDots[j].y])\n            }\n        }\n    }\n    aucPoints.reverse();\n    var point = [0, 0];\n    var pointOpacity = 0;\n    if (pointThreshold != null) { \n        point = [\n            stats.fallout(negatives, pointThreshold), \n            stats.recall(positives, pointThreshold)\n        ];\n        pointOpacity = 1;\n    } \n    if (pointSelection != null) {\n        point = aucPoints[Math.floor(pointSelection / 101 * aucPoints.length)];\n        pointOpacity = 1;\n    }\n    return Plot.plot({\n        grid: true,\n        x: {label: \"False Positive Rate\"},\n        y: {label: \"True Positive Rate\"},\n        r: {range: [0, 20]},\n        aspectRatio: 1.0,\n        width: 400,\n        marks: [\n            Plot.line([[0, point[1]], point], {\n            strokeDasharray: \"1, 7\",\n            }),\n            Plot.line([[point[0], 0], point], {\n            strokeDasharray: \"1, 7\",\n            }),\n            Plot.areaY(roc_curve(positives, negatives), {\n                x: \"x\",\n                y: \"y\",\n                opacity: 0.15,\n            }),\n            Plot.line(roc_curve(positives, negatives), {\n                x: \"x\",\n                y: \"y\",\n                strokeWidth: 2\n            }),\n            Plot.dot(falloutDots, {x: \"x\", y: \"y\", r: \"r\", fill: lightRed}),\n            Plot.dot(recallDots, {x: \"x\", y: \"y\", r: \"r\", fill: \"lightgreen\"}),\n            Plot.line([[0, 0], [1, 1]], {\n                stroke: \"orange\",\n                opacity: 0.0,\n            }),\n            Plot.dot([{x: point[0], y: point[1], r: 0.15}], {\n                x: \"x\", y: \"y\", r: \"r\",\n                fill: \"black\",\n                opacity: pointOpacity\n            }),\n        ]\n    })\n}\n\n\n\n\n\n\n\n\n\nThe area under the ROC curve is called ROC AUC score. ROC AUC stands for “Receiver Operating Characteristic Area Under the Curve”.\nThe ROC AUC score is a single number that summarizes the ML model’s performance across all threshold values. Note that the ROC curve summarizes with a visual plot, whereas the ROC AUC score summarizes with a single number.\n\nplot_roc(null, null)\n\n\n\n\n\n\nThe area of the light gray region in the above visualization is the ROC AUC Score.\nThe ROC AUC score is 0.5 for a classifier that performs no better than random guessing and approaches 1.0 for a classifier with perfect performance.\n\n\n\nThe ROC AUC score has a very nice probabilistic interpretation:\n\nThe ROC AUC is the probability that the model will predict a higher probability score to a randomly selected positive item than a randomly selected negative item.\n\nIt is nicely explained with the visualization that we already have:\n\nplot_roc(pointSelection)\n\n\n\n\n\n\n\nviewof pointSelection = html`\n&lt;input type=range min=1 max=100 step=1 value=50 style=\"width:90%\"/&gt;\n`\n\n\n\n\n\n\nEvery point (e.g., the black dot above) under the ROC curve has a corresponding green circle (follow the dotted horizontal segment) and a corresponding red circle (follow the dotted vertical segment). Then, the mentioned green circle is always bigger or equal to the red circle.\nTip: Try the slider above and compare the matching green circle with the matching red circle.\nThis is the same interpretation as the probabilistic interpretation when the number of items in our dataset approaches infinity."
  },
  {
    "objectID": "posts/roc-auc/index.html#the-end",
    "href": "posts/roc-auc/index.html#the-end",
    "title": "ROC and AUC Interpretation",
    "section": "",
    "text": "I hope you enjoyed this post.\nIn the next post, we will work on an efficient Python implementation of the ROC AUC score based on probabilistic intuition.\nSubscribe to get a notification about future posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "Why are two random vectors near orthogonal in high dimensions?\n\n\n\nmath\n\nrandom\n\nvector\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of ROC AUC Score\n\n\n\nloss\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nROC and AUC Interpretation\n\n\n\nmath\n\ncv\n\nloss\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Equation Intuition\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDot Product Intuition\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nJan 13, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Intuition for Jensen’s Inequality\n\n\n\nconvex function\n\njensen\n\n\n\n\n\n\n\n\n\nJan 4, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\n\nbasics\n\nloss\n\nl1\n\nl2\n\nlasso\n\nridge\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\n\nstable\n\ndiffusion\n\npaper\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes Notes\n\n\n\ndiffusion\n\npaper\n\nautoencoder\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\ndiffusion\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "esl.html",
    "href": "esl.html",
    "title": "The Elements of Statistical Learning Notebooks",
    "section": "",
    "text": "This page contains Jupyter notebooks implementing the algorithms found in the “The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani and Jerome Friedman, proofs and summary of the textbook.\n\nChapter 2\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nWhy are two random vectors near orthogonal in high dimensions?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nFeb 3, 2025\n\n\nImplementation of ROC AUC Score\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 31, 2025\n\n\nROC and AUC Interpretation\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 20, 2025\n\n\nLinear Equation Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 13, 2025\n\n\nDot Product Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 4, 2025\n\n\nGeometric Intuition for Jensen’s Inequality\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nDec 27, 2024\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 22, 2024\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 21, 2024\n\n\nAuto-Encoding Variational Bayes Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 15, 2024\n\n\nDenoising Diffusion Probabilistic Models\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nWhy are two random vectors near orthogonal in high dimensions?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nFeb 3, 2025\n\n\nImplementation of ROC AUC Score\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 31, 2025\n\n\nROC and AUC Interpretation\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 20, 2025\n\n\nLinear Equation Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 13, 2025\n\n\nDot Product Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 4, 2025\n\n\nGeometric Intuition for Jensen’s Inequality\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nDec 27, 2024\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 22, 2024\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 21, 2024\n\n\nAuto-Encoding Variational Bayes Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 15, 2024\n\n\nDenoising Diffusion Probabilistic Models\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "esl.html#jupyter-notebooks-for-the-book-the-elements-of-statistical-learning",
    "href": "esl.html#jupyter-notebooks-for-the-book-the-elements-of-statistical-learning",
    "title": "The Elements of Statistical Learning Notebooks",
    "section": "",
    "text": "This page contains Jupyter notebooks implementing the algorithms found in the book, proofs and summary of the textbook.\n\n\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nWhy are two random vectors near orthogonal in high dimensions?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nFeb 3, 2025\n\n\nImplementation of ROC AUC Score\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 31, 2025\n\n\nROC and AUC Interpretation\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 20, 2025\n\n\nLinear Equation Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 13, 2025\n\n\nDot Product Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 4, 2025\n\n\nGeometric Intuition for Jensen’s Inequality\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nDec 27, 2024\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 22, 2024\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 21, 2024\n\n\nAuto-Encoding Variational Bayes Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 15, 2024\n\n\nDenoising Diffusion Probabilistic Models\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "esl.html#chapter-2",
    "href": "esl.html#chapter-2",
    "title": "The Elements of Statistical Learning Notebooks",
    "section": "",
    "text": "Date\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nWhy are two random vectors near orthogonal in high dimensions?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nFeb 3, 2025\n\n\nImplementation of ROC AUC Score\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 31, 2025\n\n\nROC and AUC Interpretation\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 20, 2025\n\n\nLinear Equation Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 13, 2025\n\n\nDot Product Intuition\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJan 4, 2025\n\n\nGeometric Intuition for Jensen’s Inequality\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nDec 27, 2024\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 22, 2024\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 21, 2024\n\n\nAuto-Encoding Variational Bayes Notes\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nJul 15, 2024\n\n\nDenoising Diffusion Probabilistic Models\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "esl/index.html",
    "href": "esl/index.html",
    "title": "The Elements of Statistical Learning Notebooks",
    "section": "",
    "text": "This page contains Jupyter notebooks implementing the algorithms found in the “The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani and Jerome Friedman, proofs and summary of the textbook.\nThe main repository can be found at github.com/maitbayev/the-elements-of-statistical-learning.\n\nChapter 2\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n2.3 Least Squares and Nearest Neighbors\n\n\n\n\n\n\n2.4 Statistical Decision Theory\n\n\n\n\n\n\n2.5 Local Methods in High Dimensions\n\n\n\n\n\n\n2.6 Statistical Models, Supervised Learning and Function Approximation\n\n\n\n\n\n\n2.7 Structured Regression Models\n\n\n\n\n\n\n2.8 Classes of Restricted Estimators\n\n\n\n\n\n\n2.9 Model Selection and the Bias-Variance Tradeoff\n\n\n\n\n\n\nEx. 2.8\n\n\n\n\n\n\nNo matching items\n\n\n\nChapter 3\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n3.1 Introduction\n\n\n\n\n\n\n3.2 Linear Regression Models and Least Squares\n\n\n\n\n\n\n3.2.2 The Gauss–Markov Theorem\n\n\n\n\n\n\n3.2.3 Multiple Regression From Simple Univariate Regression\n\n\n\n\n\n\n3.2.4 Multiple Outputs\n\n\n\n\n\n\n3.3 Subset Selection\n\n\n\n\n\n\n3.4 Shrinkage Methods\n\n\n\n\n\n\n3.4.1 Ridge Regression\n\n\n\n\n\n\n3.4.2 The Lasso\n\n\n\n\n\n\n3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso\n\n\n\n\n\n\n3.4.4 Least Angle Regression\n\n\n\n\n\n\n3.5 Methods Using Derived Input Directions\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\nChapter 4\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n4.1 Introduction\n\n\n\n\n\n\n4.2 Linear Regression of an Indicator Matrix\n\n\n\n\n\n\n4.3 Linear Discriminant Analysis\n\n\n\n\n\n\n4.3.1 Regularized Discriminant Analysis\n\n\n\n\n\n\n4.3.2 Computations for LDA\n\n\n\n\n\n\n4.3.3 Reduced-Rank Linear Discriminant Analysis\n\n\n\n\n\n\n4.4 Logistic Regression\n\n\n\n\n\n\n4.4.1 Fitting Logistic Regression Models\n\n\n\n\n\n\n4.4.2 Example: South African Heart Disease\n\n\n\n\n\n\n4.4.3 Quadratic Approximations and Inference\n\n\n\n\n\n\n4.4.4 L1 Regularized Logistic Regression\n\n\n\n\n\n\nNo matching items\n\n\n\nChapter 11\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n11.7 Example: ZIP Code Data (WIP)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "esl/chapter-02/2.5-local-methods-in-high-dimensions.html",
    "href": "esl/chapter-02/2.5-local-methods-in-high-dimensions.html",
    "title": "2.5 Local Methods in High Dimensions",
    "section": "",
    "text": "Mean squared error for estimating f(0):\nAssume that the relationship between X and Y is: \\(Y = f(X)\\)\n\\[\n\\begin{align}\n\\text{MSE}(x_0) & = E_\\tau[f(x_0) - \\hat{y_0}]^2\\\\\n& = E_\\tau[(f(x_0) - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - \\hat{y_0})]^2\\\\\n& = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2  + 2(f(x_0) - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - \\hat{y_0})+ (f(x_0) - E_\\tau(\\hat{y_0}))^2]\\\\\n& = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2] + E_\\tau[(E_\\tau(\\hat{y_0}) - f(x_0))^2]\\\\\n& = E_\\tau[\\hat{y_0} - E_\\tau(\\hat{y_0})]^2 + [E_\\tau(\\hat{y_0}) - f(x_0)]^2\\\\\n& = Var_\\tau(\\hat{y_0}) + Bias^2(\\hat{y_0})\n\\end{align}\n\\]\nWe have broken down the MSE into two components: variance and squared bias. Such decomposition is always possible and is known as the bias-variance decomposition.\n(2.26) Suppose that the relationship between Y and X is linear with some noise:\n\\[Y = X^T\\beta + \\varepsilon \\]\nwhere \\(\\varepsilon \\sim N(0, \\sigma^2)\\) and we fit the model by least squares to the training data. For a test point \\(x_0\\) we have \\(\\hat{y_0}=x_0^T\\hat{\\beta}\\) which can be written as \\(\\hat{y_0} = x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}\\) where \\(l_i(x_0)\\) is the \\(i\\)th element of \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\)\nProof:\n\\[\n\\begin{equation}\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n\\hat{\\beta}=\\beta + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon \\\\\n\\end{equation}\n\\]\nand by plugging \\(\\hat{B}\\) into the linear model:\n\\[\n\\begin{equation}\n\\hat{y_0} = x_0^T(\\beta+(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon) \\\\\n\\hat{y_0} = x_0^T\\beta+x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\n\\end{equation}\n\\]\nwe can get \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\) from \\((x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T)^T\\) by using two matrix properties:\n\n\\((\\mathbf{AB})^T=\\mathbf{B}^T\\mathbf{A}^T\\)\n\\((\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}\\)\n\nUnder this model the least square estimates are unbiased, so the expected prediction error will be: \\[\n\\begin{align}\n\\text{EPE}(x_0) & = E_{y_0|x_0}E_\\tau(y_0-\\hat{y_0})^2\\\\\n& = \\text{Var}(y_0|x_0) + Var_\\tau(\\hat{y_0}) + \\text{Bias}^2(\\hat{y_0})\\\\\n& = \\sigma^2 + E_{\\tau}x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\sigma^2 + 0^2\n\\end{align}\n\\]\nProof: \\[\n\\begin{align}\n\\text{EPE}(x_0) & = E_{y_0|x_0}E_\\tau(y_0-\\hat{y_0})^2\\\\\n& = E_{y_0|x_0}E_\\tau((y_0 - f(x_0)) + (f(x_0) - \\hat{y_0}))^2\\\\\n& = E_{y_0|x_0}E_\\tau(y_0 - f(x_0))^2 + 2E_{y_0|x_0}E_\\tau(y_0 - f(x_0))(f(x_0) - \\hat{y_0}) + E_{y_0|x_0}E_\\tau(f(x_0) - \\hat{y_0})^2\\\\\n& = U_1 + U_2 + U_3\n\\end{align}\n\\]\nThere are three components \\(U_1\\), \\(U_2\\), \\(U_3\\) and we’re going to expand them as well.\n\\(U_1 = E_{y_0|x_0}E_\\tau(y_0 - f(x_0))^2 = E_{y_0|x_0}(y_0-f(x_0))^2 = \\sigma^2\\)\nNote: $f(x_0) = E_{y_0|x_0}(y_0) $\n\\(U_2 = 2E_{y_0|x_0}E_\\tau(y_0 - f(x_0))(f(x_0) - \\hat{y_0}) = 0\\)\nNote: \\(E_{y_0|x_0}(y_0-f(x_0)) = 0\\)\n\\(U_3\\):\n\\[\n\\begin{align}\nU_3 & = E_{y_0|x_0}E_\\tau(f(x_0) - \\hat{y_0})^2\\\\\n& = E_{y_0|x_0}E_\\tau((\\hat{y_0} - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - f(x_0)))^2\\\\\n& = E_{y_0|x_0}E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0}))^2 + 2E_{y_0|x_0}E_\\tau[(\\hat{y_0} - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - f(x_0))] + E_{y_0|x_0}E_\\tau(E_\\tau(\\hat{y_0}) - f(x_0))^2\\\\\n& = E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0}))^2 + (E_\\tau(\\hat{y_0}) - f(x_0))^2\\\\\n& = \\text{Var}_\\tau(\\hat{y_0}) + \\text{Bias}_\\tau^2(\\hat{y_0})\n\\end{align}\n\\]\nFinally if we sum all \\(U_i\\) we get: \\[\\text{EPE}(x_0) = U_1+U_2+U_3 = \\sigma^2 + 0 + (\\text{Var}_\\tau(\\hat{y_0}) + \\text{Bias}_\\tau^2(\\hat{y_0}))\\]\n\\(E_\\tau(\\hat{y_0}) = E_\\tau(x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i})=x_0^T\\beta + E(\\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}) = x_0^T\\beta + 0\\) thus \\(\\text{Bias}_\\tau{\\hat{y_0}} = 0\\)\n(2.27) and we can find variance: \\[\n\\begin{align}\n\\text{Var}_\\tau(\\hat{y_0}) & = E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0})) ^ 2\\\\\n& = E_\\tau(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon)\\\\\n& = E_\\tau(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\\varepsilon^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0)\n\\end{align}\n\\]\nwhere \\(\\varepsilon\\varepsilon^T=\\sigma^2\\mathbf{I}_n\\), so we can simplify further: \\[\\text{Var}_\\tau(\\hat{y_0}) = \\sigma^2x_0^{T}E_\\tau[(\\mathbf{X}^T\\mathbf{X})^{-1})]x_0\\]\n(2.28) if N is large and \\(\\tau\\) were selected at random, and assuming E(X) = 0, then \\(\\mathbf{X}^T\\mathbf{X}\\)-&gt;\\(NCov(\\mathbf{X})\\).\nProof: By definition of covariance \\(\\text{Cov}(X) = E[(X-E(X))(X-E(X))^T] = E(XX^T) = \\frac{\\mathbf{X}^T\\mathbf{X}}{N}\\)\nand we can derive that: \\[\n\\begin{align}\nE_{x_0}\\text{EPE}(x_0) & = E_{x_0}x_0^{T}\\text{Cov}^{-1}(X)x_0\\sigma^2/N+\\sigma^2\\\\\n& = \\text{trace}[\\text{Cov}^{-1}(X)\\text{Cov}(x_0)]\\sigma^2/N+\\sigma^2\\\\\n& = \\sigma^2(p/N)+\\sigma^2\n\\end{align}\n\\]\nFIGURE 2.9\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsimulations = 10000\nsample_size = 500\n\ndef least_square_error(x_0, y_0, train_x, train_y):\n    X = np.c_[np.ones((len(train_x), 1)), train_x]\n    beta = np.linalg.lstsq(X, train_y, rcond = None)[0]\n    return (np.dot(np.array([1, *x_0]), beta) - y_0) ** 2\n\n# 1-nearest neighbor error\ndef nn_error(x_0, y_0, train_x, train_y):\n    X = (train_x * train_x).sum(axis=1)\n    return (y_0 - train_y[X.argmin()]) ** 2\n\ncubic_epe_ratio = []\nlinear_epe_ratio = []\n\nfor p in range(1, 11):\n    least_square_epe = [0, 0]\n    nn_epe = [0, 0]\n    for _ in range(simulations):\n        error = np.random.standard_normal(sample_size)\n        train_x = np.random.uniform(-1, 1, size=(sample_size, p))\n        train_y = [train_x[:, 0] + error, \n                   0.5 * (train_x[:, 0] + 1) ** 3 + error]\n        x_0 = np.zeros(p)\n        y_0 = [np.random.standard_normal(),\n               0.5 + np.random.standard_normal()]\n        for i in range(2):\n            least_square_epe[i] += least_square_error(x_0, y_0[i], \n                                                      train_x, train_y[i])\n            nn_epe[i] += nn_error(x_0, y_0[i], \n                                  train_x, train_y[i])\n    for i in range(2):\n        least_square_epe[i] /= simulations\n        nn_epe[i] /= simulations\n    linear_epe_ratio.append(nn_epe[0] / least_square_epe[0])\n    cubic_epe_ratio.append(nn_epe[1] / least_square_epe[1])\n\n# plot\nfig = plt.figure(1, figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\n\naxes.set_title(\"Expected Prediction Error of 1NN vs. OLS\")\n\naxes.plot(np.arange(1, 11), linear_epe_ratio, '-o',\n          color = 'orange', label = \"Linear\")\n\naxes.plot(np.arange(1, 11), cubic_epe_ratio, '-o',\n          color = 'blue', label = \"Cubic\")\n\naxes.legend()\naxes.set_xlabel(\"Dimension\")\naxes.set_ylabel(\"EPE Ration\")\nplt.show()"
  },
  {
    "objectID": "esl/chapter-02/2.6-statistical-models-supervised-learning-and-function-approximation.html",
    "href": "esl/chapter-02/2.6-statistical-models-supervised-learning-and-function-approximation.html",
    "title": "2.6 Statistical Models, Supervised Learning and Function Approximation",
    "section": "",
    "text": "Our goal is to find a useful approximation \\(\\hat{f(x)}\\) to the function \\(f(x)\\) that underlies the predictive relationship between the inputs and outputs.\n\nWe saw that squared error loss lead us to the regression function \\(f(x)=E(Y|X = x)\\) for a qualitive response.\nThe nearest-neighbor methods estimates directly the conditional expections, but may result in large errors for the high dimension input spaces. (The curse of dimensionality)\n\nWe anticipate using other classes of models for f(x) to overcome the dimensionality problems.\n\n2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y)\n(2.29) Suppose that our data arose from a statistical model: \\[Y = f(X) + \\varepsilon\\]\nwhere the random error \\(\\varepsilon\\) has \\(E(\\varepsilon) = 0\\) and independent of X.\nThe additive error model is a useful approximation to the truth. Generally there will be unmeasured variables that also contribute to the output, including measurement error. The additive model assumes that can capture all departures via the error \\(\\varepsilon\\).\nThe assumption in (2.29) that the errors are independent and identically distributed is not strictly necessary. For example, simple modifications can be made to avoid the independence assumption, e.g \\(Var(Y| X = x) = \\sigma(x)\\), and now both the mean and variance depend on X.\n\n\n2.6.2 Supervised Learning\nSuppose that errors are additive and that the model \\(Y = f(X) + \\varepsilon\\). Supervised learning attempts to learn \\(f\\) through a teacher and learns by examples (i.e by a training set of observations (\\(\\mathcal{T} = (x_i, y_i), i = 1...N\\))\n\n\n2.6.3 Function Approximation\nThe goal is to obtain a useful approximation to f(x) for all x in some region of \\(\\mathbb{R}^p\\), given the representations in \\(\\mathcal{T}\\).\nMany of the approximations have associated a set of parameters \\(\\theta\\) that can be modified to suit the data at hand, e.g the linear model \\(f(x)=x^T\\beta\\) has \\(\\theta=\\beta\\). Another class of useful approximators can be expressed as linear basis expansions:\n\\[f_\\theta(x) = \\sum_{k=1}^{K}h_k(x)\\theta_k\\]\nwhere the \\(h_k\\) are a suitable set of functions or transformations of the input vector x. We also encounter nonlinear expansions, such as the sigmoid transformation:\n\\[h_k(x) = \\frac{1}{1+exp(-x^T\\beta_k)}\\]\nWe can use least squares to estimate the parameters \\(\\theta\\) in \\(f_\\theta\\), by minimizing the residual sum-of-squares:\n\\[RSS(\\theta)=\\sum_{i=1}^N(y_i - f_\\theta(x_i)) ^ 2\\]\nWhile least squares is very convenient, it is not only criterion used and in some cases would not make sense. A more general principle for estimation is maximum likelihood estimation. Suppose we have a random sample \\(y_i\\), i = 1…N from a density \\(Pr_\\theta(y)\\) indexed by some parameters \\(\\theta.\\) The log-probability of the observed sample is: \\[L(\\theta)=\\sum_{i=1}^N logPr_\\theta(y_i)\\]\nLeast squares for the additive error model \\(Y = f_\\theta(X)+\\varepsilon\\), with \\(\\varepsilon \\sim N(0, \\sigma^2)\\) is equivalent to maximum likelihood using the conditional likelihood\n\\[Pr(Y|X, \\theta)=N(f_\\theta(X), \\sigma^2)\\]\nThe log-likelihood of the data is: \\[\n\\begin{align}\nL(\\theta) &= \\sum_{i=1}^N log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-f_\\theta(x_i))^2}{2\\sigma^2}}\\right)\\\\\n&= \\sum_{i=1}^N log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\sum_{i=1}^N(\\frac{(y_i-f_\\theta(x_i))^2}{2\\sigma^2})\\\\\n&= \\sum_{i=1}^N log \\left((2\\pi\\sigma^2)^{-\\frac{1}{2}}\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-f_\\theta(x_i))^2\\\\\n&= \\sum_{i=1}^N \\left(-\\frac{1}{2}log(2\\pi)-log(\\sigma)\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-f_\\theta(x_i))^2\\\\\n&= \\frac{N}{2}log(2\\pi)-Nlog(\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-f_\\theta(x_i))^2\n\\end{align}\n\\]\nand the only term involving \\(\\theta\\) is the last.\nA more interesting example is the multinomial likelihood for the regression function Pr(G|X) for a qualitative output G. Suppose we have a model \\(Pr(G = \\mathcal{G}_k| X = x) = p_{k, \\theta}(x), k = 1...K\\) indexed by \\(\\theta\\). Then the log-likelihood (a.k.a the cross-entropy) is :\n\\[L(\\theta)=\\sum_{i=1}^N log (p_{g_i, \\theta}(x_i))\\]"
  },
  {
    "objectID": "esl/chapter-02/2.3-least-squares-and-nearest-neighbors.html",
    "href": "esl/chapter-02/2.3-least-squares-and-nearest-neighbors.html",
    "title": "2.3 Least Squares and Nearest Neighbors",
    "section": "",
    "text": "2.3.3 From Least Squares to Nearest Neighbors\n\nGenerates 10 means \\(m_k\\) from a bivariate Gaussian distrubition for each color:\n\n\\(N((1, 0)^T, \\textbf{I})\\) for BLUE\n\\(N((0, 1)^T, \\textbf{I})\\) for ORANGE\n\nFor each color generates 100 observations as following:\n\nFor each observation it picks \\(m_k\\) at random with probability 1/10.\nThen generates a \\(N(m_k,\\textbf{I}/5)\\)\n\n\n\n%matplotlib inline\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsample_size = 100\n\ndef generate_data(size, mean):\n    identity = np.identity(2)\n    m = np.random.multivariate_normal(mean, identity, 10)\n    return np.array([\n        np.random.multivariate_normal(random.choice(m), identity / 5)\n        for _ in range(size)\n    ])\n\ndef plot_data(orange_data, blue_data): \n    axes.plot(orange_data[:, 0], orange_data[:, 1], 'o', color='orange')\n    axes.plot(blue_data[:, 0], blue_data[:, 1], 'o', color='blue')\n    \nblue_data = generate_data(sample_size, [1, 0])\norange_data = generate_data(sample_size, [0, 1])\n\ndata_x = np.r_[blue_data, orange_data]\ndata_y = np.r_[np.zeros(sample_size), np.ones(sample_size)]\n\n# plotting\nfig = plt.figure(figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\nplot_data(orange_data, blue_data)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.3.1 Linear Models and Least Squares\n\\[\\hat{Y} = \\hat{\\beta_0} + \\sum_{j=1}^{p} X_j\\hat{\\beta_j}\\]\nwhere \\(\\hat{\\beta_0}\\) is the intercept, also know as the bias. It is convenient to include the constant variable 1 in X and \\(\\hat{\\beta_0}\\) in the vector of coefficients \\(\\hat{\\beta}\\), and then write as:\n\\[\\hat{Y} = X^T\\hat{\\beta} \\]\n\nResidual sum of squares\nHow to fit the linear model to a set of training data? Pick the coefficients \\(\\beta\\) to minimize the residual sum of squares:\n\\[RSS(\\beta) = \\sum_{i=1}^{N} (y_i - x_i^T\\beta) ^ 2 = (\\textbf{y} - \\textbf{X}\\beta)^T (\\textbf{y} - \\textbf{X}\\beta)\\]\nwhere \\(\\textbf{X}\\) is an \\(N \\times p\\) matrix with each row an input vector, and \\(\\textbf{y}\\) is an N-vector of the outputs in the training set. Differentiating w.r.t. β we get the normal equations:\n\\[\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\beta) = 0\\]\nIf \\(\\mathbf{X}^T\\mathbf{X}\\) is nonsingular, then the unique solution is given by:\n\\[\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\nclass LinearRegression:\n    def fit(self, X, y):\n        X = np.c_[np.ones((X.shape[0], 1)), X]\n        self.beta = np.linalg.inv(X.T @ X) @ X.T @ y\n\n        return self\n    \n    def predict(self, x):\n        return np.dot(self.beta, np.r_[1, x])\n\nmodel = LinearRegression().fit(data_x, data_y)\nprint(\"beta = \", model.beta)\n\nbeta =  [ 0.52677771 -0.15145005  0.15818643]\n\n\n\n\nExample of the linear model in a classification context\nThe fitted values \\(\\hat{Y}\\) are converted to a fitted class variable \\(\\hat{G}\\) according to the rule:\n\\[\n\\begin{equation}\n\\hat{G} = \\begin{cases}\n\\text{ORANGE} & \\text{ if } \\hat{Y} \\gt 0.5 \\\\\n\\text{BLUE    } & \\text{ if } \\hat{Y} \\leq 0.5\n\\end{cases}\n\\end{equation}\n\\]\n\nfrom itertools import filterfalse, product\n\ndef plot_grid(orange_grid, blue_grid):\n    axes.plot(orange_grid[:, 0], orange_grid[:, 1], '.', zorder = 0.001,\n              color='orange', alpha = 0.3, scalex = False, scaley = False)\n\n    axes.plot(blue_grid[:, 0], blue_grid[:, 1], '.', zorder = 0.001,\n          color='blue', alpha = 0.3, scalex = False, scaley = False)\n\nplot_xlim = axes.get_xlim()\nplot_ylim = axes.get_ylim()\n\ngrid = np.array([*product(np.linspace(*plot_xlim, 50), np.linspace(*plot_ylim, 50))])\n\nis_orange = lambda x: model.predict(x) &gt; 0.5\n\norange_grid = np.array([*filter(is_orange, grid)])\nblue_grid = np.array([*filterfalse(is_orange, grid)])\n\naxes.clear()\naxes.set_title(\"Linear Regression of 0/1 Response\")\nplot_data(orange_data, blue_data)\nplot_grid(orange_grid, blue_grid)\n\nfind_y = lambda x: (0.5 - model.beta[0] - x * model.beta[1]) / model.beta[2]\naxes.plot(plot_xlim, [*map(find_y, plot_xlim)], color = 'black', \n          scalex = False, scaley = False)\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Nearest-Neighbor Methods\n\\[\\hat{Y}(x) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\\]\nwhere \\(N_k(x)\\) is the neighborhood of \\(x\\) defined by the \\(k\\) closest points \\(x_i\\) in the training sample.\n\nclass KNeighborsRegressor:\n    def __init__(self, k):\n        self._k = k\n\n    def fit(self, X, y):\n        self._X = X\n        self._y = y\n        return self\n    \n    def predict(self, x):\n        X, y, k = self._X, self._y, self._k\n        distances = ((X - x) ** 2).sum(axis=1)\n      \n        return np.mean(y[distances.argpartition(k)[:k]])\n\n\ndef plot_k_nearest_neighbors(k):\n    model = KNeighborsRegressor(k).fit(data_x, data_y)\n    is_orange = lambda x: model.predict(x) &gt; 0.5\n    orange_grid = np.array([*filter(is_orange, grid)])\n    blue_grid = np.array([*filterfalse(is_orange, grid)])\n\n    axes.clear()\n    axes.set_title(str(k) + \"-Nearest Neighbor Classifier\")\n\n    plot_data(orange_data, blue_data)\n    plot_grid(orange_grid, blue_grid)\n\nplot_k_nearest_neighbors(1)\nfig\n\n\n\n\n\n\n\n\nIt appears that k-nearest-neighbor have a single parameter (k), however the effective number of parameters is N/k and is generally bigger than the p parameters in least-squares fits. Note: if the neighborhoods were nonoverlapping, there would be N/k neighborhoods and we would fit one parameter (a mean) in each neighborhood.\n\nplot_k_nearest_neighbors(15)\n\nfig"
  },
  {
    "objectID": "esl/chapter-02/exercise-solutions.html",
    "href": "esl/chapter-02/exercise-solutions.html",
    "title": "Ex. 2.8",
    "section": "",
    "text": "Compare the classification performance of linear regression and k– nearest neighbor classification on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from the book website www-stat.stanford.edu/ElemStatLearn.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef filtered_data(path):\n    data_all = np.loadtxt(path)\n    mask = np.in1d(data_all[:, 0], (2, 3))\n    data_x = data_all[mask, 1: ]\n    data_y = data_all[mask, 0]\n    return data_x, data_y\n\ntrain_x, train_y = filtered_data('../data/zipcode/zip.train')\ntest_x, test_y = filtered_data('../data/zipcode/zip.test')\nk_list = [1, 3, 5, 7, 15]\n\ndef knn_error(k, x, y, data_x, data_y):\n    distances = ((data_x - x)**2).sum(axis=1)\n    return (np.mean(data_y[distances.argpartition(k)[:k]]) - y) ** 2\n\ndef std(squared_errors):\n    \"\"\"standard deviation of the given squared errors.\"\"\"\n    return np.sqrt(np.mean(squared_errors))\n\ndef knn_stds():\n    train_stds = []\n    test_stds = []\n    for k in k_list:\n        train_errors = [knn_error(k, x, y, train_x, train_y)\n                        for (x, y) in zip(train_x, train_y)]\n        train_stds.append(std(train_errors))\n        test_errors = [knn_error(k, x, y, train_x, train_y)\n                        for (x, y) in zip(test_x, test_y)]\n        test_stds.append(std(test_errors))\n    return train_stds, test_stds\n\ndef least_square_stds():\n    X = np.c_[np.ones((len(train_x), 1)), train_x]\n    beta = np.linalg.lstsq(X, train_y, rcond = None)[0]\n    error = lambda x, y: (np.dot(np.array([1, *x]), beta) - y) ** 2\n\n    train_stds = []\n    test_stds = []\n\n    for k in k_list:\n        train_errors = [error(x, y) for (x, y) in zip(train_x, train_y)]\n        train_stds.append(std(train_errors))\n        test_errors = [error(x, y) for (x, y) in zip(test_x, test_y)]\n        test_stds.append(std(test_errors))\n\n    return train_stds, test_stds\n\nfig = plt.figure(figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\n\n# kNN plotting\ntrain_stds, test_stds = knn_stds()\naxes.plot(k_list, train_stds, '-', \n          color = 'C0', label = 'knn-train')\naxes.plot(k_list, test_stds, '-', \n          color = 'C1', label = 'knn-test')\n\n# least square plotting\ntrain_stds, test_stds = least_square_stds()\naxes.plot(k_list, train_stds, '-', \n          color = 'C2', label = 'least-square-train')\naxes.plot(k_list, test_stds, '-', \n          color = 'C3', label = 'least-square-test')\n\naxes.legend()\naxes.set_xlabel(\"k\")\naxes.set_ylabel(\"Error\")\nplt.show()"
  },
  {
    "objectID": "esl/chapter-02/2.7-structured-regression-models.html",
    "href": "esl/chapter-02/2.7-structured-regression-models.html",
    "title": "2.7 Structured Regression Models",
    "section": "",
    "text": "We have seen that although kNN and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more efficient use of the data. This section introduces classes of such approaches.\n\n2.7.1 Difficulty of the Problem\n(2.37) RSS criterian for an arbitrary function \\(f\\):\n\\[RSS(f) = \\sum_{i=1}^N(y_i-f(x_i))^2\\]\nMinimizing (2.37) leads to infinitely many solutions: any function \\(\\hat{f}\\) passing through the training points is a solution. It might be a poor predictor at test points different from the training points. If there are multiple observations at each point \\(x_i\\), i.e \\(y_{il}, l = 1 ... N\\), the risk is limited.\nIn order to obtain useful results for finite N, we must restrict the eligible solutions to a smaller set of functions."
  },
  {
    "objectID": "esl/chapter-02/2.4-statistical-decision-theory.html",
    "href": "esl/chapter-02/2.4-statistical-decision-theory.html",
    "title": "2.4 Statistical Decision Theory",
    "section": "",
    "text": "Loss function\nThe most common is squared error loss: \\(L(Y, f(X)) = (Y - f(X))^2\\)\n\n\nExpected prediction Error\n(2.9, 2.10) This leads us to a criterion for choosing \\(f\\),\n\\[EPE(f) = E(Y - f(X))^2 = \\int{[y - f(x)]^2}Pr(dx, dy)\\]\n(2.11) By conditioning on X, we can write EPE as:\n\\[EPE(f) = E_X E_{Y|X} ([Y - f(X)]^2|X) \\]\nProof:\n\\[\n\\begin{equation}\n\\int{[y - f(x)]^2}Pr(dx, dy) \\\\\n= \\int{[y - f(x)]^2}p(x, y)dxdy \\\\\n= \\int{[y - f(x)]^2}p(x)p(y | x)dxdy \\\\\n= \\int_x { \\left( \\int_y {[y - f(x)]^2p(y | x)dy} \\right)p(x)dx } \\\\\n= E_X E_{Y|X} ([Y - f(X)]^2|X)\n\\end{equation}\n\\]\n(2.12) It suffices to minimize EPE pointwise: \\[f(x) = argmin_c E_{Y|X} ( [Y - c]^2 | X = x)\\]\n(2.13) The solution is : \\[f(x)=E(Y | X = x)\\]\nThe nearest-neighbor methods attempt to directly implement this recipe using the training data. Since there is typically at most one observation at any point x, we settle for: \\[\\hat{f}(x) = \\text{Ave}(y_i|x_i \\in N_k(x))\\]\nFor large training sample size N, the points in the neighborhood are likely to be close to x, and as k gets large the average will get more stable.\nHow does linear regression fit into this framework ?\n\\[f(x) \\approx x^T\\beta\\]\nPlugging this into EPE (2.9) and differentiating we can solve for β:\n\\[\\beta= [E(XX^T)]^{-1}E(XY)\\]\nProof:\n\\[\n\\begin{equation}\n\\int{[y - x^T\\beta]^2\\text{ Pr}(dx, dy)} \\\\\n= \\int{[y^2 - 2yx^T+(x^T\\beta)^2]\\text{ Pr}(dx, dy)} \\\\\n\\end{equation}\n\\]\nDifferentiating w.r.t \\(\\beta\\):\n\\[\n\\begin{equation}\n2\\int{xx^T\\beta\\text{ Pr}(dx, dy)} -  2\\int{xy\\text{ Pr}(dx, dy)} = 0 \\\\\n2 \\times (E(XX^T\\beta) - E(XY)) = 0 \\\\\nE(XX^T)\\beta = E(XY) \\\\\n\\beta = [E(XX^T)]^{-1}E(XY)\n\\end{equation}\n\\]\nThe least squares solution \\(\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) amounts to replacing the expectation in \\(\\beta = [E(XX^T)]^{-1}E(XY)\\) by averages over the training data. Note: Each expectation produces N times more than average, however the constant (i.e 1/N) in two expectations cancel out each other.\n\n\nBayes Classifier\n\n%matplotlib inline\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsample_size = 100\n\ndef generate_data(means, size):\n    return np.array([\n        np.random.multivariate_normal(random.choice(means), np.eye(2) / 5)\n        for _ in range(size)\n    ])\n\ndef plot_data(orange_data, blue_data): \n    axes.plot(orange_data[:, 0], orange_data[:, 1], 'o', color='orange')\n    axes.plot(blue_data[:, 0], blue_data[:, 1], 'o', color='blue')\n\nblue_means = np.random.multivariate_normal([1, 0], np.identity(2), 10)\norange_means = np.random.multivariate_normal([0, 1], np.identity(2), 10)\n\nblue_data = generate_data(blue_means, sample_size)\norange_data = generate_data(orange_means, sample_size)\n\n# plotting\nfig = plt.figure(figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\nplot_data(orange_data, blue_data)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom itertools import filterfalse, product\nfrom scipy.stats import multivariate_normal\n\ndef plot_grid(orange_grid, blue_grid):\n    axes.plot(orange_grid[:, 0], orange_grid[:, 1], '.', zorder = 0.001,\n          color='orange', alpha = 0.25, scalex = False, scaley = False)\n\n    axes.plot(blue_grid[:, 0], blue_grid[:, 1], '.', zorder = 0.001,\n          color='blue', alpha = 0.25, scalex = False, scaley = False)\n\ndef pdf(x, means):\n    return np.mean([multivariate_normal.pdf(x, mean = m, cov = np.eye(2) / 5) \n                    for m in means], axis = 0)\n\nplot_xlim = axes.get_xlim()\nplot_ylim = axes.get_ylim()\n\ngrid = np.array([*product(np.linspace(*plot_xlim, 50), np.linspace(*plot_ylim, 50))])\n\norange_pdf = pdf(grid, orange_means)\nblue_pdf = pdf(grid, blue_means)\n\n# Plotting\naxes.clear()\naxes.set_title(\"Bayes Optimal Classifier\")\n\nplot_data(orange_data, blue_data)\nplot_grid(grid[orange_pdf &gt;= blue_pdf], grid[orange_pdf &lt; blue_pdf])\n\nfig"
  },
  {
    "objectID": "esl/chapter-02/2.9-model-selection-and-the-bias-variance-tradeoff.html",
    "href": "esl/chapter-02/2.9-model-selection-and-the-bias-variance-tradeoff.html",
    "title": "2.9 Model Selection and the Bias-Variance Tradeoff",
    "section": "",
    "text": "All the models described have a smoothing or complexity parameter that has to be determined:\n\nthe multiplier of the penalty term;\nthe width of the kernel\nor the number of basis functions\n\nWe cannont use RSS on the training data to determine these parameters, since we would always pick those that gave interpolating fits and have zero residuals.\nThe kNN regression fit \\(\\hat{f_k}(x_0)\\) illustrates the competing forces that effect the predictive ability of such approximations. Suppose the data arise from a model \\(Y = f(X) + \\varepsilon\\) with \\(E(\\varepsilon)=0\\) and \\(Var(\\varepsilon) = \\sigma^2\\). We assume that the values of \\(x_i\\) in the sample are fixed. The EPE at \\(x_0\\):\n\\[\n\\begin{align}\nEPE_k(x_0) &= E[(Y - \\hat{f_k}(x_0))^2|X=x_0]\\\\\n&=\\sigma^2 + [Bias^2(\\hat{f_k}(x_0)) + Var_\\tau(\\hat{f_k}(x_0))]\\\\\n&=\\sigma^2 + \\left[f(x_0) - \\frac{1}{k}\\sum_{l=1}^k{f(x_{(l)})} \\right]^2 + \\frac{\\sigma^2}{k}\n\\end{align}\n\\]\nThe subscripts in parentheses (\\(l\\)) indicates the sequence of nearest neighbors to \\(x_0\\). There are three terms in this expression:\n\n\\(\\sigma^2\\) is the irreducible error - is beyond our control, even if we know the true \\(f(x_0)\\).\nThe bias term and the expected value of the estimate - \\([E_\\tau(\\hat{f_k}(x_0))-f(x_0)]^2\\) - where the expected averages the randomness in the training data. This term increases with \\(k\\) if the function is smooth.\nThe variance term and it decreases as the inverse of k. The expected value of the variance is:\n\n\\[\n\\begin{align}\nVar_\\tau(\\hat{f_k}(x_0)) &= E_\\tau\\left[\\hat{f_k}(x_0) - E_\\tau(\\hat{f_k}(x_0))\\right]^2\\\\\n&= E_\\tau\\left[\\frac{1}{k}\\sum_{l=1}^k (f(x_{(l)}) + \\varepsilon_l) - \\frac{1}{k}\\sum_{l=1}^k{f(x_{(l)})}\\right]^2\\\\\n&= E_\\tau\\left[\\frac{1}{k}\\sum_{l=1}^k \\varepsilon_l\\right]^2\\\\\n&= \\frac{1}{k^2}E_\\tau\\left[\\sum_{l=1}^k \\varepsilon_l\\right]^2\\\\\n&= \\frac{1}{k^2}E_\\tau\\left[\\sum_{l=1}^k \\varepsilon_l^2\\right]\\\\\n&= \\frac{\\sigma^2}{k}\n\\end{align}\n\\]\nAs the model complexity of our procedure is increased, the variance tends to increase and the squared bias tends to decrease. The opposite behavior occurs as the model complexity is decreased."
  },
  {
    "objectID": "esl/chapter-02/2.8-classes-of-restricted-estimators.html",
    "href": "esl/chapter-02/2.8-classes-of-restricted-estimators.html",
    "title": "2.8 Classes of Restricted Estimators",
    "section": "",
    "text": "The variety of nonparametric regression techniques or learning methods fall into a number of different classes depending on the nature of the restrictions imposed. Each of the classes has associated with it one or more parameters, sometimes called smoothing parameters, that control the effective size of the local neighborhood.\n\n2.8.1 Roughness Penalty and Bayesian Methods\nHere the class of functions is controlled by explicitly penalizing RSS(f) with a roughness penalty\n\\[PRSS(f; \\lambda) = RSS(f) + \\lambda J(f)\\]\nThe user-selected \\(J(f)\\) will be large for functions \\(f\\) that vary too rapidly over small regions of input space. For example, the popular cubic smoothing spline for one-dimensional inputs is the solution to the PRSS:\n\\[PRSS(f; \\lambda) = \\sum_{i=1}^N(y_i - f(x_i))^2 + \\lambda\\int[f^{''}(x)]^2dx\\]\nThe amount of penalty is dictated by \\(\\lambda&gt;=0\\): - For \\(\\lambda=0\\), no penalty imposed, and any interpolating function will do - While for \\(\\lambda = \\infty\\) only linear functions are permitted\nPenalty function, or regularization methods, express our prior belief that the type of functions we seek exhibit a certain type of smooth behavior, and indeed can usually be cast in a Bayesian framework: - The penalty J corresponds to a log-prior - and \\(PRSS(f; \\lambda)\\) the log-posterior distribution; - and minimizing \\(PRSS(f; \\lambda)\\) amounts to finding the posterior mode.\n\n\n2.8.2 Kernel Methods and Local Regression\nThese methods can be thought of as explicitly providing estimates of the regression function or conditional expectation by specifying the nature of the local neighborhood, and of the class of regular functions fitted locally.\nThe local neighborhood is specified by a kernel function \\(K_\\lambda(x_0, x)\\) which assigns weights to points x in a region around \\(x_0\\), e.g the Gaussian kernel based on the Gaussian density function:\n\\[K_\\lambda(x_0, x) = \\frac{1}{\\lambda}exp\\left[-\\frac{||x-x_0||^2}{2\\lambda}\\right]\\]\nand assigns weights to points that die exponentially with their squared euclidean distance from \\(x_0\\) and \\(\\lambda\\) corresponds to the variance of the Gaussian density and controls the width of the neighborhood.\n(2.40) The simplest form of kernel estimate is the Nadaraya-Watson weighted average:\n\\[\\hat{f}(x_0)=\\frac{\\sum_{i=1}^N K_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}\\]\n(2.41) In general we can define a local regression estimate of \\(f(x_0)\\) as \\(f_\\hat{\\theta}(x_0)\\), where \\(\\hat{\\theta}\\) minimizes:\n\\[RSS(f_\\theta, x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)(y_i - f_\\theta(x_i))^2\\]\nand \\(f_\\theta\\) is some parameterized function, e.g:\n\n\\(f_\\theta(x) = \\theta_0\\), the constant function; this results in the Nadaraya-Watson estimate in (2.41)\n\\(f_\\theta(x) = \\theta_0+\\theta_1x\\) gives the local linear regression model.\n\nThese methods needs to be modified in high dimensions, to avoid curse of dimensionality.\n\n\n2.8.3 Basis Functions and Dictionary Methods\nThis class of methods includes the linear and polynomial expansions, but more importantly a wide variety of more flexible models. The models for \\(f\\) is a linear expansion of basis functions:\n\\[f_\\theta(x) = \\sum_{m=1}^M \\theta_{m}h_m(x)\\]\nthe term linear here refers to the action of the parameters \\(\\theta\\).\nTODO: 1D polynomial splines of degrees K.\nRadial basis functions are symmetric p-dimensional kernels located at particular centroids:\n\\[f_\\theta(x) = \\sum_{m=1}^M{K_{\\lambda_m}(\\mu_m, x)}\\theta_m\\]\ne.g the Gaussian kernel \\(K_\\lambda(\\mu, x) = e^{-||x-\\mu||^2}/2\\lambda\\) is popular. Radial basis functions have centroids \\(\\mu_m\\) and scales \\(\\lambda_m\\) that have to be determined. In general, we would like the data to dictate them.\nA single-layer feed-forward neural networks model with linear output weights can be thought of as an adaptive basis function methods:\n\\[f_\\theta(x) = \\sum_{m=1}^M{\\beta_m\\sigma(\\alpha_m^T{x}+b_m)}\\]\nwhere \\(\\sigma(x) = 1/(1+e^{-x})\\) is know as the activation function.\nThese adaptively chosen basis function methods are also know as dictionary methods, where one has available a infinite set or dictionary \\(\\mathcal{D}\\) of candidate basis function from which to choose."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.5-local-methods-in-high-dimensions.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.5-local-methods-in-high-dimensions.html",
    "title": "2.5 Local Methods in High Dimensions",
    "section": "",
    "text": "Mean squared error for estimating f(0):\nAssume that the relationship between X and Y is: \\(Y = f(X)\\)\n\\[\n\\begin{align}\n\\text{MSE}(x_0) & = E_\\tau[f(x_0) - \\hat{y_0}]^2\\\\\n& = E_\\tau[(f(x_0) - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - \\hat{y_0})]^2\\\\\n& = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2  + 2(f(x_0) - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - \\hat{y_0})+ (f(x_0) - E_\\tau(\\hat{y_0}))^2]\\\\\n& = E_\\tau[(E_\\tau(\\hat{y_0}) - \\hat{y_0})^2] + E_\\tau[(E_\\tau(\\hat{y_0}) - f(x_0))^2]\\\\\n& = E_\\tau[\\hat{y_0} - E_\\tau(\\hat{y_0})]^2 + [E_\\tau(\\hat{y_0}) - f(x_0)]^2\\\\\n& = Var_\\tau(\\hat{y_0}) + Bias^2(\\hat{y_0})\n\\end{align}\n\\]\nWe have broken down the MSE into two components: variance and squared bias. Such decomposition is always possible and is known as the bias-variance decomposition.\n(2.26) Suppose that the relationship between Y and X is linear with some noise:\n\\[Y = X^T\\beta + \\varepsilon \\]\nwhere \\(\\varepsilon \\sim N(0, \\sigma^2)\\) and we fit the model by least squares to the training data. For a test point \\(x_0\\) we have \\(\\hat{y_0}=x_0^T\\hat{\\beta}\\) which can be written as \\(\\hat{y_0} = x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}\\) where \\(l_i(x_0)\\) is the \\(i\\)th element of \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\)\nProof:\n\\[\n\\begin{equation}\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon) \\\\\n\\hat{\\beta}=\\beta + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon \\\\\n\\end{equation}\n\\]\nand by plugging \\(\\hat{B}\\) into the linear model:\n\\[\n\\begin{equation}\n\\hat{y_0} = x_0^T(\\beta+(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon) \\\\\n\\hat{y_0} = x_0^T\\beta+x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\n\\end{equation}\n\\]\nwe can get \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\) from \\((x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T)^T\\) by using two matrix properties:\n\n\\((\\mathbf{AB})^T=\\mathbf{B}^T\\mathbf{A}^T\\)\n\\((\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}\\)\n\nUnder this model the least square estimates are unbiased, so the expected prediction error will be: \\[\n\\begin{align}\n\\text{EPE}(x_0) & = E_{y_0|x_0}E_\\tau(y_0-\\hat{y_0})^2\\\\\n& = \\text{Var}(y_0|x_0) + Var_\\tau(\\hat{y_0}) + \\text{Bias}^2(\\hat{y_0})\\\\\n& = \\sigma^2 + E_{\\tau}x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\sigma^2 + 0^2\n\\end{align}\n\\]\nProof: \\[\n\\begin{align}\n\\text{EPE}(x_0) & = E_{y_0|x_0}E_\\tau(y_0-\\hat{y_0})^2\\\\\n& = E_{y_0|x_0}E_\\tau((y_0 - f(x_0)) + (f(x_0) - \\hat{y_0}))^2\\\\\n& = E_{y_0|x_0}E_\\tau(y_0 - f(x_0))^2 + 2E_{y_0|x_0}E_\\tau(y_0 - f(x_0))(f(x_0) - \\hat{y_0}) + E_{y_0|x_0}E_\\tau(f(x_0) - \\hat{y_0})^2\\\\\n& = U_1 + U_2 + U_3\n\\end{align}\n\\]\nThere are three components \\(U_1\\), \\(U_2\\), \\(U_3\\) and we’re going to expand them as well.\n\\(U_1 = E_{y_0|x_0}E_\\tau(y_0 - f(x_0))^2 = E_{y_0|x_0}(y_0-f(x_0))^2 = \\sigma^2\\)\nNote: $f(x_0) = E_{y_0|x_0}(y_0) $\n\\(U_2 = 2E_{y_0|x_0}E_\\tau(y_0 - f(x_0))(f(x_0) - \\hat{y_0}) = 0\\)\nNote: \\(E_{y_0|x_0}(y_0-f(x_0)) = 0\\)\n\\(U_3\\):\n\\[\n\\begin{align}\nU_3 & = E_{y_0|x_0}E_\\tau(f(x_0) - \\hat{y_0})^2\\\\\n& = E_{y_0|x_0}E_\\tau((\\hat{y_0} - E_\\tau(\\hat{y_0})) + (E_\\tau(\\hat{y_0}) - f(x_0)))^2\\\\\n& = E_{y_0|x_0}E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0}))^2 + 2E_{y_0|x_0}E_\\tau[(\\hat{y_0} - E_\\tau(\\hat{y_0}))(E_\\tau(\\hat{y_0}) - f(x_0))] + E_{y_0|x_0}E_\\tau(E_\\tau(\\hat{y_0}) - f(x_0))^2\\\\\n& = E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0}))^2 + (E_\\tau(\\hat{y_0}) - f(x_0))^2\\\\\n& = \\text{Var}_\\tau(\\hat{y_0}) + \\text{Bias}_\\tau^2(\\hat{y_0})\n\\end{align}\n\\]\nFinally if we sum all \\(U_i\\) we get: \\[\\text{EPE}(x_0) = U_1+U_2+U_3 = \\sigma^2 + 0 + (\\text{Var}_\\tau(\\hat{y_0}) + \\text{Bias}_\\tau^2(\\hat{y_0}))\\]\n\\(E_\\tau(\\hat{y_0}) = E_\\tau(x_0^T\\beta + \\sum_{i=1}^N {l_i(x_0)\\varepsilon_i})=x_0^T\\beta + E(\\sum_{i=1}^N {l_i(x_0)\\varepsilon_i}) = x_0^T\\beta + 0\\) thus \\(\\text{Bias}_\\tau{\\hat{y_0}} = 0\\)\n(2.27) and we can find variance: \\[\n\\begin{align}\n\\text{Var}_\\tau(\\hat{y_0}) & = E_\\tau(\\hat{y_0} - E_\\tau(\\hat{y_0})) ^ 2\\\\\n& = E_\\tau(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon)\\\\\n& = E_\\tau(x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\\varepsilon^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0)\n\\end{align}\n\\]\nwhere \\(\\varepsilon\\varepsilon^T=\\sigma^2\\mathbf{I}_n\\), so we can simplify further: \\[\\text{Var}_\\tau(\\hat{y_0}) = \\sigma^2x_0^{T}E_\\tau[(\\mathbf{X}^T\\mathbf{X})^{-1})]x_0\\]\n(2.28) if N is large and \\(\\tau\\) were selected at random, and assuming E(X) = 0, then \\(\\mathbf{X}^T\\mathbf{X}\\)-&gt;\\(NCov(\\mathbf{X})\\).\nProof: By definition of covariance \\(\\text{Cov}(X) = E[(X-E(X))(X-E(X))^T] = E(XX^T) = \\frac{\\mathbf{X}^T\\mathbf{X}}{N}\\)\nand we can derive that: \\[\n\\begin{align}\nE_{x_0}\\text{EPE}(x_0) & = E_{x_0}x_0^{T}\\text{Cov}^{-1}(X)x_0\\sigma^2/N+\\sigma^2\\\\\n& = \\text{trace}[\\text{Cov}^{-1}(X)\\text{Cov}(x_0)]\\sigma^2/N+\\sigma^2\\\\\n& = \\sigma^2(p/N)+\\sigma^2\n\\end{align}\n\\]\nFIGURE 2.9\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsimulations = 10000\nsample_size = 500\n\ndef least_square_error(x_0, y_0, train_x, train_y):\n    X = np.c_[np.ones((len(train_x), 1)), train_x]\n    beta = np.linalg.lstsq(X, train_y, rcond = None)[0]\n    return (np.dot(np.array([1, *x_0]), beta) - y_0) ** 2\n\n# 1-nearest neighbor error\ndef nn_error(x_0, y_0, train_x, train_y):\n    X = (train_x * train_x).sum(axis=1)\n    return (y_0 - train_y[X.argmin()]) ** 2\n\ncubic_epe_ratio = []\nlinear_epe_ratio = []\n\nfor p in range(1, 11):\n    least_square_epe = [0, 0]\n    nn_epe = [0, 0]\n    for _ in range(simulations):\n        error = np.random.standard_normal(sample_size)\n        train_x = np.random.uniform(-1, 1, size=(sample_size, p))\n        train_y = [train_x[:, 0] + error, \n                   0.5 * (train_x[:, 0] + 1) ** 3 + error]\n        x_0 = np.zeros(p)\n        y_0 = [np.random.standard_normal(),\n               0.5 + np.random.standard_normal()]\n        for i in range(2):\n            least_square_epe[i] += least_square_error(x_0, y_0[i], \n                                                      train_x, train_y[i])\n            nn_epe[i] += nn_error(x_0, y_0[i], \n                                  train_x, train_y[i])\n    for i in range(2):\n        least_square_epe[i] /= simulations\n        nn_epe[i] /= simulations\n    linear_epe_ratio.append(nn_epe[0] / least_square_epe[0])\n    cubic_epe_ratio.append(nn_epe[1] / least_square_epe[1])\n\n# plot\nfig = plt.figure(1, figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\n\naxes.set_title(\"Expected Prediction Error of 1NN vs. OLS\")\n\naxes.plot(np.arange(1, 11), linear_epe_ratio, '-o',\n          color = 'orange', label = \"Linear\")\n\naxes.plot(np.arange(1, 11), cubic_epe_ratio, '-o',\n          color = 'blue', label = \"Cubic\")\n\naxes.legend()\naxes.set_xlabel(\"Dimension\")\naxes.set_ylabel(\"EPE Ration\")\nplt.show()"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.6-statistical-models-supervised-learning-and-function-approximation.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.6-statistical-models-supervised-learning-and-function-approximation.html",
    "title": "2.6 Statistical Models, Supervised Learning and Function Approximation",
    "section": "",
    "text": "Our goal is to find a useful approximation \\(\\hat{f(x)}\\) to the function \\(f(x)\\) that underlies the predictive relationship between the inputs and outputs.\n\nWe saw that squared error loss lead us to the regression function \\(f(x)=E(Y|X = x)\\) for a qualitive response.\nThe nearest-neighbor methods estimates directly the conditional expections, but may result in large errors for the high dimension input spaces. (The curse of dimensionality)\n\nWe anticipate using other classes of models for f(x) to overcome the dimensionality problems.\n\n2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y)\n(2.29) Suppose that our data arose from a statistical model: \\[Y = f(X) + \\varepsilon\\]\nwhere the random error \\(\\varepsilon\\) has \\(E(\\varepsilon) = 0\\) and independent of X.\nThe additive error model is a useful approximation to the truth. Generally there will be unmeasured variables that also contribute to the output, including measurement error. The additive model assumes that can capture all departures via the error \\(\\varepsilon\\).\nThe assumption in (2.29) that the errors are independent and identically distributed is not strictly necessary. For example, simple modifications can be made to avoid the independence assumption, e.g \\(Var(Y| X = x) = \\sigma(x)\\), and now both the mean and variance depend on X.\n\n\n2.6.2 Supervised Learning\nSuppose that errors are additive and that the model \\(Y = f(X) + \\varepsilon\\). Supervised learning attempts to learn \\(f\\) through a teacher and learns by examples (i.e by a training set of observations (\\(\\mathcal{T} = (x_i, y_i), i = 1...N\\))\n\n\n2.6.3 Function Approximation\nThe goal is to obtain a useful approximation to f(x) for all x in some region of \\(\\mathbb{R}^p\\), given the representations in \\(\\mathcal{T}\\).\nMany of the approximations have associated a set of parameters \\(\\theta\\) that can be modified to suit the data at hand, e.g the linear model \\(f(x)=x^T\\beta\\) has \\(\\theta=\\beta\\). Another class of useful approximators can be expressed as linear basis expansions:\n\\[f_\\theta(x) = \\sum_{k=1}^{K}h_k(x)\\theta_k\\]\nwhere the \\(h_k\\) are a suitable set of functions or transformations of the input vector x. We also encounter nonlinear expansions, such as the sigmoid transformation:\n\\[h_k(x) = \\frac{1}{1+exp(-x^T\\beta_k)}\\]\nWe can use least squares to estimate the parameters \\(\\theta\\) in \\(f_\\theta\\), by minimizing the residual sum-of-squares:\n\\[RSS(\\theta)=\\sum_{i=1}^N(y_i - f_\\theta(x_i)) ^ 2\\]\nWhile least squares is very convenient, it is not only criterion used and in some cases would not make sense. A more general principle for estimation is maximum likelihood estimation. Suppose we have a random sample \\(y_i\\), i = 1…N from a density \\(Pr_\\theta(y)\\) indexed by some parameters \\(\\theta.\\) The log-probability of the observed sample is: \\[L(\\theta)=\\sum_{i=1}^N logPr_\\theta(y_i)\\]\nLeast squares for the additive error model \\(Y = f_\\theta(X)+\\varepsilon\\), with \\(\\varepsilon \\sim N(0, \\sigma^2)\\) is equivalent to maximum likelihood using the conditional likelihood\n\\[Pr(Y|X, \\theta)=N(f_\\theta(X), \\sigma^2)\\]\nThe log-likelihood of the data is: \\[\n\\begin{align}\nL(\\theta) &= \\sum_{i=1}^N log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-f_\\theta(x_i))^2}{2\\sigma^2}}\\right)\\\\\n&= \\sum_{i=1}^N log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\sum_{i=1}^N(\\frac{(y_i-f_\\theta(x_i))^2}{2\\sigma^2})\\\\\n&= \\sum_{i=1}^N log \\left((2\\pi\\sigma^2)^{-\\frac{1}{2}}\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-f_\\theta(x_i))^2\\\\\n&= \\sum_{i=1}^N \\left(-\\frac{1}{2}log(2\\pi)-log(\\sigma)\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-f_\\theta(x_i))^2\\\\\n&= \\frac{N}{2}log(2\\pi)-Nlog(\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-f_\\theta(x_i))^2\n\\end{align}\n\\]\nand the only term involving \\(\\theta\\) is the last.\nA more interesting example is the multinomial likelihood for the regression function Pr(G|X) for a qualitative output G. Suppose we have a model \\(Pr(G = \\mathcal{G}_k| X = x) = p_{k, \\theta}(x), k = 1...K\\) indexed by \\(\\theta\\). Then the log-likelihood (a.k.a the cross-entropy) is :\n\\[L(\\theta)=\\sum_{i=1}^N log (p_{g_i, \\theta}(x_i))\\]"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.3-least-squares-and-nearest-neighbors.html",
    "title": "2.3 Least Squares and Nearest Neighbors",
    "section": "",
    "text": "2.3.3 From Least Squares to Nearest Neighbors\n\nGenerates 10 means \\(m_k\\) from a bivariate Gaussian distrubition for each color:\n\n\\(N((1, 0)^T, \\textbf{I})\\) for BLUE\n\\(N((0, 1)^T, \\textbf{I})\\) for ORANGE\n\nFor each color generates 100 observations as following:\n\nFor each observation it picks \\(m_k\\) at random with probability 1/10.\nThen generates a \\(N(m_k,\\textbf{I}/5)\\)\n\n\n\n%matplotlib inline\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsample_size = 100\n\ndef generate_data(size, mean):\n    identity = np.identity(2)\n    m = np.random.multivariate_normal(mean, identity, 10)\n    return np.array([\n        np.random.multivariate_normal(random.choice(m), identity / 5)\n        for _ in range(size)\n    ])\n\ndef plot_data(orange_data, blue_data): \n    axes.plot(orange_data[:, 0], orange_data[:, 1], 'o', color='orange')\n    axes.plot(blue_data[:, 0], blue_data[:, 1], 'o', color='blue')\n    \nblue_data = generate_data(sample_size, [1, 0])\norange_data = generate_data(sample_size, [0, 1])\n\ndata_x = np.r_[blue_data, orange_data]\ndata_y = np.r_[np.zeros(sample_size), np.ones(sample_size)]\n\n# plotting\nfig = plt.figure(figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\nplot_data(orange_data, blue_data)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.3.1 Linear Models and Least Squares\n\\[\\hat{Y} = \\hat{\\beta_0} + \\sum_{j=1}^{p} X_j\\hat{\\beta_j}\\]\nwhere \\(\\hat{\\beta_0}\\) is the intercept, also know as the bias. It is convenient to include the constant variable 1 in X and \\(\\hat{\\beta_0}\\) in the vector of coefficients \\(\\hat{\\beta}\\), and then write as:\n\\[\\hat{Y} = X^T\\hat{\\beta} \\]\n\nResidual sum of squares\nHow to fit the linear model to a set of training data? Pick the coefficients \\(\\beta\\) to minimize the residual sum of squares:\n\\[RSS(\\beta) = \\sum_{i=1}^{N} (y_i - x_i^T\\beta) ^ 2 = (\\textbf{y} - \\textbf{X}\\beta)^T (\\textbf{y} - \\textbf{X}\\beta)\\]\nwhere \\(\\textbf{X}\\) is an \\(N \\times p\\) matrix with each row an input vector, and \\(\\textbf{y}\\) is an N-vector of the outputs in the training set. Differentiating w.r.t. β we get the normal equations:\n\\[\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\beta) = 0\\]\nIf \\(\\mathbf{X}^T\\mathbf{X}\\) is nonsingular, then the unique solution is given by:\n\\[\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\nclass LinearRegression:\n    def fit(self, X, y):\n        X = np.c_[np.ones((X.shape[0], 1)), X]\n        self.beta = np.linalg.inv(X.T @ X) @ X.T @ y\n\n        return self\n    \n    def predict(self, x):\n        return np.dot(self.beta, np.r_[1, x])\n\nmodel = LinearRegression().fit(data_x, data_y)\nprint(\"beta = \", model.beta)\n\nbeta =  [ 0.52677771 -0.15145005  0.15818643]\n\n\n\n\nExample of the linear model in a classification context\nThe fitted values \\(\\hat{Y}\\) are converted to a fitted class variable \\(\\hat{G}\\) according to the rule:\n\\[\n\\begin{equation}\n\\hat{G} = \\begin{cases}\n\\text{ORANGE} & \\text{ if } \\hat{Y} \\gt 0.5 \\\\\n\\text{BLUE    } & \\text{ if } \\hat{Y} \\leq 0.5\n\\end{cases}\n\\end{equation}\n\\]\n\nfrom itertools import filterfalse, product\n\ndef plot_grid(orange_grid, blue_grid):\n    axes.plot(orange_grid[:, 0], orange_grid[:, 1], '.', zorder = 0.001,\n              color='orange', alpha = 0.3, scalex = False, scaley = False)\n\n    axes.plot(blue_grid[:, 0], blue_grid[:, 1], '.', zorder = 0.001,\n          color='blue', alpha = 0.3, scalex = False, scaley = False)\n\nplot_xlim = axes.get_xlim()\nplot_ylim = axes.get_ylim()\n\ngrid = np.array([*product(np.linspace(*plot_xlim, 50), np.linspace(*plot_ylim, 50))])\n\nis_orange = lambda x: model.predict(x) &gt; 0.5\n\norange_grid = np.array([*filter(is_orange, grid)])\nblue_grid = np.array([*filterfalse(is_orange, grid)])\n\naxes.clear()\naxes.set_title(\"Linear Regression of 0/1 Response\")\nplot_data(orange_data, blue_data)\nplot_grid(orange_grid, blue_grid)\n\nfind_y = lambda x: (0.5 - model.beta[0] - x * model.beta[1]) / model.beta[2]\naxes.plot(plot_xlim, [*map(find_y, plot_xlim)], color = 'black', \n          scalex = False, scaley = False)\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Nearest-Neighbor Methods\n\\[\\hat{Y}(x) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\\]\nwhere \\(N_k(x)\\) is the neighborhood of \\(x\\) defined by the \\(k\\) closest points \\(x_i\\) in the training sample.\n\nclass KNeighborsRegressor:\n    def __init__(self, k):\n        self._k = k\n\n    def fit(self, X, y):\n        self._X = X\n        self._y = y\n        return self\n    \n    def predict(self, x):\n        X, y, k = self._X, self._y, self._k\n        distances = ((X - x) ** 2).sum(axis=1)\n      \n        return np.mean(y[distances.argpartition(k)[:k]])\n\n\ndef plot_k_nearest_neighbors(k):\n    model = KNeighborsRegressor(k).fit(data_x, data_y)\n    is_orange = lambda x: model.predict(x) &gt; 0.5\n    orange_grid = np.array([*filter(is_orange, grid)])\n    blue_grid = np.array([*filterfalse(is_orange, grid)])\n\n    axes.clear()\n    axes.set_title(str(k) + \"-Nearest Neighbor Classifier\")\n\n    plot_data(orange_data, blue_data)\n    plot_grid(orange_grid, blue_grid)\n\nplot_k_nearest_neighbors(1)\nfig\n\n\n\n\n\n\n\n\nIt appears that k-nearest-neighbor have a single parameter (k), however the effective number of parameters is N/k and is generally bigger than the p parameters in least-squares fits. Note: if the neighborhoods were nonoverlapping, there would be N/k neighborhoods and we would fit one parameter (a mean) in each neighborhood.\n\nplot_k_nearest_neighbors(15)\n\nfig"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/exercise-solutions.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/exercise-solutions.html",
    "title": "Ex. 2.8",
    "section": "",
    "text": "Compare the classification performance of linear regression and k– nearest neighbor classification on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from the book website www-stat.stanford.edu/ElemStatLearn.\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef filtered_data(path):\n    data_all = np.loadtxt(path)\n    mask = np.in1d(data_all[:, 0], (2, 3))\n    data_x = data_all[mask, 1: ]\n    data_y = data_all[mask, 0]\n    return data_x, data_y\n\ntrain_x, train_y = filtered_data('../data/zipcode/zip.train')\ntest_x, test_y = filtered_data('../data/zipcode/zip.test')\nk_list = [1, 3, 5, 7, 15]\n\ndef knn_error(k, x, y, data_x, data_y):\n    distances = ((data_x - x)**2).sum(axis=1)\n    return (np.mean(data_y[distances.argpartition(k)[:k]]) - y) ** 2\n\ndef std(squared_errors):\n    \"\"\"standard deviation of the given squared errors.\"\"\"\n    return np.sqrt(np.mean(squared_errors))\n\ndef knn_stds():\n    train_stds = []\n    test_stds = []\n    for k in k_list:\n        train_errors = [knn_error(k, x, y, train_x, train_y)\n                        for (x, y) in zip(train_x, train_y)]\n        train_stds.append(std(train_errors))\n        test_errors = [knn_error(k, x, y, train_x, train_y)\n                        for (x, y) in zip(test_x, test_y)]\n        test_stds.append(std(test_errors))\n    return train_stds, test_stds\n\ndef least_square_stds():\n    X = np.c_[np.ones((len(train_x), 1)), train_x]\n    beta = np.linalg.lstsq(X, train_y, rcond = None)[0]\n    error = lambda x, y: (np.dot(np.array([1, *x]), beta) - y) ** 2\n\n    train_stds = []\n    test_stds = []\n\n    for k in k_list:\n        train_errors = [error(x, y) for (x, y) in zip(train_x, train_y)]\n        train_stds.append(std(train_errors))\n        test_errors = [error(x, y) for (x, y) in zip(test_x, test_y)]\n        test_stds.append(std(test_errors))\n\n    return train_stds, test_stds\n\nfig = plt.figure(figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\n\n# kNN plotting\ntrain_stds, test_stds = knn_stds()\naxes.plot(k_list, train_stds, '-', \n          color = 'C0', label = 'knn-train')\naxes.plot(k_list, test_stds, '-', \n          color = 'C1', label = 'knn-test')\n\n# least square plotting\ntrain_stds, test_stds = least_square_stds()\naxes.plot(k_list, train_stds, '-', \n          color = 'C2', label = 'least-square-train')\naxes.plot(k_list, test_stds, '-', \n          color = 'C3', label = 'least-square-test')\n\naxes.legend()\naxes.set_xlabel(\"k\")\naxes.set_ylabel(\"Error\")\nplt.show()"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.5-methods-using-derived-input-directions.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.5-methods-using-derived-input-directions.html",
    "title": "3.5 Methods Using Derived Input Directions",
    "section": "",
    "text": "In many situations, we have a large number of inputs, ofter very correlated. This methods in this section produce a small number of linear combinations Z of the original inputs X.\n\n3.5.1 Principal Components Regression\nPrincinal component regression (PCR) forms the derived input columns \\(z_m=\\mathbf{X}v_m\\) and then regresses \\(\\mathbf{y} \\text{ on } \\mathbf{z}_1,...,\\mathbf{z}_M\\) for some M &lt;= p. Since the Z are orthogonal (3.61): \\[\n\\hat{\\mathbf{y}}_{(M)}^{pcr} = \\overline{y}\\mathbf{1} + \\sum_{m=1}^M \\hat{\\theta}_m\\mathbf{z}_m\n\\]\nwhere \\(\\hat{\\theta}_m = \\langle \\mathbf{z}_m, \\mathbf{y} \\rangle / \\langle \\mathbf{z}_m, \\mathbf{z}_m\\rangle\\). Since the \\(\\mathbf{z}_m\\) are linear combinations of the original \\(\\mathbf{x}_j\\), we can express in terms of coefficients of the \\(\\mathbf{x}_j\\) (3.62):\n\\[\n\\hat{\\beta}^{pcr}(M) = \\sum_{m=1}^M \\hat{\\theta}_m v_m\n\\]\nTODO: proof\n\nwe first standardize the inputs.\nNote that if M = p, we would get least squares estimates, since Z=UD span the column space of X.\nPCR is very similar to ridge regression.\n\n\n\n3.5.2 Partial Least Squares\n\nPartial Least Squares (PLS) begins by computing \\(\\hat{\\varphi}_{1j} = \\langle \\mathbf{x}_j, \\mathbf{y} \\rangle\\) for each j.\nThen we construct the derived input \\(\\mathbf{z}_1=\\sum_j \\hat{\\varphi}_{1j} \\mathbf{x}_j\\), which the 1st partial least squares direction.\nThe outcome \\(\\mathbf{y}\\) is regressed on \\(\\mathbf{z}_1\\) giving coefficient \\(\\hat{\\theta}_1\\)\nAnd then we orthogonalize \\(\\mathbf{x}_1,..., \\mathbf{x}_p\\) w.r.t \\(\\mathbf{z}_1\\).\nWe continue this process until M &lt;= p directions have been obtained.\n\nNote: PLS produces orthogonal inputs(or directions) \\(\\mathbf{z}_1,...,\\mathbf{z}_M\\) and if M = p, we would get least squares estimates.\n\n# Algorithm 3.3 Partial Least Squares.\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndf = pd.read_csv('../data/prostate/prostate.data', delimiter='\\t', index_col=0)\nmask_train = df.pop('train')\ndf_y = df.pop('lpsa')\n\ntrain_x = df[mask_train == 'T']\ntrain_y = df_y[mask_train == 'T']\n\ntrain_x_centered = train_x - train_x.mean(axis = 0)\ntrain_x_centered /= np.linalg.norm(train_x_centered, axis=0)\ntrain_y_centered = train_y - train_y.mean()\n\n\ndef PLS(X, y):\n    X = X.copy()\n    y = y.copy()\n\n    n, p = X.shape\n    y_fit = y.mean() * np.ones(n)\n    Z = np.zeros_like(X)\n    theta = np.zeros(p)\n    \n    for m in range(p):\n        for j in range(p):\n            phi = np.dot(X[:, j], y)\n            Z[:, m] += phi * X[:, j]\n\n        theta[m] = np.dot(Z[:, m], y) / np.dot(Z[:, m], Z[:, m])\n        y_fit += theta[m] * Z[:, m]\n        \n        for j in range(p):\n            X[:, j] -= np.dot(Z[:, m], X[:, j]) / np.dot(Z[:, m], Z[:, m]) * Z[:, m]\n    return y_fit\n\n\ny_fit = PLS(train_x_centered.values, train_y_centered.values)\ntrain_error = np.mean((y_fit - train_y_centered) ** 2)\nprint('Train error: ', train_error)\n\nTrain error:  0.43919976805833433\n\n\nIt can be shown that PLS seeks directions that have high variance and have high correlation with the response. The mth PLS direction \\(\\hat{\\varphi}_m\\) solves:\n\\[\n\\begin{equation}\nmax_\\alpha Corr^2(\\mathbf{y}, \\mathbf{X}\\alpha)Var(\\mathbf{X}\\alpha)\\\\\n\\text{ subject to } \\| \\alpha \\| = 1, \\alpha^T\\mathbf{S}v_l = 0, l = 1, ..., m - 1\n\\end{equation}\n\\]"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.4.2-the-lasso.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.4.2-the-lasso.html",
    "title": "3.4.2 The Lasso",
    "section": "",
    "text": "The lasso is a shrinkage method, which is estimated by (3.51): \\[\n\\begin{equation}\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n\\text{ subject to } \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{equation}\n\\]\nwe can re-parametrize the constant \\(\\beta_0\\) by standartizing the predictors; the solution for \\(\\beta_0\\) is \\(\\overline{y}\\).\nWe can also write the lasso in the equivalent Lagragngian form (3.52):\n\\[\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} \\left\\{\n\\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n+\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\\]\nThe lasso solution is nonlinear in the \\(y_i\\) and it is a quadratic programming problem."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.2.2-the-gauss-markov-theorem.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.2.2-the-gauss-markov-theorem.html",
    "title": "3.2.2 The Gauss–Markov Theorem",
    "section": "",
    "text": "The Gauss-Markov theorem states that the least squares estimates of the \\(\\beta\\) have the smallest variance among all linear unbiased estimates. We focus on estimation of any linear combination of the parameters \\(\\theta=\\alpha^T\\hat{\\beta}\\), i.e predictions \\(f(x_0)={x_0}^T\\beta\\) are of this form. The least squares estimate is (3.17):\n\\[\n\\hat{\\theta}=\\alpha^T\\hat{\\beta} = \\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\nand \\(\\alpha^T\\hat{\\beta}\\) is unbiased, since (3.18):\n\\[\n\\begin{align}\nE(\\alpha^T\\hat{\\beta}) &= E( \\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y})\\\\\n&=  \\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\beta\\\\\n&= \\alpha^T\\beta\n\\end{align}\n\\]\nThe Gauss-Markov states that if we have any other linear estimator \\(\\tilde{\\theta}=\\mathbf{c}^T\\mathbf{y}\\), that is, \\(E(\\mathbf{c}^T\\mathbf{y})=\\alpha^T\\beta\\), then (3.19):\n\\[\nVar(\\alpha^T\\hat{\\beta})\\le Var(\\mathbf{c}^T\\mathbf{y})\n\\]\nProof:\nLet’s assume that \\(\\mathbf{c}^T\\mathbf{y}=(\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{d}^T)\\mathbf{y}\\), then:\n\\[\n\\begin{align}\nE(\\mathbf{c}^T\\mathbf{y})&= E((\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{d}^T)\\mathbf{y})\\\\\n&= E((\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{d}^T)(\\mathbf{X}\\beta + \\varepsilon))\\\\\n&= \\alpha^T\\beta + d^T\\mathbf{X}\\beta\n\\end{align}\n\\]\nSince \\(E(\\mathbf{c}^T\\mathbf{y})\\) is unbiased \\(d^T\\mathbf{X}=0\\).\n\\[\n\\begin{align}\nVar(\\mathbf{c}^T\\mathbf{y}) &= \\mathbf{c}^{T}Var(\\mathbf{y})\\mathbf{c} = \\sigma^2\\mathbf{c}^T\\mathbf{c}\\\\\n&= \\sigma^2(\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{d}^T)(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\alpha + \\mathbf{d})\\\\\n&= \\sigma^2(\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\alpha+\\mathbf{d}^T\\mathbf{d})\\\\\n&= Var(\\alpha^T\\hat{\\beta})+\\sigma^2\\mathbf{d}^T\\mathbf{d}\n\\end{align}\n\\]\nSince \\(\\mathbf{d}^T\\mathbf{d}\\ge 0\\), the proof is completed.\nConsider the MSE of an estimator \\(\\tilde{\\theta}\\) is estimating \\(\\theta\\): \\[\n\\begin{align}\nMSE(\\tilde{\\theta}) &= E(\\tilde{\\theta} - \\theta)^2\\\\\n&= Var(\\tilde{\\theta})+[E(\\tilde{\\theta}) - \\theta]^2\n\\end{align}\n\\]\nThe first term is the variance, while the second term is the squared bias. The theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may exist a biased estimator with smaller MSE.\nMean squared error is related to prediction accuracy. Consider the prediction of the \\(x_0\\), (3.21):\n\\[\nY_0 = f(x_0) + \\varepsilon_0\n\\]\nThen the EPE of an estimate \\(\\tilde{f}(x_0)={x_0}^T\\tilde{\\beta}\\) is (3.22): \\[\n\\begin{align}\nE(Y_0 - \\tilde{f}(x_0)) &= \\sigma^2 + E({x_0}^T\\tilde{\\beta}-f(x_0))^2\\\\\n&= \\sigma^2 + MSE(\\tilde{f}(x_0))\n\\end{align}\n\\]\nTherefore, EPE and MSE differ only by the constant \\(\\sigma^2\\)."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.2-linear-regression-models-and-least-squares.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.2-linear-regression-models-and-least-squares.html",
    "title": "3.2 Linear Regression Models and Least Squares",
    "section": "",
    "text": "We have an input vector \\(X^T=(X_1,...,X_p)\\) and want to predict a real-valued output \\(Y\\).The linear regression model has the form:\n\\[f(X) = B_0 + \\sum_{j=1}^p {X_j\\beta_j}\\]\nTypically we have a set of training data \\((x_1, y_1)...(x_N, y_n)\\) from which to estimate the parameters \\(\\beta\\). The most popular estimation method is least squares, in which we pick \\(\\beta\\) to minimize the residual sum of squares, (3.2): \\[\n\\begin{align}\nRSS(\\beta)&=\\sum_{i=1}^N(y_i-f(x_i))\\\\\n&=\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j})^2\n\\end{align}\n\\]\nHow do we minimize (3.2)? We can write the (3.2) using matrix, (3.3): \\[RSS(\\beta)=(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta)\\]\nDifferentiating with respect to \\(\\beta\\) we obtain: \\[\n\\begin{align}\n\\frac{\\partial{RSS}}{\\partial\\beta} = -2\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)\n\\end{align}\n\\]\nAssuming that X has full column rank, and hence the second derivative is positive definite: \\[\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)=0\\]\nand the unique solution is: \\[\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nThe predicted value at an input vector \\(x_0\\) are given by \\(\\hat{f}(x_0)=(1:x_0)^T\\hat{\\beta}\\):\n\\[\\hat{y}=\\mathbf{X}\\hat{\\beta}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nThe matrix \\(\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\) is sometimes called the “hat” matrix.\nGeometrical representation of the least squares: We denote the column vectors of X by \\(x_0, x_1, ..., x_p\\). These vectors span a subspace of \\(\\mathcal{R}^N\\), also referred as the column space of X. We minimize \\(RSS(\\beta)=||\\mathbf{y}-\\mathbf{X}\\beta||^2\\) by choosing \\(\\hat{\\beta}\\) so that the residual vector \\(\\mathbf{y} - \\hat{\\mathbf{y}}\\) is orthogonal to this subspace and the orthogonality is expressed by \\(\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta)=0\\). The hat matrix H is the projection matrix.\nSampling properties of \\(\\hat{\\beta}\\): In order to pin down the sampling properties of \\(\\hat{\\beta}\\), we assume that the observations \\(y_i\\) are uncorrelated and have constant variance \\(\\sigma^2\\), and that the \\(x_i\\) are fixed. The variance-covariance matrix is given by (3.8):\n\\[\n\\begin{align}\nVar(\\hat{\\beta}) &= E\\left[(\\hat{\\beta}-E(\\hat{\\beta}))(\\hat{\\beta}-E(\\hat{\\beta})^T)\\right]\\\\\n&= E\\left[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{\\varepsilon}\\mathbf{\\varepsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\right]\\\\\n&= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align}\n\\]\nOne estimates the variance \\(\\sigma^2\\) by: \\[\n\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum_{i=1}^N(y_i-\\hat{y_i})^2\n\\]\nThe N-p-1 rather than N in the denominator makes \\(\\hat{\\sigma}^2\\) an unbiased estimate of \\(\\sigma^2\\): \\(E(\\hat{\\sigma}^2)=\\sigma^2\\).\nProof: \\[\n\\begin{align}\n\\hat{\\varepsilon} &= \\mathbf{y} - \\mathbf{\\hat{y}}\\\\\n&= \\mathbf{X}\\beta + \\varepsilon - \\mathbf{X}\\hat{\\beta}\\\\\n&= \\mathbf{X}\\beta + \\varepsilon - \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta + \\varepsilon)\\\\\n&= \\varepsilon - \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\varepsilon\\\\\n&= (\\mathbf{I}_n - \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T)\\varepsilon\\\\\n&= (\\mathbf{I}_n - \\mathbf{H})\\varepsilon\n\\end{align}\n\\]\nand we would like to find \\(Var(\\hat{\\varepsilon})=E(\\hat{\\varepsilon}^T\\hat{\\varepsilon})\\):\n\\[\n\\begin{align}\nE[\\hat{\\varepsilon}^T\\hat{\\varepsilon}]\n&= E\\left[\\varepsilon^T(\\mathbf{I}_n - \\mathbf{H})^T(\\mathbf{I}_n - \\mathbf{H})\\varepsilon\\right]\\\\\n&= E\\left[tr(\\varepsilon^T(\\mathbf{I}_n - \\mathbf{H})^T(\\mathbf{I}_n - \\mathbf{H})\\varepsilon)\\right]\\\\\n&= E\\left[tr(\\varepsilon\\varepsilon^T(\\mathbf{I}_n - \\mathbf{H})^T(\\mathbf{I}_n - \\mathbf{H}))\\right]\\\\\n&= \\sigma^2E\\left[tr((\\mathbf{I}_n - \\mathbf{H})^T(\\mathbf{I}_n - \\mathbf{H}))\\right]\\\\\n&= \\sigma^2E\\left[tr(\\mathbf{I}_n - \\mathbf{H})\\right]\\\\\n&= \\sigma^2E\\left[tr(\\mathbf{I}_n) - tr(\\mathbf{I}_{p+1})\\right]\\\\\n&= \\sigma^2(n-p-1)\n\\end{align}\n\\]\nNote that, both \\(\\mathbf{H}\\) and \\(\\mathbf{I_n}-\\mathbf{H}\\) are:\n\nSymmetry matrix, i.e \\(\\mathbf{H}^T=\\mathbf{H}\\)\nIdempotent matrix, i.e \\(\\mathbf{H}^2=\\mathbf{H}\\)\n\nInferences about the parameters and the model: We now assume that deviations of Y around its expectations and Gaussian. Hence (3.9):\n\\[\n\\begin{align}\nY &=E(Y|X_1,...,X_p)+\\varepsilon\\\\\n&= \\beta_0 + \\sum_{j=1}^P{X_j\\beta_j} + \\varepsilon\n\\end{align}\n\\]\nwhere $N(0, ^2) $\nUnder (3.9), it is easy to show that (3.10): \\[\n\\hat{\\beta} \\sim N(\\beta, (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\n\\]\nAlso (3.11): \\[(N-p-1)\\hat{\\sigma}^2 \\sim \\sigma^2\\chi_{N-p-1}^2\\]\na chi-squared distribution with N-p-1 degrees of freedom and \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}\\) are statistically independent.\nHypothesis test: To test \\(H_0: \\beta_j = 0\\) we form the standardized coefficient or Z-score: \\[\nz_j=\\frac{\\hat{B}_j}{\\hat{\\sigma}\\sqrt{v_j}}\n\\]\nwhere \\(v_j\\) is the jth diagonal element of \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\). Under the null hypothesis \\(z_j\\) is distributed as \\(t_{N-p-1}\\), and hence a large value of \\(z_j\\) will lead to rejection. If \\(\\hat{\\sigma}\\) is replaced by \\(\\sigma\\) then \\(z_j\\) is a standard normal distribution. The difference between tail quantiles of a t-distribution and a standard normal become negligible as the sample size increases, see the Figure (3.3) below:\n\n# Figure 3.3\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm, t\n   \nfig = plt.figure(figsize = (12, 8))\naxes = fig.add_subplot(1, 1, 1)\n\nz = np.linspace(1.9, 3, 500)\n\nnormal_probabilities = 1 - norm.cdf(z) + norm.cdf(-z)\nt_30_probabilities = 1 - t.cdf(z, 30) + t.cdf(-z, 30) \nt_100_probabilities = 1 - t.cdf(z, 100) + t.cdf(-z, 100) \n\naxes.plot(z, normal_probabilities, color='C0', label = 'normal')\naxes.plot(z, t_30_probabilities, color='C1', label = '$t_{30}$')\naxes.plot(z, t_100_probabilities, color='C2', label = '$t_{100}$')\n\nxlim = axes.get_xlim()\n\nfor y in [0.01, 0.05]:\n    axes.plot(xlim, [y, y], '--', color = 'gray', \n              scalex = False, scaley = False)\n\n    for index, probs in enumerate([normal_probabilities, t_30_probabilities,\n                                   t_100_probabilities]):\n        x = z[np.abs(probs - y).argmin()]\n        axes.plot([x, x], [0, y], '--', color = f\"C{index}\",\n                  scalex = False, scaley = False)\n    \naxes.legend()\naxes.set_xlabel('Z')\naxes.set_ylabel('Tail Probabilities')\nplt.show()\n\n\n\n\n\n\n\n\nTest for the significance of groups of coefficients simultaneously: We use the F-statistics (3.13):\n\\[F=\\frac{(RSS_0-RSS_1) / (p_1 - p_0)}{RSS_1/(N-p_1-1)}\\]\nWhere \\(RSS_1\\) is for the bigger model with \\(p_1+1\\) parameters and \\(RSS_0\\) for the nested smaller model with \\(p_0+1\\) parameters. Under the null hypothesis that the smaller model is correct, the F statistic will have a \\(F_{p_1-p_0,N-p_1-1}\\) distribution. The \\(z_j\\) in (3.13) is equivalent to the F statistic for dropping the single coefficient \\(\\beta_j\\) from the model.\nSimilarly, we can isolate \\(\\beta_j\\) in (3.10) to obtain \\(1-2\\alpha\\) confidence interval (3.14) \\[(\\hat{\\beta_j} - z^{(1-\\alpha)}v_j^{\\frac{1}{2}}\\hat{\\sigma}, \\hat{\\beta_j} + z^{(1-\\alpha)}v_j^{\\frac{1}{2}}\\hat{\\sigma})\\]\nIn a similar fashion we can obtain an approximate confidence set for the entire parameter vector \\(\\beta\\) (3.15): \\[ C_{\\beta} = \\{{\\beta|(\\hat{\\beta}-\\beta)^T\\mathbf{X}^T\\mathbf{X}(\\hat{\\beta}-\\beta)} \\le \\hat{\\sigma}^2{\\chi_{p+1}^2}^{(1-\\alpha)} \\}\\]\nwhere \\({\\chi_{l}^2}^{(1-\\alpha)}\\) is the \\(1-\\alpha\\) percentile of the chi-squared distribution of \\(l\\) degrees of freedom."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.3-subset-selection.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.3-subset-selection.html",
    "title": "3.3 Subset Selection",
    "section": "",
    "text": "There are two reasons why we are not satisfied with the least squares estimates:\n\nPrediction accuracy: The least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero.\nInterpretation: In order to get the “big picture”, we are willing to sacrifice the small details by selecting a smaller subset.\n\n\n3.3.1 Best-Subset Selection\nBest subset regression finds for each k the subset of size k that givest smallest RSS. An efficient algorithm - the leaps and bounds procedure (feasible for p ~ 30, 40). The question of how to choose k involves the tradeoff between bias and variance; typically we choose the smallest model that minimizes an EPE.\nTODO: implement FIGURE 3.5\n\n\n3.3.2 Forward- and Backward-Stepwise Selection\nRather than search through all possible subset (which is infeasible for p &gt; 40), we can seek a good path through them.\nForward-stepwise selection.\nForward-stepwise selection starts with the intercept, and sequentially adds the predictor that most improves the fit. Clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate. List best-subset regression, the subset size k must be determined.\nThe Forward-stepwise selection is a greedy algorithm, however, there are reasons why it might be preferred: - Computational: for large p we cannot compute the best subset sequence. - Statistical: will have lower variance, but perhaps more bias.\nBackward-stepwise selection.\nBackward-stepwise selection starts with the full model, and sequentially deletes the predictors that has the least impact on the fit (i.e the smallest Z-score). Backward selection can be used only when N &gt; p.\nHybrid stepwise selection.\nSome software packages implement hybrid stepwise-selection strategies that consider both forward and backward at each step, and select the best of the two (i.e in the R package the “step” function).\n\n\n3.3.3 Forward-Stagewise Regression\nForward-Stagewise Regression(FS) is even more constrained than forward-stepwise regression. It starts with an intercept equal to \\(\\overline{y}\\), and centered predictors with coefficients intally all 0. At each step it identifies the variable most correlated with the current residual. It then computes the simple linear regression coefficient of the residual on this chosen variable, and then adds it to the current coefficient for that variable. This is continue till none of variables have correlation with the residuals.\n\n\n3.3.4 Prostate Cancer Data Example\nTODO:"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.2.1-example-prostate-cancer.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.2.1-example-prostate-cancer.html",
    "title": "Madiyar's Page",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndf = pd.read_csv('../data/prostate/prostate.data', delimiter='\\t', index_col=0)\nmask_train = df.pop('train')\ndf_y = df.pop('lpsa')\n\ndf.head()\n\n\n\n\n\n\n\n\nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n\n\n\n\n1\n-0.579818\n2.769459\n50\n-1.386294\n0\n-1.386294\n6\n0\n\n\n2\n-0.994252\n3.319626\n58\n-1.386294\n0\n-1.386294\n6\n0\n\n\n3\n-0.510826\n2.691243\n74\n-1.386294\n0\n-1.386294\n7\n20\n\n\n4\n-1.203973\n3.282789\n58\n-1.386294\n0\n-1.386294\n6\n0\n\n\n5\n0.751416\n3.432373\n62\n-1.386294\n0\n-1.386294\n6\n0\n\n\n\n\n\n\n\n\n# Table 3.1. Correlations of predictors in the prostate cancer data\ndf[mask_train == 'T'].corr()\n\n\n\n\n\n\n\n\nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\n\n\n\n\nlcavol\n1.000000\n0.300232\n0.286324\n0.063168\n0.592949\n0.692043\n0.426414\n0.483161\n\n\nlweight\n0.300232\n1.000000\n0.316723\n0.437042\n0.181054\n0.156829\n0.023558\n0.074166\n\n\nage\n0.286324\n0.316723\n1.000000\n0.287346\n0.128902\n0.172951\n0.365915\n0.275806\n\n\nlbph\n0.063168\n0.437042\n0.287346\n1.000000\n-0.139147\n-0.088535\n0.032992\n-0.030404\n\n\nsvi\n0.592949\n0.181054\n0.128902\n-0.139147\n1.000000\n0.671240\n0.306875\n0.481358\n\n\nlcp\n0.692043\n0.156829\n0.172951\n-0.088535\n0.671240\n1.000000\n0.476437\n0.662533\n\n\ngleason\n0.426414\n0.023558\n0.365915\n0.032992\n0.306875\n0.476437\n1.000000\n0.757056\n\n\npgg45\n0.483161\n0.074166\n0.275806\n-0.030404\n0.481358\n0.662533\n0.757056\n1.000000\n\n\n\n\n\n\n\n\n\"\"\" TABLE 3.2. Linear model fit to the prostate cancer data. The Z score is the\ncoefficient divided by its standard error (3.12). Roughly a Z score larger than two\nin absolute value is significantly nonzero at the p = 0.05 level.\n\"\"\"\nclass LinearRegression:\n    def fit(self, X, y):\n        X = np.c_[np.ones((X.shape[0], 1)), X]\n        XX_inv = np.linalg.inv(X.T @ X)\n        self.beta = XX_inv @ X.T @ y\n        \n        var = np.sum((X @ self.beta - y)**2) / (X.shape[0] - X.shape[1])\n        self.stderr = np.sqrt(np.diag(XX_inv * var))\n        self.z_score = self.beta / self.stderr\n\n        return self\n        \n    def predict(self, X):\n        X = np.c_[np.ones((X.shape[0], 1)), X]\n        return X @ self.beta\n    \ndf = df.apply(stats.zscore)\ntrain_x = df[mask_train == 'T']\ntrain_y = df_y[mask_train == 'T']\n\nmodel = LinearRegression().fit(train_x.values, train_y.values)\n\npd.DataFrame(data = {'Coefficient': model.beta, \n                     'Std. Error': model.stderr,\n                     'Z Score' : model.z_score}, \n             index = [\"Intercept\", *df.columns.tolist()])\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ Score\n\n\n\n\nIntercept\n2.464933\n0.089315\n27.598203\n\n\nlcavol\n0.676016\n0.125975\n5.366290\n\n\nlweight\n0.261694\n0.095134\n2.750789\n\n\nage\n-0.140734\n0.100819\n-1.395909\n\n\nlbph\n0.209061\n0.101691\n2.055846\n\n\nsvi\n0.303623\n0.122962\n2.469255\n\n\nlcp\n-0.287002\n0.153731\n-1.866913\n\n\ngleason\n-0.021195\n0.144497\n-0.146681\n\n\npgg45\n0.265576\n0.152820\n1.737840\n\n\n\n\n\n\n\n\nrss1 = sum((model.predict(train_x.values) - train_y) ** 2)\n\ntrain_x_hyp = train_x.drop(columns = ['age', 'lcp', 'gleason', 'pgg45'])\nmodel_hyp = LinearRegression().fit(train_x_hyp.values, train_y.values)\n\nrss0 = sum((model_hyp.predict(train_x_hyp) - train_y) ** 2)\n\ndfn = train_x.shape[1] - train_x_hyp.shape[1]\ndfd = train_x.shape[0] - train_x.shape[1] - 1\nf_stats = ((rss0 - rss1) / dfn) / (rss1 / dfd)\nprob = 1 - stats.f.cdf(f_stats, dfn = dfn, dfd = dfd)\n\nprint ('RSS1 = ', rss1)\nprint ('RSS0 = ', rss0)\nprint ('F =', f_stats)\nprint (f'Pr(F({dfn}, {dfd}) &gt; {f_stats:.2f}) = {prob:.2f}')\n\nRSS1 =  29.426384459908398\nRSS0 =  32.81499474881554\nF = 1.6697548846375183\nPr(F(4, 58) &gt; 1.67) = 0.17\n\n\n\ntest_x = df[mask_train == 'F']\ntest_y = df_y[mask_train == 'F']\n\ntest_error = np.mean((test_y - model.predict(test_x)) ** 2)\nbase_error = np.mean((test_y - np.mean(train_y)) ** 2)\nprint ('Prediction error: ', test_error)\nprint ('Base error rate: ', base_error)\n\nPrediction error:  0.5212740055076002\nBase error rate:  1.056733228060382"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.1-introduction.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.1-introduction.html",
    "title": "4.1 Introduction",
    "section": "",
    "text": "Our predictor G(x) takes values in a discrete set \\(\\mathcal{G}\\).\nWe can divide the input space into regions labeled according to the classification.\nDecision boundaries of regions are linear; this is what we’ll mean by linear methods for classification.\n\nSuppose there are K classes and the fitted linear model: \\(\\hat{f}_k(x)=\\hat{\\beta}_{k0} + \\hat{\\beta}_k^Tx\\); then the decision boundary between class k and l is \\(\\hat{f}_k(x)=\\hat{f}_l(x)\\) that is \\(\\{ x : (\\hat{\\beta}_{k0}-\\hat{\\beta}_{l0}) + (\\hat{\\beta}_{k}-\\hat{\\beta}_{l})^Tx = 0 \\}\\). This regression approach is a member of a class of methods that model discriminant functions \\(\\delta_k(x)\\) for each class, and then classify x to the class with the largest value for its discriminant function. Methods that model the posterior probabilities Pr(G = k | X = x) are also in this class. Clearly, if either the \\(\\delta_k(x)\\) or Pr(G = k| X = x) are linear in x, then the decision boundaries will be linear.\nFor example, a popular model for the posterior probabilities for two classes are (4.1):\n\\[\n\\begin{equation}\nPr(G = 1|X=x)=\\cfrac{exp(\\beta_0 + \\beta^Tx)}{1+exp(\\beta_0 + \\beta^Tx)}\\\\\nPr(G = 2|X=x)=\\cfrac{1}{1+exp(\\beta_0 + \\beta^Tx)}\n\\end{equation}\n\\]\nHere the monotone transformation is the logit (or log-odds) transformation: log[p/(1-p)], in fact (4.2):\n\\[\nlog \\frac{Pr(G = 1|X=x)}{Pr(G = 2|X=x)} = \\beta_0 + \\beta^Tx\n\\]\nThe decision boundary defined by \\(\\{x : \\beta_0 + \\beta^Tx = 0\\}\\)"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.4.2-example-south-african-heart-disease.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.4.2-example-south-african-heart-disease.html",
    "title": "4.4.2 Example: South African Heart Disease",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../data/SAheart/SAheart.data', index_col = 0)\ndf['famhist'] = df['famhist'].map({'Present': 1, 'Absent': 0})\n\n# These columns are not used in the book.\ndf.pop('adiposity')\ndf.pop('typea')\n\ndf_y = df.pop('chd')\ndf.head()\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nfamhist\nobesity\nalcohol\nage\n\n\nrow.names\n\n\n\n\n\n\n\n\n\n\n\n1\n160\n12.00\n5.73\n1\n25.30\n97.20\n52\n\n\n2\n144\n0.01\n4.41\n0\n28.87\n2.06\n63\n\n\n3\n118\n0.08\n3.48\n1\n29.14\n3.81\n46\n\n\n4\n170\n7.50\n6.41\n1\n31.99\n24.26\n58\n\n\n5\n134\n13.60\n3.50\n1\n25.99\n57.34\n49\n\n\n\n\n\n\n\n\npd.plotting.scatter_matrix(df, marker='O',\n                           facecolors = 'none', \n                           edgecolors = df_y.map({0: 'blue', 1: 'red'}),\n                           figsize = (10, 10))\nplt.show()\n\n\n\n\n\n\n\n\n\nclass LogisticRegression:\n    def predict(X):\n        X = np.c_[np.ones((X.shape[0], 1)), X]\n        return self._sigmoid(X @ self.beta)\n    \n    def fit(self, X, y):\n        X = np.c_[np.ones((X.shape[0], 1)), X]\n        self.beta = np.zeros(X.shape[1])\n        \n        while True:\n            p = self._sigmoid(X @ self.beta)\n            W = np.diag(p * (1 - p)) \n            derivative = X.T @ (y - p)\n            hessian = -X.T @ W @ X\n\n            if np.allclose(derivative, 0):\n                break    \n            self.beta -= np.linalg.inv(hessian) @ derivative\n        \n        self.stderr = np.sqrt(np.diag(np.linalg.inv(-hessian)))\n        self.z_score = self.beta / self.stderr\n        return self\n    \n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\nmodel = LogisticRegression().fit(df.values, df_y.values)\n\npd.options.display.float_format = '{0:.3f}'.format\n\npd.DataFrame(data = {'Coefficient': model.beta, \n                     'Std. Error': model.stderr,\n                     'Z Score' : model.z_score}, \n             index = [\"Intercept\", *df.columns.tolist()])\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ Score\n\n\n\n\nIntercept\n-4.130\n0.964\n-4.283\n\n\nsbp\n0.006\n0.006\n1.023\n\n\ntobacco\n0.080\n0.026\n3.034\n\n\nldl\n0.185\n0.057\n3.218\n\n\nfamhist\n0.939\n0.225\n4.177\n\n\nobesity\n-0.035\n0.029\n-1.187\n\n\nalcohol\n0.001\n0.004\n0.136\n\n\nage\n0.043\n0.010\n4.181"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.3.2-computations-for-LDA.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.3.2-computations-for-LDA.html",
    "title": "4.3.2 Computations for LDA",
    "section": "",
    "text": "Suppose we compute the eigen-decomposition for each \\(\\hat{\\Sigma}_k=\\mathbf{U}_k\\mathbf{D}_k\\mathbf{U}_k^T\\), then ingredients for \\(\\delta_k(x)\\) is:\n\n\\((x-\\hat{\\mu}_k)^T\\hat{\\Sigma}_k^{-1}(x - \\hat{\\mu}_k)=\\left[\\mathbf{U}_k^T (x - \\hat{\\mu}_k)\\right]^T\n\\mathbf{D}_k^{-1}\n\\left[\\mathbf{U}_k^T(x-\\hat{\\mu}_k)\\right]\\)\n\\(\\log |\\hat{\\Sigma}_k|=\\Sigma_l \\log d_{kl}\\)\n\nthe LDA classifier can be implemented by the following steps:\n\nSphere the data w.r.t the covariance estimate \\(\\hat{\\Sigma}: X^{*}=\\mathbf{D}^{-1/2}\\mathbf{U}^TX\\)\nClassify to the closest class centroid in the transformed space, modulo the effect of the class prior probabilities \\(\\pi_k\\)."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.4.3-quadratic-approximations-and-inference.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.4.3-quadratic-approximations-and-inference.html",
    "title": "4.4.3 Quadratic Approximations and Inference",
    "section": "",
    "text": "The maximum-likelihood parameter estimates \\(\\hat{\\beta}\\) satisfy a self-consistency relationship: they are the coefficients of a weighted least squares fit, where responses are (4.29): \\[\nz_i = x_i^T\\hat{\\beta} + \\cfrac{(y_i - \\hat{p}_i)}{\\hat{p}_i(1-\\hat{p}_i)}\n\\] and the weights are \\(w_i=\\hat{p}_i(1-\\hat{p}_i)\\). This connection has more to offer:\n\nThe weight RSS is the familiar Pearson chi-square statistic (4.30) \\[\n\\sum_{i=1}^N\\cfrac{(y_i-\\hat{p}_i)^2}{\\hat{p}_i(1-\\hat{p}_i)}\n\\]\na quadratic approximation to the deviance.\nAsymptotic likelihood theory says that if the model is correct, then \\(\\hat{\\beta}\\) converges to the true \\(\\beta\\).\nA central limit theorem then shows that the distribution of \\(\\hat{\\beta}\\) converges to \\(N(\\beta, (\\mathbf{X}^T\\mathbf{XW})^{-1})\\). (Can be derived from weighted least squares fit)."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.4-logistic-regression.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.4-logistic-regression.html",
    "title": "4.4 Logistic Regression",
    "section": "",
    "text": "The model has the form (4.17): \\[\n\\begin{equation}\n\\log \\cfrac{Pr(G = 1 | X = x)}{Pr(G = k | X =x)} = \\beta_{10} + \\beta_1^Tx\\\\\n\\log \\cfrac{Pr(G = 2 | X = x)}{Pr(G = k | X =x)} = \\beta_{20} + \\beta_2^Tx\\\\\n\\vdots\\\\\n\\log \\cfrac{Pr(G = K-1 | X = x)}{Pr(G = k | X =x)} = \\beta_{(K-1)0} + \\beta_{K-1}^Tx\\\\\n\\end{equation}\n\\]\nThe model is specified in terms of K - 1 log-odds or logit transformations. A simple calculation shows that (4.18): \\[\n\\begin{equation}\nPr(G=k|X=x) = \\cfrac{exp(\\beta_{k0} + \\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}exp(\\beta_{l0} + \\beta_l^Tx)},\nk = 1,...,K - 1,\\\\\nPr(G=K|X=x) = \\cfrac{1}{1+\\sum_{l=1}^{K-1}exp(\\beta_{l0} + \\beta_l^Tx)}\n\\end{equation}\n\\]\nand they clearly sum to one."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.3.3-reduced-rank-linear-discriminant-analysis.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.3.3-reduced-rank-linear-discriminant-analysis.html",
    "title": "4.3.3 Reduced-Rank Linear Discriminant Analysis",
    "section": "",
    "text": "The K centroids (p-dimensional) lie in an affine subspace of dimension &lt;= K - 1. Morever, in locating the closest centroid, we can ignore distances orthogonal to this subspace, since they contribute equally to each class. Thus, we might just project the \\(X^{*}\\) onto this subspace.\nIf K = 3, this could allow us to view the data in a two dimensional plot.\nIf K &gt; 3, we might then ask for a L &lt; K - 1 dimensional subspace \\(H_l \\subseteq H_{k-1}\\). In summary, finding the sequences of optimal subspaces for LDA involves the following steps:\n\ncompute the \\(K \\times p\\) matrix of class centroids M and the common covariance matrix W (for within-class covariance);\nCompute \\(\\mathbf{M}^{*}=\\mathbf{M}\\mathbf{W}^{-1/2}\\) using the eigen-decomposition of W;\nCompute \\(\\mathbf{B}^{*}\\), the covariance matrix of \\(\\mathbf{M}^{*}\\) (B the between-class covariance), and its eigen-decomposition \\(\\mathbf{B}^{*}=\\mathbf{V}^{*}\\mathbf{D}_B\\mathbf{V}^{*T}\\). The columns \\(v_l^{*} \\text{ of } \\mathbf{V}^{*}\\) in sequence from first to last define the coordinates of the optimal subspaces.\n\nCombining all these operations the lth discriminant variable is given by \\(Z_l = v_l^TX\\) with \\(v_l=\\mathbf{W}^{-1/2}v_l^{*}\\).\nTODO: Implement the algorithm and plot Figure 4.8.\nFisher arrived at this decomposition via a different route. He posed the problem:\n\nFind the linear combination Z = a^T X such that the between-class variance is maximized relative to the within-class variance.\n\nThe between-class variance of Z is \\(a^T\\mathbf{B}a\\) and within-class variance \\(a^T\\mathbf{W}a\\), where W is defined earlier, and B is the covariance matrix of the class centroid matrix M. Note that \\(\\mathbf{B}+\\mathbf{W}=\\mathbf{T}\\), where T is the total covariance matrix of X, ignoring class information.\nFisher’s problem amounts to maximizing the Rayleigh quotient (4.15): \\[\n\\max_{a} \\cfrac{a^T\\mathbf{B}a}{a^T\\mathbf{W}a}\n\\]\nor equivalently (4.16):\n\\[\n\\max_{a} a^T\\mathbf{B}a \\text{ subject to } a^T\\mathbf{W}a = 1.\n\\]\nThis is a generalized eigenvalue problem, with a given by the largest eigenvalue of \\(\\mathbf{W}^{-1}\\mathbf{B}\\). - The optimal \\(a_1\\) is identical to \\(v_1\\) (defined above) - The next direction \\(a_2\\), orthogonal in W to \\(a_1\\) is \\(a_2=v_2\\). - The \\(a_l\\) are referred to as discriminant coordinates, or canonical variates.\nTo summarize the developments so far:\n\nClassification can be achieved by sphering the data with respect to W.\nSince only the relative distance to the centroids count, one can confine the data to the subspace spanned by the centroids in the sphered space.\nThis subsapce can be further decomposed into successively optimal subspaces in term of centroid separation. This decomposition is identical to the decomposition due to Fisher.\n\nTODO: Finish the last paragraphs."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-11/11.7-example-zip-code-data.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-11/11.7-example-zip-code-data.html",
    "title": "11.7 Example: ZIP Code Data (WIP)",
    "section": "",
    "text": "Net-1: No hidden layer, equivalent to multinomial logistic regression.\nNet-2: One hidden layer, 12 hidden units fully connected.\nNet-3: Two hidden layers locally connected. (3x3 patch\nNet-4: Two hidden layers, locally connected with weight sharing.\nNet-5: Two hidden layers, locally connected, two levels of weight sharing.\n\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n\ndef load_data(path):\n    df = pd.read_csv(path, delim_whitespace=True, header=None)\n    df_y = df.pop(0)\n    return (tf.convert_to_tensor(df.values, dtype=tf.float32), \n            tf.convert_to_tensor(df_y.values, dtype=tf.int32))\n\ntrain_x, train_y = load_data('../data/zipcode/zip.train')\ntest_x, test_y = load_data('../data/zipcode/zip.test')\nepochs = 30\n\n\nfrom abc import ABC, abstractmethod\n\nclass BaseModel(ABC, tf.keras.Model):\n    def __init__(self):\n        super(BaseModel, self).__init__()\n       \n    @abstractmethod\n    def call(self, x):\n        pass\n\n    def loss(self, x, y):\n        preds = self(x)\n        return tf.keras.losses.sparse_categorical_crossentropy(y, preds, from_logits=True)\n\n    def grad(self, x, y):\n        with tf.GradientTape() as tape:\n            loss_value = self.loss(x, y)\n        return tape.gradient(loss_value, self.variables)\n    \n    def accuracy(self, dataset):\n        accuracy = tf.metrics.Accuracy()\n        for (x, y) in dataset:\n            preds = tf.argmax(self(x), axis=1, output_type=tf.int32)\n            accuracy(preds, y)\n        return accuracy.result()\n       \n    def fit(self, x, y, test_x, test_y, epochs = 60, batch_size = 128):\n        dataset = tf.data.Dataset.from_tensor_slices((x, y)) \\\n                                 .shuffle(buffer_size=1000) \\\n                                 .batch(batch_size)\n        test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size)\n        \n        train_hist = []\n        test_hist = []\n\n        optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n        for i in range(epochs):\n            for (x, y) in dataset:\n                grads = self.grad(x, y)\n                optimizer.apply_gradients(zip(grads, self.variables))\n            train_hist.append(self.accuracy(dataset))\n            test_hist.append(self.accuracy(test_dataset))\n\n        return np.array(train_hist), np.array(test_hist)\n\n\n# Set up plotting\nfig = plt.figure(figsize = (12, 6))\ntrain_axes = fig.add_subplot(1, 2, 1)\ntrain_axes.set_title('Train Results')\ntrain_axes.set_xlabel('Epochs')\ntrain_axes.set_ylabel('% Correct on Train Data')\ntrain_axes.set_ylim([60, 100])\n\ntest_axes = fig.add_subplot(1, 2, 2)\ntest_axes.set_title('Test Results')\ntest_axes.set_xlabel('Epochs')\ntest_axes.set_ylabel('% Correct on Test Data')\ntest_axes.set_ylim([60, 100])\n\ndef plot_model(model, label, conv=False):\n    shape = [-1, 16, 16, 1] if conv else [-1, 16*16]\n    shaped_train_x = tf.reshape(train_x, shape)\n    shaped_test_x = tf.reshape(test_x, shape)\n    print(shaped_train_x.shape)\n    print(shaped_test_x.shape)\n    epochs_hist = np.arange(1, epochs + 1)\n    train_hist, test_hist = model.fit(shaped_train_x, train_y, \n                                      test_x=shaped_test_x, test_y=test_y, \n                                      epochs=epochs)\n    train_axes.plot(epochs_hist, train_hist * 100, label=label)\n    train_axes.legend()\n    test_axes.plot(epochs_hist, test_hist * 100, label=label)\n    test_axes.legend()\n\n\n\n\n\n\n\n\n\nclass Net1(BaseModel):\n    def __init__(self):\n        super(Net1, self).__init__()\n        self.layer = tf.keras.layers.Dense(units=10)\n    \n    def call(self, x):\n        return self.layer(x)\n\nplot_model(Net1(), 'Net-1')\nfig\n\n(7291, 256)\n(2007, 256)\n\n\n\n\n\n\n\n\n\n\nclass Net2(BaseModel):\n    def __init__(self):\n        super(Net2, self).__init__()\n        self.layer1 = tf.keras.layers.Dense(units=12, activation=tf.sigmoid)\n        self.layer2 = tf.keras.layers.Dense(units=10)\n\n    def call(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        return out\n\nplot_model(Net2(), 'Net-2')\nfig\n\n(7291, 256)\n(2007, 256)\n\n\n\n\n\n\n\n\n\n\nclass Net3(BaseModel):\n    def __init__(self):\n        super(Net3, self).__init__()\n        self.layer1 = tf.keras.layers.LocallyConnected2D(1, 2, strides=2, activation='sigmoid')\n        self.layer2 = tf.keras.layers.LocallyConnected2D(1, 5, activation='sigmoid')\n        self.layer3 = tf.keras.layers.Flatten()\n        self.layer4 = tf.keras.layers.Dense(units=10)\n\n\n    def call(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        return out\n\nplot_model(Net3(), 'Net-3', conv=True)\nfig\n\n(7291, 16, 16, 1)\n(2007, 16, 16, 1)\n\n\n\n\n\n\n\n\n\n\nclass Net4(BaseModel):\n    def __init__(self):\n        super(Net4, self).__init__()\n        self.layer1 = tf.keras.layers.Conv2D(2, 2, strides=2, activation='sigmoid')\n        self.layer2 = tf.keras.layers.LocallyConnected2D(1, 5, activation='sigmoid')\n        self.layer3 = tf.keras.layers.Flatten()\n        self.layer4 = tf.keras.layers.Dense(units=10)\n\n\n    def call(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        return out\n\nplot_model(Net4(), 'Net-4', conv=True)\nfig\n\n(7291, 16, 16, 1)\n(2007, 16, 16, 1)\n\n\n\n\n\n\n\n\n\n\nclass Net5(BaseModel):\n    def __init__(self):\n        super(Net5, self).__init__()\n        self.layer1 = tf.keras.layers.Conv2D(2, 2, strides=2, activation='sigmoid')\n        self.layer2 = tf.keras.layers.Conv2D(4, 5, activation='sigmoid')\n        self.layer3 = tf.keras.layers.Flatten()\n        self.layer4 = tf.keras.layers.Dense(units=10)\n\n\n    def call(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        return out\n\nplot_model(Net5(), 'Net-5', conv=True)\nfig\n\n(7291, 16, 16, 1)\n(2007, 16, 16, 1)"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.3.1-regularized-discriminant-analysis.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.3.1-regularized-discriminant-analysis.html",
    "title": "4.3.1 Regularized Discriminant Analysis",
    "section": "",
    "text": "The regularized covariance matrices have the form (4.13): \\[\n\\hat{\\Sigma}_k(\\alpha)=\\alpha \\hat{\\Sigma}_k + (1 - \\alpha)\\hat{\\Sigma}\n\\]\nHere \\(\\alpha \\in [0, 1]\\) allows a continuum of models between LDA and QDA.\nSimilar modifications allow \\(\\hat{\\Sigma}\\) itself to be shrunk toward the scalar covariance (4.14), \\[\n\\hat{\\Sigma}(\\gamma)=\\gamma\\hat{\\Sigma}+(1-\\gamma)\\hat{\\sigma}^2\\mathbf{I}\n\\]\nfor \\(\\gamma \\in [0, 1]\\). Replacing \\(\\hat{\\Sigma}\\) in (4.13) by \\(\\hat{\\Sigma}(\\gamma)\\) leads to a more general family of covariances \\(\\hat{\\Sigma}(\\alpha, \\gamma)\\)."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.3-linear-discriminant-analysis.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.3-linear-discriminant-analysis.html",
    "title": "4.3 Linear Discriminant Analysis",
    "section": "",
    "text": "Suppose \\(f_k(x)\\) is the class-conditional density of X, and let \\(\\pi_k\\) be the prior-probability, with \\(\\sum \\pi_k = 1\\). The Bayes theorem gives us (4.7):\n\\[\nPr(G = k, X = x) = \\cfrac{f_k(x)\\pi_k}{\\sum_{l=1}^K f_l(x)\\pi_l}\n\\]\nSuppose that we model each class density as multivariate Gaussian (4.8):\n\\[\nf_k(x) = \\cfrac{1}{(2\\pi)^{p/2} |\\Sigma_k|^{1/2}} e^{-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)}\n\\]\nLinear discriminant analysis (LDA) arises when \\(\\Sigma_k = \\Sigma \\text{ }\\forall k\\). The log-ration between two classes \\(k \\text{ and } l\\) is (4.9):\n\\[\n\\begin{align}\nlog \\cfrac{PR(G=k|X=x)}{PR(G=l|X=x)} &= log \\cfrac{f_k(x)}{f_l(x)} + log \\cfrac{\\pi_k}{\\pi_l}\\\\\n&= log \\cfrac{\\pi_k}{\\pi_l} - \\frac{1}{2}(\\mu_k+\\mu_l)^T\\Sigma^{-1}(\\mu_k - \\mu_l)\n+ x^T\\Sigma^{-1}(\\mu_k-\\mu_l),\n\\end{align}\n\\]\nan equation is linear in x.\nFrom (4.9) we see that the linear discriminant functions (4.10):\n\\[\n\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + log \\pi_k\n\\]\nare an equivalent description of the decision rule, with \\(G(x) = argmax_k \\delta_k(x)\\).\nIn practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using the training data:\n\n\\(\\hat{\\pi_k} = N_k / N, N_k\\) is the number of class-k observations;\n\\(\\hat{\\mu}_k = \\sum_{g_i = k} x_i / N_k\\)\n\\(\\hat{\\Sigma} = \\sum_{k=1}^K\\sum_{g_i=k} (x_i - \\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T / (N - K)\\)\n\nWith two classes, the LDA rule classifies to class 2 if (4.11):\n\\[\nx^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1) &gt;\n\\frac{1}{2}\\hat{\\mu}_2^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_2\n- \\frac{1}{2}\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1\n+ log(N_1/N)\n- log(N_2/N)\n\\]\nSuppose we code the targets in the 2-classes as +1 and -1. It is easy to show that the coefficient vector from least squares is proportional to the LDA direction given in (4.11). However unless \\(N_1 = N_2\\) the intercepts are different. (TODO: solve exercise 4.11)\nSince LDA direction via least squares does not use a Gaussian assumption, except the derivation of the intercept or cut-point via (4.11). Thus it makes sense to choose the cut-point that minimizes the training error.\nWith more than two classes, LDA is not the same as linear regression and it avoids the masking problems.\nQuadratic discriminant functions\nIf \\(\\Sigma_k\\) are assumed to be equal, then we get quadratic discriminant functions (QDA) (4.12): \\[\n\\delta_k(x)=\\cfrac{1}{2}log|\\Sigma_k| - \\cfrac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)+log \\pi_k\n\\]"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.2-linear-regression-of-an-indicator-matrix.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.2-linear-regression-of-an-indicator-matrix.html",
    "title": "4.2 Linear Regression of an Indicator Matrix",
    "section": "",
    "text": "if \\(\\mathcal{G}\\) has K classes, there will be K indicators \\(Y_k, k = 1,...,K\\) with Y_k = 1 if G = k else 0.\nThe N training instance will form an \\(N \\times K\\) indicator response matrix Y.\n\nWe fit a linear regression model to columns of Y simultaneously (4.3): \\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\]\nand the \\((p+1)\\times K\\) coefficient matrix: \\(\\mathbf{\\hat{B}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)\nA new input x is classified as follows:\n\nCompute the fitted output \\(\\hat{f}(x)^T=(1, x^T)\\hat{\\mathbf{B}}\\), a K vector;\nIdentify the largest component and classify accordingly (4.4): \\[\n\\hat{G}(x) = argmax_{k \\in \\mathcal{G}}  \\hat{f}_k(x)\n\\]\n\nSome properties: - \\(\\sum_{k \\in \\mathcal{G}} \\hat{f}_k(x) = 1\\)\n\nHowever \\(\\hat{f}_k(x)\\) can be negative or greater than 1. Thus violets probability distribution.\nThe violation doesn’t say that it won’t work.\n\nA more simplistic viewpoint is to construct targets \\(t_k\\) for each class, where \\(t_k\\) is the kth column of the \\(\\mathbf{I}_k\\), then fit the linear model by least squares (4.5): \\[\n\\underset{\\mathbf{B}}{min} \\sum_{i=1}^N \\| y_i - [(1, x_i^T)\\mathbf{B}]^T\\|^2\n\\]\nis a sum-of-squared Euclidean distances of the fitted vectors from their targets. A new observation is classified to the closest target (4.6):\n\\[\n\\hat{G}(x) = \\underset{k}{argmin} \\| \\hat{f}(x) - t_k \\|^2\n\\]\n\nThe sum-of-squared-norm criterian is exactly the criterian for multiple response linear regression, just view differently. The components of (4.5) can rearranged as a separate linear model for each element.\nThe closest target classification rule is exactly as the same as the maximum fitted component cirterion.\n\nThere is a serious problem with the regression approach when K &gt;= 3. Because of the rigid nature of the regression model, classes can be masked by others. A general rule to fix this issue for K &gt;= 4 classes is using polynomial terms up to degrees K - 1.\nTODO: implement FIGURE 4.2\nTODO: implement FIGURE 4.3"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.4.4-L-1-regularized-logistic-regression.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.4.4-L-1-regularized-logistic-regression.html",
    "title": "4.4.4 L1 Regularized Logistic Regression",
    "section": "",
    "text": "The L1 penalty for logistic regression, we would maximize a penalized version of 4.20 (4.31): \\[\n\\max_{\\beta_0, \\beta} \\left\\{\n\\sum_{i=1}^N \\left[y_i(\\beta_0+\\beta^Tx_i) - \\log(1+e^{\\beta_0+\\beta^Tx_i}) \\right]\n-\\gamma\\sum_{j=1}^p |\\beta_j|\n\\right\\}\n\\]\nAs with the lasso, we do not penalize the intercept term, and standardize the predictors for the penalty. Criterion (4.31) is concave, and a solution can be found using nonlinear programming methods. Alternatively, using the same quadratic approximations that were used in the Newton algorithm, we can solve by repeated application of a weighted lasso algorithm (4.32): \\[\n\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{p})=\\gamma \\cdot sign(\\beta_j)\n\\]\nPath algorithms such as LAR for lasso are more difficult, because the coefficient profiles are piecewise smooth rather than linear. Nevertheless, progress can be made using quadratice approximations."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-04/4.4.1-fitting-logistic-regression-models.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-04/4.4.1-fitting-logistic-regression-models.html",
    "title": "4.4.1 Fitting Logistic Regression Models",
    "section": "",
    "text": "Logistic regression models fit by maximum likelihood, the log-likelihood for N observations is (4.19): \\[\nl(\\theta)=\\sum_{i=1}^N \\log p_{g_i}(x_i, \\theta),\n\\]\nWe discuss in detail the two-class case, it is convenient to code the two-class \\(g_i\\) via 0/1 response \\(y_i\\). The log-likelihood can be written (4.20): \\[\n\\begin{align}\nl(\\beta) &= \\sum_{i=1}^N \\left \\{y_i \\log p(x_i; \\beta) + (1-y_i)log(1 - p(x_i;\\beta)) \\right \\}\\\\\n&= \\sum_{i=1}^N \\left \\{\n  y_i(\\beta^Tx_i - log(1+e^{\\beta^Tx_i})) -\n  (1-y_i)log(1+e^{\\beta^Tx_i})\n\\right\\}\\\\\n&= \\sum_{i=1}^N \\left \\{ y_i\\beta^Tx_i - \\log (1+e^{\\beta^Tx_i}) \\right\\}\n\\end{align}\n\\]\nHere \\(\\beta = \\{\\beta_{10}, \\beta_1\\}\\), and we assume that the vector of inputs \\(x_i\\) includes the constant term 1.\nTo maximize the log-likelihood, we set its derivative to zero (4.21): \\[\n\\begin{align}\n\\cfrac{\\partial l(\\beta)}{\\partial \\beta} &=\\cfrac{\\partial \\left [\n\\sum_{i=1}^N \\left \\{ y_i\\beta^Tx_i - \\log (1+e^{\\beta^Tx_i}) \\right\\}\n\\right]}\n{\\partial \\beta}\\\\\n&= \\sum_{i=1}^N \\left \\{ y_ix_i - \\cfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}   x_i\\right\\}\\\\\n&= \\sum_{i=1}^N x_i(y_i-p(x_i; \\beta))\n\\end{align}\n\\]\nwhich are p + 1 equations nonlinear in \\(\\beta\\). Notice that since the first component of \\(x_i\\) is 1, the first score equation specifies that \\(\\sum_{i=1}^N y_i = \\sum_{i=1}^N p(x_i;\\beta)\\), the expected number of class ones matches the observed number (ane hence also class twos).\nTo solve the score equation (4.21), we use the Newton-Raphson algorithm, which requires the second-derivatie or Hessian Matrix (4.22): \\[\n\\begin{align}\n\\cfrac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\n&= \\cfrac{\\partial \\left[ \\sum_{i=1}^N x_iy_i - x_i\\cfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}\\right]}{ \\partial \\beta^T}\\\\\n&= \\cfrac{\\partial \\left[ \\sum_{i=1}^N -x_i\\cfrac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}}\\right]}{ \\partial \\beta^T}\\\\\n&= \\sum_{i=1}^N -x_i x_i^T\n   \\cfrac{\n     e^{\\beta^Tx_i}(1+e^{\\beta^Tx_i}) - e^{2\\beta^Tx_i}\n   }{\n     (1+e^{\\beta^Tx_i})^2\n   }\\\\\n&= \\sum_{i=1}^N -x_i x_i^T\n\\cfrac{ e^{\\beta^Tx_i} }{ 1+e^{\\beta^Tx_i} }\n\\cfrac{ 1 }{ 1+e^{\\beta^Tx_i} }\\\\\n&= -\\sum_{i=1}^N x_ix_i^Tp(x_i;\\beta)(1-p(x_i;\\beta))\n\\end{align}\n\\]\nA single Newton update is (4.23): \\[\n\\beta^{new}=\\beta^{old} -\n\\left (\n  \\cfrac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\n\\right)^{-1}\n\\cfrac{\\partial l(\\beta)}{\\partial \\beta}\n\\]\nWe can write it as (4.24, 4.25): \\[\n\\begin{equation}\n\\cfrac{\\partial l(\\beta)}{\\partial \\beta} = \\mathbf{X}^T(\\mathbf{y}-\\mathbf{p})\\\\\n\\cfrac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T} = -\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\n\\end{equation}\n\\] where:\n\nX - the \\(N \\times (p + 1)\\) input matrix,\np - the vector of fitted probabilities.\nW - a \\(N \\times N\\) diagonal matrix with ith element \\(p(x_i, \\beta_{old})(1-p(x_i;\\beta_{old}))\\)\n\nThe Newton step is thus (4.26): \\[\n\\begin{align}\n\\beta^{new}\n&= \\beta^{old} + (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T(\\mathbf{y}-\\mathbf{p})\\\\\n&= (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}\n\\left(\\mathbf{X}\\beta^{old}+\\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p})\\right)\\\\\n&= (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{z}\n\\end{align}\n\\]\nWe re-expressed the Newton step as a weighted least squares step, with the response (4.27): \\[\n\\mathbf{z}=\\mathbf{X}\\beta^{old}+\\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p})\n\\]\nalso known as the adjusted response. These equations get solved repeatedly and referred to as iteratively least squares (IRLS), since each iteration solves the weighted least squares problem(4.28): \\[\n\\beta^{new} \\leftarrow \\underset{\\beta}{argmin} (\\mathbf{z}-\\mathbf{X}\\beta)^T\\mathbf{W}(\\mathbf{z}-\\mathbf{X}\\beta)\n\\]"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.4.4-least-angle-regression.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.4.4-least-angle-regression.html",
    "title": "3.4.4 Least Angle Regression",
    "section": "",
    "text": "Least angle regression (LAR) uses a similar strategy to Forwarf stepwise regression, but only enters “as much” of a predictor as it deserves.\nAlgorithm 3.2\n\nStandardize the predictors to have mean zero and unit norm. Start with the residual \\(\\mathbf{r} = \\mathbf{y} - \\mathbf{\\overline{y}}\\) and \\(\\beta_1,...,\\beta_p = 0\\)\nFind the predictor \\(\\mathbf{x}_j\\) most correlated with \\(\\mathbf{r}\\).\nMove \\(\\beta_j\\) from 0 towards its least-squares coefficient \\(\\langle \\mathbf{x}_j, \\mathbf{r} \\rangle\\), until some other competitor \\(\\mathbf{x}_k\\) has as much correlation with the current residual as does \\(\\mathbf{x}_j\\).\nMove \\(\\beta_j\\) and \\(\\beta_k\\) in the direction defined by their joint least squares coefficient of the current residual on \\(\\langle \\mathbf{x}_j, \\mathbf{x}_k \\rangle\\), until some other competitor \\(\\mathbf{x}_l\\) has as much correlation with the current residual.\nContinue in this way until all \\(p\\) predictors have been entered. After min(N - 1, p) steps, we arrive at the full least-squares solution.\n\nSuppose at the beginning of the kth step:\n\n\\(\\mathcal{A}_k\\) is the active set of variables\n\\(\\beta_{\\mathcal{A}_k}\\) be the coefficients\n\\(\\mathbf{r}_k=\\mathbf{y} - \\mathbf{X}_{\\mathcal{A}_k}\\beta_{\\mathcal{A}_k}\\) is the current residual,\n\nthen the direction for this step is (3.55):\n\\[\\delta_k = (\\mathbf{X}_{\\mathcal{A}_k}^T\\mathbf{X}_{\\mathcal{A}_k})^{-1}\\mathbf{X}_{\\mathcal{A}_k}^T\\mathbf{r}_k\\]\nThe coefficient profile then evolves as \\(\\beta_{\\mathcal{A}_k}(\\alpha)=\\beta_{\\mathcal{A}_k} + \\alpha \\cdot \\delta_k\\) and the fit vector evolves as \\(\\hat{f}_k(\\alpha)=\\hat{f}_k + \\alpha \\cdot \\mathbf{u}_k\\)\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndf = pd.read_csv('../data/prostate/prostate.data', delimiter='\\t', index_col=0)\nmask_train = df.pop('train')\ndf_y = df.pop('lpsa')\n\ntrain_x = df[mask_train == 'T']\ntrain_y = df_y[mask_train == 'T']\n\ntrain_x_centered = train_x - train_x.mean(axis = 0)\ntrain_x_centered /= np.linalg.norm(train_x_centered, axis=0)\ntrain_y_centered = train_y - train_y.mean()\n\n\ndef lars(X, y):\n    n, p = X.shape\n    mu = np.zeros_like(y)\n    beta = np.zeros(p)\n\n    for _ in range(p):\n        c = X.T @ (y - mu) \n        c_abs = np.abs(c)\n        c_max = c_abs.max() \n\n        active = np.isclose(c_abs, c_max)\n        signs = np.where(c[active] &gt; 0, 1, -1)\n      \n        X_active = signs * X[:, active]\n\n        G = X_active.T @ X_active\n        Ginv = np.linalg.inv(G)\n\n        A = Ginv.sum() ** (-0.5)\n\n        w = A * Ginv.sum(axis = 1)\n        u = X_active @ w\n\n        gamma = c_max / A\n\n        if not np.all(active):\n            a = X.T @ u\n            complement = np.invert(active)\n            cc = c[complement]\n            ac = a[complement]\n            candidates = np.concatenate([(c_max - cc) / (A - ac),\n                                         (c_max + cc) / (A + ac)])\n            gamma = candidates[candidates &gt;= 0].min()\n        mu += gamma * u\n        beta[active] += gamma * signs\n    return mu, beta\n\n\ny_fit, beta = lars(train_x_centered.as_matrix(), train_y_centered.as_matrix())\ntrain_error = np.mean((y_fit - train_y_centered) ** 2)\nprint ('Beta: ', beta)\nprint ('train error: ', train_error)\n\nBeta:  [10.06592433  6.58925225 -1.95834047  4.00665484  5.62572779 -1.65081245\n -0.20495795  3.9589639 ]\ntrain error:  0.43919976805833433\n\n\nAlgorithm 3.2a\n4a. If a non-zero coefficient hits zero, drop its variable from the active set of variables and recompute the current joint least squares direction.\nThe LAR(lasso) algorithm is extremely efficient, requiring the same order of computation as that of a single least squares fit using the p predictors.\nHeuristic argument why LAR and Lasso are similar\nSuppose \\(\\mathcal{A}\\) is the active set of variables at some stage. We can express as (3.56): \\[\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta)=\\lambda \\cdot s_j, j \\in \\mathcal{A}\\]\nalso \\(|\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta)| \\le \\lambda, j \\notin \\mathcal{A}\\). Now consider the lasso criterian (3.57):\n\\[R(\\beta)=\\frac{1}{2}||\\mathbf{y}-\\mathbf{X}\\beta||_2^2 + \\lambda||\\beta||_1\\]\nLet \\(\\mathcal{B}\\) be the active set of variables in the solution for a given value of \\(\\lambda\\), and \\(R(\\beta)\\) is differentiable, and the stationarity conditions give (3.58):\n\\[\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta)=\\lambda \\cdot sign(\\beta_j), j \\in \\mathcal{B}\\]\nComparing (3.56) and (3.58), we see that they are identical only if the sign of \\(\\beta{j}\\) matches the sign of the inner product. That is why the LAR algorithm and lasso starts to differ when an active coefficient passes through zero; The stationary conditions for the non-active variable require that (3.59):\n\\[|\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta)|\\le \\lambda, j \\notin \\mathcal{B}\\]\n\nDegrees-of-Freedom Formula for LAR and Lasso\nWe define the degrees of freedom of the fitted vector \\(\\hat{y}\\) as:\n\\[\ndf(\\hat{y})=\\frac{1}{\\sigma^2}\\sum_{i=1}^N Cov(\\hat{y}_i,y_i)\n\\]\nThis makes intuitive sense: the harder that we fit to the data, the larger this covariance and hence \\(df(\\hat{\\mathbf{y}})\\)."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.4-shrinkage-methods.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.4-shrinkage-methods.html",
    "title": "3.4 Shrinkage Methods",
    "section": "",
    "text": "Subset selection may produces lower prediction error, however, because it is a discrete - variables are either retained or discarded - it often exhibits high variance. Shrinkage methods are more continuous, and don’t suffer as much from high variability."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.1-introduction.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.1-introduction.html",
    "title": "3.1 Introduction",
    "section": "",
    "text": "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs \\(X_1,...,X_p\\). For prediction purposes they can sometimes outperform nonlinear models, e.g in situations with small numbers of training cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be applied to transformations of inputs and this considerably expands their scope."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.4.1-ridge-regression.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.4.1-ridge-regression.html",
    "title": "3.4.1 Ridge Regression",
    "section": "",
    "text": "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized RSS (3.41): \\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} \\left\\{\n\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n+\\gamma\\sum_{j=1}^p{\\beta_j^2}\n\\right\\}\n\\] Here \\(\\gamma \\ge 0\\) is a complexity parameter: \\(\\gamma \\rightarrow \\infty\\), the coefficients are shrunk toward zero (and each other). This idea also used in neural networks (known as weight decay).\nAn equivalent way to write the ridge problem is (3.42): \\[\n\\begin{equation}\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} \\left\\{\n\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n\\right\\},\\\\\n\\text{subject to } \\sum_{j=1}^p {\\beta_j^2 \\le t}\n\\end{equation}\n\\]\nThere is a one-to-one correspondence between \\(\\gamma \\text{ and } t\\). A large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing constraints as in (3.42), this problem is alleviated.\nThe ridge solutions are not equivariant under scaling of the inputs, so one normally standardizes the inputs before solving (3.41).\nNotice that the intercepts has been left out of the penalty term: because, adding a constant to each target \\(y_i\\) would not simply result in a shift of the prediction by the same amount. It can be shown that the solution to (3.41) can be separated into two parts, after reparametrization using centered inputs, i.e replacing \\(x_{ij} \\text{ by } x_{ij}-\\overline{x}_j\\): 1. we estimate \\(\\beta_0 \\text{ as } \\overline{y}\\), 2. The remaining coefficients get estimated by a ridge regression.\nProof:\n\\[\n\\begin{equation}\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} \\left\\{\n\\sum_{i=1}^N \\left( y_i- \\beta_0-\\sum_{j=1}^p\\overline{x}_j\\beta_j -\\sum_{j=1}^p{(x_{ij} - \\overline{x}_j)\\beta_j} \\right)^2\n+\\gamma\\sum_{j=1}^p{\\beta_j^2}\n\\right\\}\\\\\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} \\left\\{\n\\sum_{i=1}^N \\left( y_i- \\beta_0^C  -\\sum_{j=1}^p{(x_{ij} - \\overline{x}_j)\\beta_j} \\right)^2\n+\\gamma\\sum_{j=1}^p{\\beta_j^2}\n\\right\\}\n\\end{equation}\n\\]\nwhere \\(\\beta_0^C = \\beta_0-\\sum_{j=1}^p\\overline{x}_j\\beta_j\\). Now the stationary point w.r.t \\(\\beta_0^C\\) will be: \\[\n\\begin{align}\n\\beta_0^C &= \\frac{1}{N}(\\sum_{i=1}^N y_i- \\sum_{i=1}^N\\sum_{j=1}^p{x_{ij}\\beta_j +\n\\sum_{i=1}^N\\sum_{j=1}^p\\overline{x}_j\\beta_j})\n\\\\\n&=\\frac{1}{N}(\\sum_{i=1}^N y_i- \\sum_{j=1}^p\\beta_j\\sum_{i=1}^N x_{ij} +\nN\\sum_{j=1}^p\\overline{x}_j\\beta_j)\n\\\\\n&=\\frac{1}{N}(\\sum_{i=1}^N y_i- \\sum_{j=1}^p\\beta_j\\overline{x}_jN +\nN\\sum_{j=1}^p\\overline{x}_j\\beta_j)\n\\\\\n&=\\overline{y}\n\\end{align}\n\\]\nThis completes proof.\nWe assume that the centering has been done, so that the matrix X has p columns (3.43):\n\\[\nRSS(\\gamma)=(\\mathbf{y} - \\mathbf{X}\\beta)^T(\\mathbf{y} - \\mathbf{X}\\beta) + \\gamma\\beta^T\\beta\n\\]\nthe ridge regression solutions are easily seen to be: \\[\n\\hat{\\beta}^{ridge}=(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\nwhere I is the \\(p\\times p\\) identity matrix. Notice that the solution is again a linear function of y.\nProof:\n\\[\n\\begin{align}\n\\cfrac{\\partial{RSS(\\gamma)}}{\\partial{\\beta}}\n&=\\cfrac{\\partial{((\\mathbf{y} - \\mathbf{X}\\beta)^T(\\mathbf{y} - \\mathbf{X}\\beta) + \\gamma\\beta^T\\beta)}}{\\partial{\\beta}}\\\\\n&=\\cfrac{\\partial{\\left(\n  \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T\\mathbf{X}\\beta\n  + (\\mathbf{X}\\beta)^T\\mathbf{X}\\beta\n  + \\gamma\\beta^T\\beta\n\\right)}}{\\partial{\\beta}}\\\\\n&=-2\\mathbf{y}^T\\mathbf{X}+2\\beta^T\\mathbf{X}^T\\mathbf{X}+2\\gamma\\beta^T\n\\end{align}\n\\]\nWe set the first derivative to zero:\n\\[\n\\begin{equation}\n\\cfrac{\\partial{RSS(\\gamma)}}{\\partial{\\beta}} = 0\\\\\n-\\mathbf{y}^T\\mathbf{X}+\\beta^T\\mathbf{X}^T\\mathbf{X}+\\gamma\\beta^T=0\\\\\n\\beta^T(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})=\\mathbf{y}^T\\mathbf{X}\\\\\n\\beta^T=\\mathbf{y}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\\\\n\\beta=(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n\\end{equation}\n\\]\nFigure 3.8 shows the ridge coefficient estimates for the prostate cancer example, plotted as functions of \\(df(\\gamma)\\), the effective degrees of freedom implied by the penalty \\(\\gamma\\). In the case of orthonormal inputs, the ridge estimates are: \\(\\hat{\\beta}^{ridge}=\\hat{\\beta}/(1+\\gamma)\\)\nTODO: Implement Figure 3.8.\nRidge expression can be also derived as the mean or mode of a posterior distribution, with a prior distribution. Suppose, \\(y_i \\sim N(\\beta_0+x_i^T\\beta, \\sigma^2)\\) and the \\(\\beta_j \\sim N(0, \\mathcal{T}^2)\\), independently of one another. Then the log-posterior density of \\(\\beta\\), is equal to the expression in (3.41), with \\(\\gamma=\\sigma^2/\\mathcal{T}^2\\).\nTODO: proof.\nThe singular value decomposition (SVD) of the centered input matrix X gives us some insight into the nature of ridge regression. The SVD of the \\(N \\times p\\) matrix X (3.45): \\[\n\\mathbf{X}=\\mathbf{UD}\\mathbf{V}^T\n\\] - U - \\(N \\times p\\) orthogonal matrix, the columns of U span the column space of X. - V - \\(N \\times p\\) orthogonal matrix, the columns of V span the row space of X. - D - \\(p \\times p\\) diagonal matrix, with diagonal entries \\(d_1 \\ge d_2 \\ge ... \\ge d_p \\ge 0\\), called singular values of X. If one or more values \\(d_j = 0\\), X is singular.\nUsing SVD we can write the least squares fitted vector as (3.46):\n\\[\n\\begin{align}\n\\mathbf{X}\\hat{\\beta}^{ls} &= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\\n&= \\mathbf{UD}\\mathbf{V}^T(\\mathbf{VD}\\mathbf{U}^T\\mathbf{UD}\\mathbf{V}^T)^{-1}\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n&= \\mathbf{UD}\\mathbf{V}^T(\\mathbf{VD}\\mathbf{D}\\mathbf{V}^T)^{-1}\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n&= \\mathbf{UD}\\mathbf{V}^T[\\mathbf{V}^T]^{-1}\\mathbf{D}^{-1}\\mathbf{D}^{-1}\\mathbf{V}^{-1}\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n&= \\mathbf{UD}\\mathbf{D}^{-1}\\mathbf{D}^{-1}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\\\\n&= \\mathbf{U}\\mathbf{U}^T\\mathbf{y}\n\\end{align}\n\\]\nNow the ridge solutions are (3.47):\n\\[\n\\begin{align}\n\\mathbf{X}\\hat{\\beta}^{ridge}&=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\\n&=\\mathbf{UD}\\mathbf{V}^T\n(\\mathbf{V}\\mathbf{D}^2\\mathbf{V}^T+\\gamma\\mathbf{V}\\mathbf{V}^T)^{-1}\n\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n&=\\mathbf{UD}\\mathbf{V}^T\n(\\mathbf{V}(\\mathbf{D}^2+\\gamma\\mathbf{I})\\mathbf{V}^T)^{-1}\n\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n&=\\mathbf{UD}\\mathbf{V}^T\n\\mathbf{V}(\\mathbf{D}^2+\\gamma\\mathbf{I})^{-1}\\mathbf{V}^T\n\\mathbf{VD}\\mathbf{U}^T\\mathbf{y}\\\\\n&=\\mathbf{UD}(\\mathbf{D}^2+\\gamma\\mathbf{I})^{-1}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\\\\n&=\\sum_{j=1}^p\\mathbf{u}_j\\cfrac{d_j^2}{d_j^2+\\gamma}\\mathbf{u}_j^T\\mathbf{y}\n\\end{align}\n\\] - Computes the coordinates of y w.r.t the orthonormal basis U. - Then shrinks these coordinates of y by the factors \\(d_j^2/(d_j^2+\\gamma)\\). - A greater amount of shrinkage is applied to the coordinates of basis vectors with smaller \\(d_j^2\\).\nPrincipal Components\nWhat does a small value of \\(d_j^2\\) mean? The sample covariance matrix is \\(\\mathbf{S}=\\mathbf{X}^T\\mathbf{X}/N\\) (3.46): \\[\n\\mathbf{X}^T\\mathbf{X} = \\mathbf{VD}^2\\mathbf{V}^T\n\\]\nwhich is eigen decomposition of \\(\\mathbf{X}^T\\mathbf{X}\\), aka the principal components directions of X.\nThe first principal component (PC) direction \\(v_1\\) has the largest variance, that is: \\[\n\\begin{align}\nVar(\\mathbf{z}_1) &= Var(\\mathbf{X}v_1)\\\\\n&= \\frac{1}{N} v_1^T\\mathbf{VD}^2\\mathbf{V}^Tv_1\\\\\n&= \\frac{1}{N}\n   \\begin{pmatrix} 1&0 & \\dots & 0 \\end{pmatrix}\n   \\mathbf{D}^2\n   \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\\\\n&=\\frac{d_1^2}{N}\n\\end{align}\n\\]\nand in fact \\(\\mathbf{z}_1 = \\mathbf{X}v_1 = \\mathbf{UD}\\mathbf{V}^{T}v_1= \\mathbf{u}_1d_1\\) and it is called the first PC. Ridge regression shrinks PC directions having small variance the most.\nEffective degrees of freedom\nIn Figure 3.7 we have plotted the estimated prediction error versus the quantity: \\[\n\\begin{align}\ndf(\\gamma) &= tr[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\gamma\\mathbf{I})^{-1}\\mathbf{X}^T]\\\\\n&= tr(\\mathbf{H_{\\gamma}})\\\\\n&= tr[\\mathbf{UD}(\\mathbf{D}^2+\\gamma\\mathbf{I})^{-1}\\mathbf{D}\\mathbf{U}^T]\\\\\n&= \\sum_{j=1}^p \\cfrac{d_j^2}{d_j^2+\\gamma}\n\\end{align}\n\\]\nThis monotone decreasing function of \\(\\gamma\\) is the effective degrees of freedom of the ridge regression fit. Note that: - \\(\\gamma = 0 \\text{  then  } df(\\gamma)=p\\) - \\(\\gamma \\rightarrow \\infty \\text{  then  } df(\\gamma) \\rightarrow 0\\) - Although all p coefficients in a ridge fit will be non-zero, they are fit in a restricted fashion controlled by \\(\\gamma\\)."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.2.3-multiple-regression-from-simple-multivariate-regression.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.2.3-multiple-regression-from-simple-multivariate-regression.html",
    "title": "3.2.3 Multiple Regression From Simple Univariate Regression",
    "section": "",
    "text": "Suppose we have a univariate (p = 1) model with no intercept (3.23): \\[Y=X\\beta+\\varepsilon\\]\nThe least squares estimate and residuals are (3.24): \\[\n\\begin{equation}\n\\hat{\\beta} = \\cfrac{\\sum_1^N {x_iy_i}}{\\sum_1^N {x_i^2}} \\\\\nr_i = y_i - x_i\\hat{\\beta}\n\\end{equation}\n\\]\nWith the inner product: \\[\n\\begin{equation}\n\\hat{\\beta} = \\cfrac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\langle \\mathbf{x}, \\mathbf{x}\\rangle}\\\\\n\\mathbf{r} = \\mathbf{y} - \\mathbf{x}\\hat{\\beta}\n\\end{equation}\n\\]\nSuppose that the columns of the matrix X are orthogonal; that is \\(\\langle \\mathbf{x}_j, \\mathbf{x}_k \\rangle = 0\\) then it is easy to check that \\(\\hat{\\beta_j} = \\langle \\mathbf{x}_j, \\mathbf{y} \\rangle / \\langle \\mathbf{x}_j, \\mathbf{x}_j \\rangle\\), i.e the inputs have no effect on each other’s parameter estimates.\nSuppose next that we have an intercept and a single input x (3.27): \\[\\hat{B}_1 = \\cfrac{\\langle \\mathbf{x} - \\overline{x}\\mathbf{1}, \\mathbf{y} \\rangle}{ \\langle \\mathbf{x} - \\overline{x}\\mathbf{1}, \\mathbf{x} - \\overline{x}\\mathbf{1} \\rangle}\\]\nWe can view the estimate as the result of two simple regression:\n\nRegress x on 1 to produce the residual \\(\\mathbf{z} = \\mathbf{x} - \\overline{x}\\mathbf{1}\\)\nRegress y on the residual z to give the coefficient \\(\\hat{\\beta}_1\\).\n\nRegress b on a means \\(\\hat{\\gamma}=\\langle \\mathbf{a},\\mathbf{b} \\rangle / \\langle \\mathbf{a}, \\mathbf{a}\\rangle\\) and the residual vector \\(\\mathbf{b} - \\hat{\\gamma}\\mathbf{a}\\).\nThis recipe generalizes to the case of p inputs, as shown in Algorithm 3.1.\nAlgorithm 3.1 Regression by Successive Orthogonalization 1. \\(\\mathbf{z}_0 = \\mathbf{x}_0 = \\mathbf{1}\\)\n\nFor \\(j = 1, 2, \\cdots, p\\)\n\nRegress \\(\\mathbf{x}_j\\) on \\(\\mathbf{z}_0,...,\\mathbf{z}_{j - 1}\\) to produce \\(\\hat{\\gamma}_{lj}=\\langle \\mathbf{z}_l, \\mathbf{x}_j \\rangle / \\langle \\mathbf{z}_l,\\mathbf{z}_l \\rangle\\) \\(l=0,\\cdots,j-1\\), and residualt vector \\(\\mathbf{z}_j=\\mathbf{x}_j - \\sum_{k=0}^{j-1} \\hat{\\gamma}_{kj}\\mathbf{z}_k\\)\n\nRegress \\(\\mathbf{y}\\) on the residual \\(\\mathbf{z}_p\\) to give the estimate \\(\\hat{\\beta}_p\\)\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats, linalg\n\ndf = pd.read_csv('../data/prostate/prostate.data', delimiter='\\t', index_col=0)\nmask_train = df.pop('train')\ndf_y = df.pop('lpsa')\ndf = df.apply(stats.zscore)\n\ndef orthogonalize(X):\n    p = X.shape[1]\n    G = np.eye(p)\n    Z = X.copy()\n    for j in range(1, p): \n        for l in range(j):\n            G[l, j] = np.dot(Z[:, l], X[:, j]) / np.dot(Z[:, l], Z[:, l])\n        for k in range(j):\n            Z[:, j] -= G[k, j] * Z[:, k]\n    return Z, G\n\nThe result of this algorithm is (3.28):\n\\[\\hat{\\beta}_p=\\cfrac{\\langle \\mathbf{z}_p, \\mathbf{y} \\rangle}{\\langle \\mathbf{z}_p,\\mathbf{z}_p \\rangle}\\]\nIf \\(\\mathbf{x}_p\\) is highly correlated with some of the other \\(\\mathbf{x}_k\\)’s the residual vector \\(\\mathbf{x}_p\\) will be close to zero, and from (3.28) the coefficient \\(\\hat{\\beta}_p\\) will be unstable.\nFrom (3.28) we also obtain an alternative formula for the variance estimates, (3.29):\n\\[Var(\\hat{\\beta}_p) = \\cfrac{\\sigma^2}{\\langle \\mathbf{z}_p, \\mathbf{z}_p \\rangle}=\\cfrac{\\sigma^2}{||\\mathbf{z}_p||^2}  \\]\nOn other words, the precision with which we can estimate \\(\\hat{\\beta}_p\\) depends on the lengths of the residual vector \\(\\mathbf{z}_p\\);\nAlgorithm 3.1 is known as the Gram–Schmidt procedure for multiple regression. We can represent step 2 of Algorithm 3.1 in matrix form (3.30):\n\\[\\mathbf{X}=\\mathbf{Z\\Gamma}\\]\nwhere \\(\\mathbf{Z}\\) has as columns the \\(z_j\\) (in order), and \\(\\mathbf{\\Gamma}\\) is the upper triangular matrix with entries \\(\\hat{\\gamma}_{kj}\\). Introducing the diagonal matrix \\(\\mathbf{D}\\) with \\(D_{jj}=||z_j||\\), we get (3.31):\n\\[\\mathbf{X}=\\mathbf{Z}\\mathbf{D}^{-1}\\mathbf{D}\\mathbf{\\Gamma}=\\mathbf{QR}\\]\nthe so-called QR decomposition of \\(\\mathbf{X}\\). Here \\(\\mathbf{Q}\\) is an N × (p +1) orthogonal matrix, \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\), and R is a (p + 1) × (p + 1) upper triangular matrix.\nThe least squares solution is given by:\n\\[\n\\hat{\\beta}=\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}\n\\]\nProof: \\[\n\\begin{equation}\n\\mathbf{X}^T\\mathbf{y}=\\mathbf{X}^T\\mathbf{X}\\hat{\\beta}\\\\\n\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y}=\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{R}\\hat{\\beta}\\\\\n\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y}=\\mathbf{R}^T\\mathbf{R}\\hat{\\beta}\\\\\n\\mathbf{Q}^T\\mathbf{y}=\\mathbf{R}\\hat{\\beta}\\\\\n\\end{equation}\n\\] And the predicted training values:\n\\[\n\\hat{\\mathbf{y}}=\\mathbf{QQ}^T\\mathbf{y}\n\\]\nProof:\n\\[\n\\begin{align}\n\\hat{\\mathbf{y}}&=\\mathbf{X}\\hat{\\beta}\\\\\n&=\\mathbf{QR}\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}\\\\\n&=\\mathbf{QQ}^T\\mathbf{y}\n\\end{align}\n\\]\nWe can obtain from it not just \\(\\hat{\\beta}_p\\), but also the entire multiple least squares fit.\nProof: We can easily derive that: \\[\n\\mathbf{R}\\hat{\\beta}=\\mathbf{Q}^T\\mathbf{y}\n\\]\nwhich can be expanded into: \\[\n\\begin{equation}\n\\begin{bmatrix}\n    R_{0 0} & R_{02}  & \\dots   & R_{0p} \\\\\n    0       & R_{11}  & \\dots   & R_{1p} \\\\\n    \\vdots  & \\vdots  & \\ddots  & \\vdots \\\\\n    0       & 0       & \\dots   & R_{pp}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\hat{\\beta_0} \\\\\n    \\hat{\\beta_1} \\\\\n    \\vdots        \\\\\n    \\hat{\\beta_p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    {Q_{0}}^T\\mathbf{y} \\\\\n    {Q_{1}}^T\\mathbf{y} \\\\\n    \\vdots        \\\\\n    {Q_{p}}^T\\mathbf{y}\n\\end{bmatrix}\n\\end{equation}\n\\]\nNow by applying the backward substitution it is possible to obtain the entire multiple least squares fit. For example to find the \\(\\hat{\\beta}_p\\): \\[\n\\begin{equation}\nR_{pp}\\hat{\\beta}_p = {Q_{p}}^T\\mathbf{y}\\\\\n\\hat{\\beta}_p = \\cfrac{\\langle Q_p, \\mathbf{y} \\rangle}{R_{pp}}=\\cfrac{\\langle \\mathbf{z}_p, \\mathbf{y} \\rangle}{\\langle \\mathbf{z}_p,\\mathbf{z}_p \\rangle}\n\\end{equation}\n\\]\n\ndef least_squares_qr(data_x, data_y):\n    X = np.c_[np.ones((len(data_x), 1)), data_x]\n    Z, G = orthogonalize(X)\n\n    D = linalg.norm(Z, axis=0)\n    Q = Z / D\n    R = np.diag(D) @ G\n    beta = linalg.solve_triangular(R, Q.T @ data_y)\n    return beta\n\nbeta = least_squares_qr(df[mask_train == 'T'].as_matrix(), df_y[mask_train == 'T'].as_matrix())\nprint (\"Coefficient: \", beta)\n\nCoefficient:  [ 2.46493292  0.67601634  0.26169361 -0.14073374  0.20906052  0.30362332\n -0.28700184 -0.02119493  0.26557614]"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.4.4-discussion-subset-selection-ridge-regression-and-the-lasso.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.4.4-discussion-subset-selection-ridge-regression-and-the-lasso.html",
    "title": "3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-03/3.2.4-multiple-outputs.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-03/3.2.4-multiple-outputs.html",
    "title": "3.2.4 Multiple Outputs",
    "section": "",
    "text": "Suppose we have multiple outputs \\(Y_1, Y_2, ..., Y_k\\) that we wish to predict from our inputs \\(X_0, X_1, ..., X_p\\). We assume a linear model for each output (3.34, 3.35):\n\\[\n\\begin{align}\nY_k &= \\beta_{0k} + \\sum_{j=1}^{p} X_j\\beta_{jk} + \\varepsilon_k\\\\\n&= f_k(X) + \\varepsilon_k\n\\end{align}\n\\]\nWe can write the model in matrix notation:\n\\[\n\\mathbf{Y} = \\mathbf{XB} + \\mathbf{E}\n\\]\nHere Y is the $NK $ response matrix, X is the \\(N\\times(p+1)\\) input matrix, B is the \\((p+1)\\times K\\) matrix and E is the \\(N\\times K\\) matrix of errors.\nA straightforward generalization of the univariate loss functio is (3.37, 3.38): \\[\n\\begin{align}\nRSS(\\mathbf{B}) &= \\sum_{k=1}^K{\\sum_{i=1}^N (y_{ik} - f_k(x_i))^2}\\\\\n&= tr\\left[(\\mathbf{Y}-\\mathbf{XB})^T(\\mathbf{Y}-\\mathbf{XB})\\right]\n\\end{align}\n\\]\nThe least squares estimates is (3.39): \\[\\hat{\\mathbf{B}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\]\nIf the errors \\(\\varepsilon=(\\varepsilon_1,...,\\varepsilon_K)\\) are correlated, then (3.40):\n\\[\nRSS(B; \\Sigma)=\\sum_{i=1}^N(y_i - f(x_i))^T\\Sigma^{-1}(y_i - f(x_i))\n\\]\narises from multivariate Gaussian theory."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.7-structured-regression-models.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.7-structured-regression-models.html",
    "title": "2.7 Structured Regression Models",
    "section": "",
    "text": "We have seen that although kNN and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more efficient use of the data. This section introduces classes of such approaches.\n\n2.7.1 Difficulty of the Problem\n(2.37) RSS criterian for an arbitrary function \\(f\\):\n\\[RSS(f) = \\sum_{i=1}^N(y_i-f(x_i))^2\\]\nMinimizing (2.37) leads to infinitely many solutions: any function \\(\\hat{f}\\) passing through the training points is a solution. It might be a poor predictor at test points different from the training points. If there are multiple observations at each point \\(x_i\\), i.e \\(y_{il}, l = 1 ... N\\), the risk is limited.\nIn order to obtain useful results for finite N, we must restrict the eligible solutions to a smaller set of functions."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.4-statistical-decision-theory.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.4-statistical-decision-theory.html",
    "title": "2.4 Statistical Decision Theory",
    "section": "",
    "text": "Loss function\nThe most common is squared error loss: \\(L(Y, f(X)) = (Y - f(X))^2\\)\n\n\nExpected prediction Error\n(2.9, 2.10) This leads us to a criterion for choosing \\(f\\),\n\\[EPE(f) = E(Y - f(X))^2 = \\int{[y - f(x)]^2}Pr(dx, dy)\\]\n(2.11) By conditioning on X, we can write EPE as:\n\\[EPE(f) = E_X E_{Y|X} ([Y - f(X)]^2|X) \\]\nProof:\n\\[\n\\begin{equation}\n\\int{[y - f(x)]^2}Pr(dx, dy) \\\\\n= \\int{[y - f(x)]^2}p(x, y)dxdy \\\\\n= \\int{[y - f(x)]^2}p(x)p(y | x)dxdy \\\\\n= \\int_x { \\left( \\int_y {[y - f(x)]^2p(y | x)dy} \\right)p(x)dx } \\\\\n= E_X E_{Y|X} ([Y - f(X)]^2|X)\n\\end{equation}\n\\]\n(2.12) It suffices to minimize EPE pointwise: \\[f(x) = argmin_c E_{Y|X} ( [Y - c]^2 | X = x)\\]\n(2.13) The solution is : \\[f(x)=E(Y | X = x)\\]\nThe nearest-neighbor methods attempt to directly implement this recipe using the training data. Since there is typically at most one observation at any point x, we settle for: \\[\\hat{f}(x) = \\text{Ave}(y_i|x_i \\in N_k(x))\\]\nFor large training sample size N, the points in the neighborhood are likely to be close to x, and as k gets large the average will get more stable.\nHow does linear regression fit into this framework ?\n\\[f(x) \\approx x^T\\beta\\]\nPlugging this into EPE (2.9) and differentiating we can solve for β:\n\\[\\beta= [E(XX^T)]^{-1}E(XY)\\]\nProof:\n\\[\n\\begin{equation}\n\\int{[y - x^T\\beta]^2\\text{ Pr}(dx, dy)} \\\\\n= \\int{[y^2 - 2yx^T+(x^T\\beta)^2]\\text{ Pr}(dx, dy)} \\\\\n\\end{equation}\n\\]\nDifferentiating w.r.t \\(\\beta\\):\n\\[\n\\begin{equation}\n2\\int{xx^T\\beta\\text{ Pr}(dx, dy)} -  2\\int{xy\\text{ Pr}(dx, dy)} = 0 \\\\\n2 \\times (E(XX^T\\beta) - E(XY)) = 0 \\\\\nE(XX^T)\\beta = E(XY) \\\\\n\\beta = [E(XX^T)]^{-1}E(XY)\n\\end{equation}\n\\]\nThe least squares solution \\(\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) amounts to replacing the expectation in \\(\\beta = [E(XX^T)]^{-1}E(XY)\\) by averages over the training data. Note: Each expectation produces N times more than average, however the constant (i.e 1/N) in two expectations cancel out each other.\n\n\nBayes Classifier\n\n%matplotlib inline\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsample_size = 100\n\ndef generate_data(means, size):\n    return np.array([\n        np.random.multivariate_normal(random.choice(means), np.eye(2) / 5)\n        for _ in range(size)\n    ])\n\ndef plot_data(orange_data, blue_data): \n    axes.plot(orange_data[:, 0], orange_data[:, 1], 'o', color='orange')\n    axes.plot(blue_data[:, 0], blue_data[:, 1], 'o', color='blue')\n\nblue_means = np.random.multivariate_normal([1, 0], np.identity(2), 10)\norange_means = np.random.multivariate_normal([0, 1], np.identity(2), 10)\n\nblue_data = generate_data(blue_means, sample_size)\norange_data = generate_data(orange_means, sample_size)\n\n# plotting\nfig = plt.figure(figsize = (8, 8))\naxes = fig.add_subplot(1, 1, 1)\nplot_data(orange_data, blue_data)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom itertools import filterfalse, product\nfrom scipy.stats import multivariate_normal\n\ndef plot_grid(orange_grid, blue_grid):\n    axes.plot(orange_grid[:, 0], orange_grid[:, 1], '.', zorder = 0.001,\n          color='orange', alpha = 0.25, scalex = False, scaley = False)\n\n    axes.plot(blue_grid[:, 0], blue_grid[:, 1], '.', zorder = 0.001,\n          color='blue', alpha = 0.25, scalex = False, scaley = False)\n\ndef pdf(x, means):\n    return np.mean([multivariate_normal.pdf(x, mean = m, cov = np.eye(2) / 5) \n                    for m in means], axis = 0)\n\nplot_xlim = axes.get_xlim()\nplot_ylim = axes.get_ylim()\n\ngrid = np.array([*product(np.linspace(*plot_xlim, 50), np.linspace(*plot_ylim, 50))])\n\norange_pdf = pdf(grid, orange_means)\nblue_pdf = pdf(grid, blue_means)\n\n# Plotting\naxes.clear()\naxes.set_title(\"Bayes Optimal Classifier\")\n\nplot_data(orange_data, blue_data)\nplot_grid(grid[orange_pdf &gt;= blue_pdf], grid[orange_pdf &lt; blue_pdf])\n\nfig"
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.9-model-selection-and-the-bias-variance-tradeoff.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.9-model-selection-and-the-bias-variance-tradeoff.html",
    "title": "2.9 Model Selection and the Bias-Variance Tradeoff",
    "section": "",
    "text": "All the models described have a smoothing or complexity parameter that has to be determined:\n\nthe multiplier of the penalty term;\nthe width of the kernel\nor the number of basis functions\n\nWe cannont use RSS on the training data to determine these parameters, since we would always pick those that gave interpolating fits and have zero residuals.\nThe kNN regression fit \\(\\hat{f_k}(x_0)\\) illustrates the competing forces that effect the predictive ability of such approximations. Suppose the data arise from a model \\(Y = f(X) + \\varepsilon\\) with \\(E(\\varepsilon)=0\\) and \\(Var(\\varepsilon) = \\sigma^2\\). We assume that the values of \\(x_i\\) in the sample are fixed. The EPE at \\(x_0\\):\n\\[\n\\begin{align}\nEPE_k(x_0) &= E[(Y - \\hat{f_k}(x_0))^2|X=x_0]\\\\\n&=\\sigma^2 + [Bias^2(\\hat{f_k}(x_0)) + Var_\\tau(\\hat{f_k}(x_0))]\\\\\n&=\\sigma^2 + \\left[f(x_0) - \\frac{1}{k}\\sum_{l=1}^k{f(x_{(l)})} \\right]^2 + \\frac{\\sigma^2}{k}\n\\end{align}\n\\]\nThe subscripts in parentheses (\\(l\\)) indicates the sequence of nearest neighbors to \\(x_0\\). There are three terms in this expression:\n\n\\(\\sigma^2\\) is the irreducible error - is beyond our control, even if we know the true \\(f(x_0)\\).\nThe bias term and the expected value of the estimate - \\([E_\\tau(\\hat{f_k}(x_0))-f(x_0)]^2\\) - where the expected averages the randomness in the training data. This term increases with \\(k\\) if the function is smooth.\nThe variance term and it decreases as the inverse of k. The expected value of the variance is:\n\n\\[\n\\begin{align}\nVar_\\tau(\\hat{f_k}(x_0)) &= E_\\tau\\left[\\hat{f_k}(x_0) - E_\\tau(\\hat{f_k}(x_0))\\right]^2\\\\\n&= E_\\tau\\left[\\frac{1}{k}\\sum_{l=1}^k (f(x_{(l)}) + \\varepsilon_l) - \\frac{1}{k}\\sum_{l=1}^k{f(x_{(l)})}\\right]^2\\\\\n&= E_\\tau\\left[\\frac{1}{k}\\sum_{l=1}^k \\varepsilon_l\\right]^2\\\\\n&= \\frac{1}{k^2}E_\\tau\\left[\\sum_{l=1}^k \\varepsilon_l\\right]^2\\\\\n&= \\frac{1}{k^2}E_\\tau\\left[\\sum_{l=1}^k \\varepsilon_l^2\\right]\\\\\n&= \\frac{\\sigma^2}{k}\n\\end{align}\n\\]\nAs the model complexity of our procedure is increased, the variance tends to increase and the squared bias tends to decrease. The opposite behavior occurs as the model complexity is decreased."
  },
  {
    "objectID": "esl/the-elements-of-statistical-learning/chapter-02/2.8-classes-of-restricted-estimators.html",
    "href": "esl/the-elements-of-statistical-learning/chapter-02/2.8-classes-of-restricted-estimators.html",
    "title": "2.8 Classes of Restricted Estimators",
    "section": "",
    "text": "The variety of nonparametric regression techniques or learning methods fall into a number of different classes depending on the nature of the restrictions imposed. Each of the classes has associated with it one or more parameters, sometimes called smoothing parameters, that control the effective size of the local neighborhood.\n\n2.8.1 Roughness Penalty and Bayesian Methods\nHere the class of functions is controlled by explicitly penalizing RSS(f) with a roughness penalty\n\\[PRSS(f; \\lambda) = RSS(f) + \\lambda J(f)\\]\nThe user-selected \\(J(f)\\) will be large for functions \\(f\\) that vary too rapidly over small regions of input space. For example, the popular cubic smoothing spline for one-dimensional inputs is the solution to the PRSS:\n\\[PRSS(f; \\lambda) = \\sum_{i=1}^N(y_i - f(x_i))^2 + \\lambda\\int[f^{''}(x)]^2dx\\]\nThe amount of penalty is dictated by \\(\\lambda&gt;=0\\): - For \\(\\lambda=0\\), no penalty imposed, and any interpolating function will do - While for \\(\\lambda = \\infty\\) only linear functions are permitted\nPenalty function, or regularization methods, express our prior belief that the type of functions we seek exhibit a certain type of smooth behavior, and indeed can usually be cast in a Bayesian framework: - The penalty J corresponds to a log-prior - and \\(PRSS(f; \\lambda)\\) the log-posterior distribution; - and minimizing \\(PRSS(f; \\lambda)\\) amounts to finding the posterior mode.\n\n\n2.8.2 Kernel Methods and Local Regression\nThese methods can be thought of as explicitly providing estimates of the regression function or conditional expectation by specifying the nature of the local neighborhood, and of the class of regular functions fitted locally.\nThe local neighborhood is specified by a kernel function \\(K_\\lambda(x_0, x)\\) which assigns weights to points x in a region around \\(x_0\\), e.g the Gaussian kernel based on the Gaussian density function:\n\\[K_\\lambda(x_0, x) = \\frac{1}{\\lambda}exp\\left[-\\frac{||x-x_0||^2}{2\\lambda}\\right]\\]\nand assigns weights to points that die exponentially with their squared euclidean distance from \\(x_0\\) and \\(\\lambda\\) corresponds to the variance of the Gaussian density and controls the width of the neighborhood.\n(2.40) The simplest form of kernel estimate is the Nadaraya-Watson weighted average:\n\\[\\hat{f}(x_0)=\\frac{\\sum_{i=1}^N K_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}\\]\n(2.41) In general we can define a local regression estimate of \\(f(x_0)\\) as \\(f_\\hat{\\theta}(x_0)\\), where \\(\\hat{\\theta}\\) minimizes:\n\\[RSS(f_\\theta, x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)(y_i - f_\\theta(x_i))^2\\]\nand \\(f_\\theta\\) is some parameterized function, e.g:\n\n\\(f_\\theta(x) = \\theta_0\\), the constant function; this results in the Nadaraya-Watson estimate in (2.41)\n\\(f_\\theta(x) = \\theta_0+\\theta_1x\\) gives the local linear regression model.\n\nThese methods needs to be modified in high dimensions, to avoid curse of dimensionality.\n\n\n2.8.3 Basis Functions and Dictionary Methods\nThis class of methods includes the linear and polynomial expansions, but more importantly a wide variety of more flexible models. The models for \\(f\\) is a linear expansion of basis functions:\n\\[f_\\theta(x) = \\sum_{m=1}^M \\theta_{m}h_m(x)\\]\nthe term linear here refers to the action of the parameters \\(\\theta\\).\nTODO: 1D polynomial splines of degrees K.\nRadial basis functions are symmetric p-dimensional kernels located at particular centroids:\n\\[f_\\theta(x) = \\sum_{m=1}^M{K_{\\lambda_m}(\\mu_m, x)}\\theta_m\\]\ne.g the Gaussian kernel \\(K_\\lambda(\\mu, x) = e^{-||x-\\mu||^2}/2\\lambda\\) is popular. Radial basis functions have centroids \\(\\mu_m\\) and scales \\(\\lambda_m\\) that have to be determined. In general, we would like the data to dictate them.\nA single-layer feed-forward neural networks model with linear output weights can be thought of as an adaptive basis function methods:\n\\[f_\\theta(x) = \\sum_{m=1}^M{\\beta_m\\sigma(\\alpha_m^T{x}+b_m)}\\]\nwhere \\(\\sigma(x) = 1/(1+e^{-x})\\) is know as the activation function.\nThese adaptively chosen basis function methods are also know as dictionary methods, where one has available a infinite set or dictionary \\(\\mathcal{D}\\) of candidate basis function from which to choose."
  },
  {
    "objectID": "esl/index.html#chapter-4",
    "href": "esl/index.html#chapter-4",
    "title": "The Elements of Statistical Learning Notebooks",
    "section": "Chapter 4",
    "text": "Chapter 4\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n4.1 Introduction\n\n\n\n\n\n\n4.2 Linear Regression of an Indicator Matrix\n\n\n\n\n\n\n4.3 Linear Discriminant Analysis\n\n\n\n\n\n\n4.3.1 Regularized Discriminant Analysis\n\n\n\n\n\n\n4.3.2 Computations for LDA\n\n\n\n\n\n\n4.3.3 Reduced-Rank Linear Discriminant Analysis\n\n\n\n\n\n\n4.4 Logistic Regression\n\n\n\n\n\n\n4.4.1 Fitting Logistic Regression Models\n\n\n\n\n\n\n4.4.2 Example: South African Heart Disease\n\n\n\n\n\n\n4.4.3 Quadratic Approximations and Inference\n\n\n\n\n\n\n4.4.4 L1 Regularized Logistic Regression\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "esl/index.html#chapter-2",
    "href": "esl/index.html#chapter-2",
    "title": "The Elements of Statistical Learning Notebooks",
    "section": "Chapter 2",
    "text": "Chapter 2\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n2.3 Least Squares and Nearest Neighbors\n\n\n\n\n\n\n2.4 Statistical Decision Theory\n\n\n\n\n\n\n2.5 Local Methods in High Dimensions\n\n\n\n\n\n\n2.6 Statistical Models, Supervised Learning and Function Approximation\n\n\n\n\n\n\n2.7 Structured Regression Models\n\n\n\n\n\n\n2.8 Classes of Restricted Estimators\n\n\n\n\n\n\n2.9 Model Selection and the Bias-Variance Tradeoff\n\n\n\n\n\n\nEx. 2.8\n\n\n\n\n\n\nNo matching items\n\n\nChapter 3\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n3.1 Introduction\n\n\n\n\n\n\n3.2 Linear Regression Models and Least Squares\n\n\n\n\n\n\n3.2.2 The Gauss–Markov Theorem\n\n\n\n\n\n\n3.2.3 Multiple Regression From Simple Univariate Regression\n\n\n\n\n\n\n3.2.4 Multiple Outputs\n\n\n\n\n\n\n3.3 Subset Selection\n\n\n\n\n\n\n3.4 Shrinkage Methods\n\n\n\n\n\n\n3.4.1 Ridge Regression\n\n\n\n\n\n\n3.4.2 The Lasso\n\n\n\n\n\n\n3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso\n\n\n\n\n\n\n3.4.4 Least Angle Regression\n\n\n\n\n\n\n3.5 Methods Using Derived Input Directions\n\n\n\n\n\n\n \n\n\n\n\n\n\nNo matching items\n\nChapter 4 hell\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n4.1 Introduction\n\n\n\n\n\n\n4.2 Linear Regression of an Indicator Matrix\n\n\n\n\n\n\n4.3 Linear Discriminant Analysis\n\n\n\n\n\n\n4.3.1 Regularized Discriminant Analysis\n\n\n\n\n\n\n4.3.2 Computations for LDA\n\n\n\n\n\n\n4.3.3 Reduced-Rank Linear Discriminant Analysis\n\n\n\n\n\n\n4.4 Logistic Regression\n\n\n\n\n\n\n4.4.1 Fitting Logistic Regression Models\n\n\n\n\n\n\n4.4.2 Example: South African Heart Disease\n\n\n\n\n\n\n4.4.3 Quadratic Approximations and Inference\n\n\n\n\n\n\n4.4.4 L1 Regularized Logistic Regression\n\n\n\n\n\n\nNo matching items"
  }
]