[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Engineer at Snap Inc, ex-Google."
  },
  {
    "objectID": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "href": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "title": "WIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes",
    "section": "",
    "text": "Notes for “High-Resolution Image Synthesis with Latent Diffusion Models”\nMy notes for the “High-Resolution Image Synthesis with Latent Diffusion Models” paper. Feel free to ask questions on my telegram channel"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html",
    "href": "posts/auto-encoding-variational-bayes/index.html",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "My notes for the “Auto-Encoding Variational Bayes” paper. Feel free to ask questions on my telegram channel\n\n\n\n\nLet us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable.\n\n\n\n\nWe use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#method",
    "href": "posts/auto-encoding-variational-bayes/index.html#method",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "Let us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable."
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "href": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "We use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/dot-product/index.html",
    "href": "posts/dot-product/index.html",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "The dot product (or scalar product) is a simple yet powerful operation that is used in many places in Machine Learning and other fields. In this post, I will explain the geometric intuition behind the dot product. You need to have a basic grasp of trigonometry and vector algebra to follow this post.\nI will need this explanation for my future posts.\nFeel free to ask questions on my telegram channel\n\n\n\nMore beginner friendly explanations are available in the following resources:\n\nen.wikipedia.org/wiki/Dot_product\nbrilliant.org/wiki/dot-product-definition\nbetterexplained.com/articles/vector-calculus-understanding-the-dot-product\n\nFeel free to checkout the above or other resources first.\nAssume we have two vectors \\(\\textbf{a}=[a_1, a_2, \\cdots, a_n]\\) and \\(\\textbf{b} = [b_1, b_2, \\cdots, b_n]\\), then there are two definitions of the dot product: algebraic and geometric.\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\sum_{i=1}^n{a_ib_i} = a_1b_1+a_2b_2+\\cdots+a_nb_n\n\\]\n\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is: \\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\theta\\) is the angle between a and b and \\(\\|\\textbf{a}\\|\\) is the magnitude of a vector a.\n\n\n\n\n\n\nGeometric properties\n\n\n\n\n\nThe geometric definition gives us a few useful properties:\n\nThe dot product is zero when a and b are orthogonal, since \\(\\cos(90 \\degree) = 0\\)\nThe dot product is positive for acute angles and negative for obtuse, e.g., \\(\\cos(45\\degree)\\) or \\(\\cos(89\\degree)\\) are positive but \\(\\cos(180\\degree), \\cos(91\\degree)\\) are negative.\nWe can find the angle between vectors by \\(\\theta = \\arccos(\\frac{\\textbf{a} \\cdot \\textbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|})\\). A picture from Wikipedia:\n\n\n\n\nSource Wikipedia\n\n\n\n\n\n\n\n\nWait, how are the two dot product definitions the same? How is \\(cos( \\theta)\\) related to such a straightforward sum of components? A few questions I asked when I first encountered the dot product. I accepted the fact and moved on with my life until today. I will try to understand myself and also explain by interactive visualizations.\n\n\n\n\nLet’s first simplify the problem as much as possible. Once we build up an intuition for simpler cases, we can come back and try to understand the general cases.\n\n\nFirst, we will only consider the \\(n=2\\) case. Assume that we have 2D vectors: \\(\\textbf{a}=[a_x, a_y]\\) and \\(\\textbf{b}=[b_x, b_y]\\) with the dot product:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\|\\textbf{a}\\|=\\sqrt{a_x^2+a_y^2}\\) is the length of the vector and \\(\\theta\\) is the angle between the vectors.\nSecond, scaling either \\(\\textbf{a}\\) or \\(\\textbf{b}\\) by a real number \\(k\\) does not invalidate the dot product equivalence. Let’s say we scale \\(\\textbf{b}\\) by \\(k\\), then:\n\\[\n\\textbf{a} \\cdot (k \\textbf{b}) = a_x(kb_x)+a_y(kb_y)=\\|\\textbf{a}\\|k\\|\\textbf{b}\\|\\cos \\theta\n\\]\nWith the above, we can simplify further by constraining \\(\\|\\textbf{b}\\| = 1\\). If the equivalence holds for \\(\\|\\textbf{b}\\| = 1\\), then it holds for any \\(\\|k\\textbf{b}\\|\\).\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: false,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board(\n  name, showSlider=true, sliderValue=0.0, \n  showP=true, showPLabel=true\n) {\n  function vec_length(a)  {\n    return Math.sqrt(a[0] * a[0] + a[1] * a[1]);\n  }\n  function vec_unit(a) {\n    const len = vec_length(a);\n    return vec_scale(a, 1 / len);\n  }\n  function vec_scale(a, scalar) {\n    return [a[0] * scalar, a[1] * scalar];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  function vec_cross(a, b) {\n    return a[0] * b[1] - a[1] * b[0];\n  }\n  function vec_mid(a, b) {\n    return vec_scale(vec_add(a, b), 0.5);\n  }\n  function vec_rot90(v) {\n    return [-v[1], v[0]];\n  }\n  function vec_add(a, b) {\n    return [a[0] + b[0], a[1] + b[1]];\n  }\n\n  function calcP(a, b) {\n    const unit_b = vec_unit(b);\n    return vec_scale(unit_b, vec_dot(a, unit_b));\n  }\n\n  function calcUnder(p, a, delta=0.05) {\n    var n = vec_scale(vec_rot90(p), delta / vec_length(p));\n    if (vec_cross(a, p) &lt; 0) {\n      n = vec_scale(n, -1);\n    }\n    return [n, vec_add(p, n)];\n  }\n\n  const a = [2.5, 2];\n  const b = [1, 0];\n  const p = [a[0], 0];\n  var board = init_board(name, 3.5);\n  const pointa = board.create(\"point\", a, {\n    fixed: false,\n    name: \"a\",\n    color: \"blue\"\n  })\n  const pointb = board.create(\"point\", b, {\n    fixed: true,\n    name: \"b\",\n    color: \"red\"\n  });\n  const pointp = board.create(\"point\", [function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[0];\n  }, function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[1];\n  }], {\n    visible: showP,\n    fixed: true,\n    name: \"p\",\n    color: \"green\",\n    size: 5,\n  });\n  const lineGreen = board.create(\"line\", [function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[0];\n  }, function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[1];\n  }], {\n    visible: showP,\n    lastArrow:true,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    strokeColor: \"green\",\n    strokeWidth: 2.5,\n    label:{offsets:[-1,1]}\n  });\n  const linea = board.create(\"line\", [[0, 0], pointa], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"blue\",\n    strokeWidth: 2\n  });\n  const lineb = board.create(\"line\", [[0, 0], pointb], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"red\",\n    strokeWidth: 2\n  });\n\n  const lineap = board.create(\"line\", [pointa, pointp], {\n    visible: showP,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    dash: 2,\n    strokeColor: \"black\",\n    strokeWidth: 1\n  })\n  const rightAngle = board.create(\"nonreflexangle\", [pointa, pointp, pointb], {\n    visible: showP,\n    name: \"\",\n    strokeColor: \"black\",\n    strokeWidth: 0.7,\n    fillColor: \"#00000000\",\n    radius: 0.3\n  });\n  const pointo = board.create(\"point\", [0, 0], {\n    visible: false,\n    fixed: true,\n    name: \"o\",\n  });\n  const alphaAngle = board.create(\"angle\", [pointb, pointo, pointa], {\n    name: \"θ\"\n  });\n \n  const slider = board.create('slider', [[-1, -2], [1, -2], [0, sliderValue, 360]], {\n    visible: showSlider,\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const text = board.create('text', [function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[0];\n  }, function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[1];\n  }, '||a||cos(θ)'], {\n    visible: showP && showPLabel,\n    anchorX: \"middle\",\n    anchorY: \"middle\",\n    // fontSize: 15,\n  })\n  const rot = board.create('transform', [function(){return slider.Value() / 180 * Math.PI;}, [0, 0]], {type:'rotate'});\n  rot.bindTo([pointb, pointa]);\n  return board;\n}\n\n\n\n\n\n\nA visualization for the setup we have so far:\n\nboard_div(\"board1\", 300)\n\n\n\n\n\n\n\nboard1 = make_board(\"board1\", /*showSlider=*/false, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe have the following items shown above:\n\nThe vector \\(\\textbf{a}\\): it is interactive and visualized with blue; feel free to move the point \\(\\textbf{a}\\) around.\nThe vector \\(\\textbf{b}\\): it is visualized with red and \\(\\|\\textbf{b}\\|=1\\).\n\\(\\theta\\): the angle from \\(\\textbf{b}\\) to \\(\\textbf{a}\\)\n\nThe geometric definition implies that the dot product is invariant under rotations. So, if we rotate the both vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) by the same angle \\(\\alpha\\), the dot product won’t change. It is not obvious from the algebraic definition, but a detailed proof is provided in the next section. Let’s trust this property for now until the next section, and simplify our problem for the third time.\nWe can rotate the both vectors such that \\(\\textbf{b}\\) is aligned with the x-axis. Click the button below for the illustration:\n\nviewof mybutton = {\n  const form = Inputs.button(\"Play to Rotate\", {value: null, reduce: () =&gt; {\n    var slider = board2.elementsByName[\"alpha\"];\n    function updateRotate() {\n      if (slider.Value() == 0) {\n        return;\n      }\n      slider.setValue(Math.max(0, slider.Value() - 1));\n      board2.update();\n      setTimeout(updateRotate, 12);\n    }\n    if (slider.Value() == 0) {\n      slider.setValue(60);\n    }\n    updateRotate();\n  }});\n  const scope = DOM.uid().id;\n  const cls = form.classList[0];\n  form.classList.add(scope);\n  form.append(html`&lt;style&gt;\n    .${cls} &gt; button { color: white }\n    &lt;/style&gt;\n  `);\n  return form;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", 300)\n\n\n\n\n\n\n\nboard2 = make_board(\"board2\", /*showSlider=*/true, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe arrived at a simplifed version of the problem: a vector \\(\\textbf{a}=[a_x, a_y]\\) and a fixed vector \\(\\textbf{b}=[1, 0]\\). Let’s check if the definitions agree now. The algebraic dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=a_x\n\\]\nand the geometric is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =  \\|\\textbf{a}\\|\\cos \\theta\n\\]\nnow, we have \\(\\textbf{a} \\cdot \\textbf{b} = a_x =  \\|\\textbf{a}\\|\\cos \\theta\\). This is correct due to the polar coordinates. Alternatively, we can easily derive from the cosine definition. The proof is left as an exercise to the reader. You can use the visualization below to convince yourself:\n\nboard_div(\"board3\", 400)\n\n\n\n\n\n\n\nboard3 = make_board(\"board3\", /*showSlider=*/false, /*sliderValue=*/0, /*showP=*/true)\n\n\n\n\n\n\nWe introduced a new vector \\(\\textbf{p}\\) in the above visualization:\n\nIt is visualized in green and visually represents the dot product\nIt has coordinates \\(\\textbf{p}=[\\|\\textbf{a}\\|\\cos \\theta, 0]=[a_x, 0]\\):\n\nWhere \\(p_x\\) is the dot product as we derived above\n\nThe vector \\(\\textbf{p}\\) is collinear with the vector \\(\\textbf{b}\\)\nSince the vectors \\(\\textbf{p}\\) and \\(\\textbf{b}\\) lie along the same line:\n\nWe can express \\(\\textbf{p}\\) using vector arithmetic: \\(\\textbf{p}=(\\|\\textbf{a}\\|\\cos \\theta)\\textbf{b}\\)\n\\(\\|\\textbf{a}\\|\\cos \\theta\\) is a real number, which means how many \\(\\textbf{b}\\)`s are required to reach the vector \\(\\textbf{p}\\)\nYou can visualize a 1D number line spanned by the vector \\(\\textbf{b}\\), then \\(\\|\\textbf{a}\\|\\cos \\theta\\) is the location of \\(\\textbf{p}\\) in that 1D coordinate system.\n\n\\(p_x\\) can be 0, positive or negative. When negative, the vector \\(\\textbf{p}\\) is opposite to the vector \\(\\textbf{b}\\)\nThe vector \\(\\textbf{p}\\) is actually the projection of the vector \\(\\textbf{a}\\) into \\(\\textbf{b}\\).\nPlay with the point \\(\\textbf{a}\\) to gain more intuition!\n\nTry different values of \\(\\textbf{a}\\) by moving it in the visualization, and verify the following questions:\n\nWhen the dot product is zero?\nWhen the dot product is negative ?\nWhen the dot product is positive ?\nWhat values of \\(\\textbf{a}\\) result in the same dot product?\n\nThe simplified form of the dot product is quite useful for intuition. I visualize this version in my mind when I forget some details of the dot product. At this point, you should be comfortable with the dot product of the simplified form: \\(\\textbf{a} \\cdot [1, 0]\\).\n\n\n\nFirst, let’s visualize the rotation:\n\nboard_div(\"board4\", 500)\n\n\n\n\n\n\n\nboard4 = make_board(\"board4\", /*showSlider=*/true, /*sliderValue=*/10, /*showP=*/true,  /*showP=*/false)\n\n\n\n\n\n\nThere is a slider which represents the angle \\(\\alpha\\) (in degrees) by which both vectors are rotated. Feel free to play with the slider! You can also interact with the point \\(\\textbf{a}\\).\nIt is easy to see why the dot product is invariant under rotations from the geometric definition. The geometric definition relies only on the lengths of the vectors, and the lengths don’t change when rotated.\nHowever, it is not immediately clear from the algebraic definition. To prove it, we derive an alternative formula for the algebraic dot product:\n\\[\n\\begin{align}\n\\|\\textbf{a}-\\textbf{b}\\|^2 &= (a_x-b_x)^2 + (a_y-b_y)^2 \\\\\n&= a_x^2 + b_x^2 - 2a_xb_x + a_y^2 + b_y^2 - 2a_yb_y \\\\\n&= (a_x^2 + a_y^2) + (b_x^2 + b_y^2) - 2(a_xb_x + a_yb_y) \\\\\n&= \\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - 2 \\textbf{a} \\cdot \\textbf{b}\n\\end{align}\n\\]\nwhich gives us:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= a_xb_x + a_yb_y \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nThe dot product above is derived purely from the lengths. Since the lengths remain unchanged under rotations, the algebraic dot product remains unchanged as well, which completes our proof.\nNote that we derived the law of cosine, which is quite cool!\n\n\n\nIt’s not hard to extend the 2D version of the proof to n-dimensional vectors, since we didn’t rely on any properties unique to 2D. The shortest way is:\n\nNote that the dot product of n-dimensional vectors is invariant under rotations. The same proof as in the rotational invariance\nRotate the vectors such that they lie in the xy-plane\nNow, we reduced n-dimensional vectors to 2D vectors, which we already know how to prove\n\nI will provide more alternative proofs below, mostly for myself. They’re optional and collapsed, feel free to read them.\n\n\n\n\n\n\nAlternative Proof 1\n\n\n\n\n\nWe only scaled the vector \\(\\textbf{b}\\) but we can also scale the vector \\(\\textbf{a}\\), so that \\(\\|\\textbf{a}\\|=1\\). Also switch to the polar coordinates: \\(\\textbf{a}=[\\cos \\alpha, \\sin \\alpha]\\) and \\(\\textbf{b}=[\\cos \\beta, \\sin \\beta]\\).\nThen, the geometric dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos(\\alpha - \\beta)\n\\]\nThe algebraic is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\nWe arrive at the cosine subtraction rule: \\[\n\\cos(\\alpha-\\beta)=\\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\n\n\n\n\n\n\n\n\n\nAlternative Proof 2\n\n\n\n\n\nWe will extend the formula from the rotational invariance section further. We derived the following dot product formula:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= \\sum_{i=1}^n a_ib_i \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nFrom the law of cosine, we also have:\n\\[\n\\|\\textbf{a}-\\textbf{b}\\|=\\|\\textbf{a}\\|^2+\\|\\textbf{b}\\|^2-2\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nif we rearrange and combine the above two equations: \\[\n\\begin{align}\n\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =& \\textbf{a} \\cdot \\textbf{b}= \\sum_{i=1}^n a_ib_i  \\\\\n=& \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nAlternative Proof 3\n\n\n\n\n\nWe can use the Pythagorean theorem:\n\\[\n\\begin{align}\n\\|\\textbf{a}\\|^2&=\\|\\textbf{p}\\|^2 + \\|\\textbf{a}-\\textbf{p}\\|^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x-p_x)^2 + (a_y-p_y)^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x^2+a_y^2) + (p_x^2+p_y^2) - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2\\textbf{a}\\cdot\\textbf{p} \\\\\n\\end{align}\n\\]\nThis gives us the following:\n\\[\na_xp_x+a_yp_y=\\textbf{a}\\cdot\\textbf{p}=\\|\\textbf{p}\\|^2\n\\]\nif we substitute \\(\\textbf{p}\\) by \\((\\|\\textbf{a}\\|\\cos\\theta) \\textbf{b}\\):\n\\[\n\\|\\textbf{a}\\|\\cos\\theta(a_xb_x+a_yb_y)=\\|\\textbf{a}\\|\\cos\\theta(\\textbf{a}\\cdot \\textbf{b})=(\\|\\textbf{a}\\|\\cos\\theta)^2\n\\]\nwhich gives back the both geometric and algebraic dot product.\n\n\n\n\n\n\n\n\n\nAlternative Proof 4\n\n\n\n\n\nYou can also find more proofs in proofwiki.\n\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/dot-product/index.html#introduction",
    "href": "posts/dot-product/index.html#introduction",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "The dot product (or scalar product) is a simple yet powerful operation that is used in many places in Machine Learning and other fields. In this post, I will explain the geometric intuition behind the dot product. You need to have a basic grasp of trigonometry and vector algebra to follow this post.\nI will need this explanation for my future posts.\nFeel free to ask questions on my telegram channel"
  },
  {
    "objectID": "posts/dot-product/index.html#recap",
    "href": "posts/dot-product/index.html#recap",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "More beginner friendly explanations are available in the following resources:\n\nen.wikipedia.org/wiki/Dot_product\nbrilliant.org/wiki/dot-product-definition\nbetterexplained.com/articles/vector-calculus-understanding-the-dot-product\n\nFeel free to checkout the above or other resources first.\nAssume we have two vectors \\(\\textbf{a}=[a_1, a_2, \\cdots, a_n]\\) and \\(\\textbf{b} = [b_1, b_2, \\cdots, b_n]\\), then there are two definitions of the dot product: algebraic and geometric.\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\sum_{i=1}^n{a_ib_i} = a_1b_1+a_2b_2+\\cdots+a_nb_n\n\\]\n\n\n\nThe dot product of \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is: \\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\theta\\) is the angle between a and b and \\(\\|\\textbf{a}\\|\\) is the magnitude of a vector a.\n\n\n\n\n\n\nGeometric properties\n\n\n\n\n\nThe geometric definition gives us a few useful properties:\n\nThe dot product is zero when a and b are orthogonal, since \\(\\cos(90 \\degree) = 0\\)\nThe dot product is positive for acute angles and negative for obtuse, e.g., \\(\\cos(45\\degree)\\) or \\(\\cos(89\\degree)\\) are positive but \\(\\cos(180\\degree), \\cos(91\\degree)\\) are negative.\nWe can find the angle between vectors by \\(\\theta = \\arccos(\\frac{\\textbf{a} \\cdot \\textbf{b}}{\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|})\\). A picture from Wikipedia:\n\n\n\n\nSource Wikipedia\n\n\n\n\n\n\n\n\nWait, how are the two dot product definitions the same? How is \\(cos( \\theta)\\) related to such a straightforward sum of components? A few questions I asked when I first encountered the dot product. I accepted the fact and moved on with my life until today. I will try to understand myself and also explain by interactive visualizations."
  },
  {
    "objectID": "posts/dot-product/index.html#explanation",
    "href": "posts/dot-product/index.html#explanation",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "Let’s first simplify the problem as much as possible. Once we build up an intuition for simpler cases, we can come back and try to understand the general cases.\n\n\nFirst, we will only consider the \\(n=2\\) case. Assume that we have 2D vectors: \\(\\textbf{a}=[a_x, a_y]\\) and \\(\\textbf{b}=[b_x, b_y]\\) with the dot product:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nwhere \\(\\|\\textbf{a}\\|=\\sqrt{a_x^2+a_y^2}\\) is the length of the vector and \\(\\theta\\) is the angle between the vectors.\nSecond, scaling either \\(\\textbf{a}\\) or \\(\\textbf{b}\\) by a real number \\(k\\) does not invalidate the dot product equivalence. Let’s say we scale \\(\\textbf{b}\\) by \\(k\\), then:\n\\[\n\\textbf{a} \\cdot (k \\textbf{b}) = a_x(kb_x)+a_y(kb_y)=\\|\\textbf{a}\\|k\\|\\textbf{b}\\|\\cos \\theta\n\\]\nWith the above, we can simplify further by constraining \\(\\|\\textbf{b}\\| = 1\\). If the equivalence holds for \\(\\|\\textbf{b}\\| = 1\\), then it holds for any \\(\\|k\\textbf{b}\\|\\).\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: false,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board(\n  name, showSlider=true, sliderValue=0.0, \n  showP=true, showPLabel=true\n) {\n  function vec_length(a)  {\n    return Math.sqrt(a[0] * a[0] + a[1] * a[1]);\n  }\n  function vec_unit(a) {\n    const len = vec_length(a);\n    return vec_scale(a, 1 / len);\n  }\n  function vec_scale(a, scalar) {\n    return [a[0] * scalar, a[1] * scalar];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  function vec_cross(a, b) {\n    return a[0] * b[1] - a[1] * b[0];\n  }\n  function vec_mid(a, b) {\n    return vec_scale(vec_add(a, b), 0.5);\n  }\n  function vec_rot90(v) {\n    return [-v[1], v[0]];\n  }\n  function vec_add(a, b) {\n    return [a[0] + b[0], a[1] + b[1]];\n  }\n\n  function calcP(a, b) {\n    const unit_b = vec_unit(b);\n    return vec_scale(unit_b, vec_dot(a, unit_b));\n  }\n\n  function calcUnder(p, a, delta=0.05) {\n    var n = vec_scale(vec_rot90(p), delta / vec_length(p));\n    if (vec_cross(a, p) &lt; 0) {\n      n = vec_scale(n, -1);\n    }\n    return [n, vec_add(p, n)];\n  }\n\n  const a = [2.5, 2];\n  const b = [1, 0];\n  const p = [a[0], 0];\n  var board = init_board(name, 3.5);\n  const pointa = board.create(\"point\", a, {\n    fixed: false,\n    name: \"a\",\n    color: \"blue\"\n  })\n  const pointb = board.create(\"point\", b, {\n    fixed: true,\n    name: \"b\",\n    color: \"red\"\n  });\n  const pointp = board.create(\"point\", [function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[0];\n  }, function() {\n    return calcP([pointa.X(), pointa.Y()], [pointb.X(), pointb.Y()])[1];\n  }], {\n    visible: showP,\n    fixed: true,\n    name: \"p\",\n    color: \"green\",\n    size: 5,\n  });\n  const lineGreen = board.create(\"line\", [function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[0];\n  }, function() {\n    return calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.03)[1];\n  }], {\n    visible: showP,\n    lastArrow:true,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    strokeColor: \"green\",\n    strokeWidth: 2.5,\n    label:{offsets:[-1,1]}\n  });\n  const linea = board.create(\"line\", [[0, 0], pointa], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"blue\",\n    strokeWidth: 2\n  });\n  const lineb = board.create(\"line\", [[0, 0], pointb], {\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n    strokeColor: \"red\",\n    strokeWidth: 2\n  });\n\n  const lineap = board.create(\"line\", [pointa, pointp], {\n    visible: showP,\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    dash: 2,\n    strokeColor: \"black\",\n    strokeWidth: 1\n  })\n  const rightAngle = board.create(\"nonreflexangle\", [pointa, pointp, pointb], {\n    visible: showP,\n    name: \"\",\n    strokeColor: \"black\",\n    strokeWidth: 0.7,\n    fillColor: \"#00000000\",\n    radius: 0.3\n  });\n  const pointo = board.create(\"point\", [0, 0], {\n    visible: false,\n    fixed: true,\n    name: \"o\",\n  });\n  const alphaAngle = board.create(\"angle\", [pointb, pointo, pointa], {\n    name: \"θ\"\n  });\n \n  const slider = board.create('slider', [[-1, -2], [1, -2], [0, sliderValue, 360]], {\n    visible: showSlider,\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const text = board.create('text', [function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[0];\n  }, function() {\n    const s = calcUnder([pointp.X(), pointp.Y()], [pointa.X(), pointa.Y()], 0.4);\n    return vec_mid(s[0], s[1])[1];\n  }, '||a||cos(θ)'], {\n    visible: showP && showPLabel,\n    anchorX: \"middle\",\n    anchorY: \"middle\",\n    // fontSize: 15,\n  })\n  const rot = board.create('transform', [function(){return slider.Value() / 180 * Math.PI;}, [0, 0]], {type:'rotate'});\n  rot.bindTo([pointb, pointa]);\n  return board;\n}\n\n\n\n\n\n\nA visualization for the setup we have so far:\n\nboard_div(\"board1\", 300)\n\n\n\n\n\n\n\nboard1 = make_board(\"board1\", /*showSlider=*/false, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe have the following items shown above:\n\nThe vector \\(\\textbf{a}\\): it is interactive and visualized with blue; feel free to move the point \\(\\textbf{a}\\) around.\nThe vector \\(\\textbf{b}\\): it is visualized with red and \\(\\|\\textbf{b}\\|=1\\).\n\\(\\theta\\): the angle from \\(\\textbf{b}\\) to \\(\\textbf{a}\\)\n\nThe geometric definition implies that the dot product is invariant under rotations. So, if we rotate the both vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) by the same angle \\(\\alpha\\), the dot product won’t change. It is not obvious from the algebraic definition, but a detailed proof is provided in the next section. Let’s trust this property for now until the next section, and simplify our problem for the third time.\nWe can rotate the both vectors such that \\(\\textbf{b}\\) is aligned with the x-axis. Click the button below for the illustration:\n\nviewof mybutton = {\n  const form = Inputs.button(\"Play to Rotate\", {value: null, reduce: () =&gt; {\n    var slider = board2.elementsByName[\"alpha\"];\n    function updateRotate() {\n      if (slider.Value() == 0) {\n        return;\n      }\n      slider.setValue(Math.max(0, slider.Value() - 1));\n      board2.update();\n      setTimeout(updateRotate, 12);\n    }\n    if (slider.Value() == 0) {\n      slider.setValue(60);\n    }\n    updateRotate();\n  }});\n  const scope = DOM.uid().id;\n  const cls = form.classList[0];\n  form.classList.add(scope);\n  form.append(html`&lt;style&gt;\n    .${cls} &gt; button { color: white }\n    &lt;/style&gt;\n  `);\n  return form;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", 300)\n\n\n\n\n\n\n\nboard2 = make_board(\"board2\", /*showSlider=*/true, /*sliderValue=*/30, /*showP=*/false)\n\n\n\n\n\n\nWe arrived at a simplifed version of the problem: a vector \\(\\textbf{a}=[a_x, a_y]\\) and a fixed vector \\(\\textbf{b}=[1, 0]\\). Let’s check if the definitions agree now. The algebraic dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = a_xb_x+a_yb_y=a_x\n\\]\nand the geometric is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =  \\|\\textbf{a}\\|\\cos \\theta\n\\]\nnow, we have \\(\\textbf{a} \\cdot \\textbf{b} = a_x =  \\|\\textbf{a}\\|\\cos \\theta\\). This is correct due to the polar coordinates. Alternatively, we can easily derive from the cosine definition. The proof is left as an exercise to the reader. You can use the visualization below to convince yourself:\n\nboard_div(\"board3\", 400)\n\n\n\n\n\n\n\nboard3 = make_board(\"board3\", /*showSlider=*/false, /*sliderValue=*/0, /*showP=*/true)\n\n\n\n\n\n\nWe introduced a new vector \\(\\textbf{p}\\) in the above visualization:\n\nIt is visualized in green and visually represents the dot product\nIt has coordinates \\(\\textbf{p}=[\\|\\textbf{a}\\|\\cos \\theta, 0]=[a_x, 0]\\):\n\nWhere \\(p_x\\) is the dot product as we derived above\n\nThe vector \\(\\textbf{p}\\) is collinear with the vector \\(\\textbf{b}\\)\nSince the vectors \\(\\textbf{p}\\) and \\(\\textbf{b}\\) lie along the same line:\n\nWe can express \\(\\textbf{p}\\) using vector arithmetic: \\(\\textbf{p}=(\\|\\textbf{a}\\|\\cos \\theta)\\textbf{b}\\)\n\\(\\|\\textbf{a}\\|\\cos \\theta\\) is a real number, which means how many \\(\\textbf{b}\\)`s are required to reach the vector \\(\\textbf{p}\\)\nYou can visualize a 1D number line spanned by the vector \\(\\textbf{b}\\), then \\(\\|\\textbf{a}\\|\\cos \\theta\\) is the location of \\(\\textbf{p}\\) in that 1D coordinate system.\n\n\\(p_x\\) can be 0, positive or negative. When negative, the vector \\(\\textbf{p}\\) is opposite to the vector \\(\\textbf{b}\\)\nThe vector \\(\\textbf{p}\\) is actually the projection of the vector \\(\\textbf{a}\\) into \\(\\textbf{b}\\).\nPlay with the point \\(\\textbf{a}\\) to gain more intuition!\n\nTry different values of \\(\\textbf{a}\\) by moving it in the visualization, and verify the following questions:\n\nWhen the dot product is zero?\nWhen the dot product is negative ?\nWhen the dot product is positive ?\nWhat values of \\(\\textbf{a}\\) result in the same dot product?\n\nThe simplified form of the dot product is quite useful for intuition. I visualize this version in my mind when I forget some details of the dot product. At this point, you should be comfortable with the dot product of the simplified form: \\(\\textbf{a} \\cdot [1, 0]\\).\n\n\n\nFirst, let’s visualize the rotation:\n\nboard_div(\"board4\", 500)\n\n\n\n\n\n\n\nboard4 = make_board(\"board4\", /*showSlider=*/true, /*sliderValue=*/10, /*showP=*/true,  /*showP=*/false)\n\n\n\n\n\n\nThere is a slider which represents the angle \\(\\alpha\\) (in degrees) by which both vectors are rotated. Feel free to play with the slider! You can also interact with the point \\(\\textbf{a}\\).\nIt is easy to see why the dot product is invariant under rotations from the geometric definition. The geometric definition relies only on the lengths of the vectors, and the lengths don’t change when rotated.\nHowever, it is not immediately clear from the algebraic definition. To prove it, we derive an alternative formula for the algebraic dot product:\n\\[\n\\begin{align}\n\\|\\textbf{a}-\\textbf{b}\\|^2 &= (a_x-b_x)^2 + (a_y-b_y)^2 \\\\\n&= a_x^2 + b_x^2 - 2a_xb_x + a_y^2 + b_y^2 - 2a_yb_y \\\\\n&= (a_x^2 + a_y^2) + (b_x^2 + b_y^2) - 2(a_xb_x + a_yb_y) \\\\\n&= \\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - 2 \\textbf{a} \\cdot \\textbf{b}\n\\end{align}\n\\]\nwhich gives us:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= a_xb_x + a_yb_y \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nThe dot product above is derived purely from the lengths. Since the lengths remain unchanged under rotations, the algebraic dot product remains unchanged as well, which completes our proof.\nNote that we derived the law of cosine, which is quite cool!\n\n\n\nIt’s not hard to extend the 2D version of the proof to n-dimensional vectors, since we didn’t rely on any properties unique to 2D. The shortest way is:\n\nNote that the dot product of n-dimensional vectors is invariant under rotations. The same proof as in the rotational invariance\nRotate the vectors such that they lie in the xy-plane\nNow, we reduced n-dimensional vectors to 2D vectors, which we already know how to prove\n\nI will provide more alternative proofs below, mostly for myself. They’re optional and collapsed, feel free to read them.\n\n\n\n\n\n\nAlternative Proof 1\n\n\n\n\n\nWe only scaled the vector \\(\\textbf{b}\\) but we can also scale the vector \\(\\textbf{a}\\), so that \\(\\|\\textbf{a}\\|=1\\). Also switch to the polar coordinates: \\(\\textbf{a}=[\\cos \\alpha, \\sin \\alpha]\\) and \\(\\textbf{b}=[\\cos \\beta, \\sin \\beta]\\).\nThen, the geometric dot product is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos(\\alpha - \\beta)\n\\]\nThe algebraic is:\n\\[\n\\textbf{a} \\cdot \\textbf{b} = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\nWe arrive at the cosine subtraction rule: \\[\n\\cos(\\alpha-\\beta)=\\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta\n\\]\n\n\n\n\n\n\n\n\n\nAlternative Proof 2\n\n\n\n\n\nWe will extend the formula from the rotational invariance section further. We derived the following dot product formula:\n\\[\n\\begin{align}\n\\textbf{a} \\cdot \\textbf{b} &= \\sum_{i=1}^n a_ib_i \\\\\n&= \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\nFrom the law of cosine, we also have:\n\\[\n\\|\\textbf{a}-\\textbf{b}\\|=\\|\\textbf{a}\\|^2+\\|\\textbf{b}\\|^2-2\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta\n\\]\nif we rearrange and combine the above two equations: \\[\n\\begin{align}\n\\|\\textbf{a}\\|\\|\\textbf{b}\\|\\cos \\theta =& \\textbf{a} \\cdot \\textbf{b}= \\sum_{i=1}^n a_ib_i  \\\\\n=& \\frac{\\|\\textbf{a}\\|^2 + \\|\\textbf{b}\\|^2 - \\|\\textbf{a}-\\textbf{b}\\|^2 }{2}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nAlternative Proof 3\n\n\n\n\n\nWe can use the Pythagorean theorem:\n\\[\n\\begin{align}\n\\|\\textbf{a}\\|^2&=\\|\\textbf{p}\\|^2 + \\|\\textbf{a}-\\textbf{p}\\|^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x-p_x)^2 + (a_y-p_y)^2 \\\\\n&= \\|\\textbf{p}\\|^2 + (a_x^2+a_y^2) + (p_x^2+p_y^2) - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2(a_xp_x + a_yp_y) \\\\\n&= \\|\\textbf{p}\\|^2 + \\|\\textbf{a}\\|^2 + \\|\\textbf{p}\\|^2 - 2\\textbf{a}\\cdot\\textbf{p} \\\\\n\\end{align}\n\\]\nThis gives us the following:\n\\[\na_xp_x+a_yp_y=\\textbf{a}\\cdot\\textbf{p}=\\|\\textbf{p}\\|^2\n\\]\nif we substitute \\(\\textbf{p}\\) by \\((\\|\\textbf{a}\\|\\cos\\theta) \\textbf{b}\\):\n\\[\n\\|\\textbf{a}\\|\\cos\\theta(a_xb_x+a_yb_y)=\\|\\textbf{a}\\|\\cos\\theta(\\textbf{a}\\cdot \\textbf{b})=(\\|\\textbf{a}\\|\\cos\\theta)^2\n\\]\nwhich gives back the both geometric and algebraic dot product.\n\n\n\n\n\n\n\n\n\nAlternative Proof 4\n\n\n\n\n\nYou can also find more proofs in proofwiki."
  },
  {
    "objectID": "posts/dot-product/index.html#the-end",
    "href": "posts/dot-product/index.html#the-end",
    "title": "Dot Product Intuition",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/jensens-inequality/index.html",
    "href": "posts/jensens-inequality/index.html",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is fundamental in many fields, including machine learning and statistics. For example, it is useful in the diffusion models paper for understanding the variational lower bound. In this post, I will give a simple geometric intuition for Jensen’s inequality.\nFeel free to leave feedback on my telegram channel.\n\n\n\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\n\n\ncode for fig_jensen_inequality\nimport itertools\nfrom typing import Optional\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n\ndef alpha_profiles(n: int):\n    if n == 2:\n        space = np.linspace(0.01, 0.99, 100)\n        return np.column_stack((space, 1.0 - space))\n    space = np.linspace(0.01, 0.99, 15 - max(0, (n - 3) * 5))\n    space_prod = itertools.product(*[space for _ in range(n - 1)])\n    profiles = np.array(list(space_prod))\n    profiles = profiles[np.sum(profiles, axis=1) &lt; 1.0]\n    return np.concatenate([profiles, 1 - np.sum(profiles, axis=1).reshape(-1, 1)], axis=1)\n\n\ndef fig_jensen_inequality(f, x_range: list, x: np.array, y_range: Optional[list] = None):\n    x_linspace = np.linspace(x_range[0], x_range[1], 100)\n    points = np.column_stack([x, f(x)])\n    n = len(points)\n    steps = []\n    hull_points = []\n    titles = []\n    for index, alphas in enumerate(alpha_profiles(n)):\n        hp = np.average(points, weights=alphas, axis=0)\n        hull_points.append(hp)\n        title = \",\".join([\"\\\\lambda_\" + f\"{i + 1}={a:.2f}\" for i, a in enumerate(alphas)])\n        title = f\"${title}$\"\n        titles.append(title)\n        step = dict(name=index, label=index, method=\"update\",\n                    args=[{\n                        \"x\": [[hp[0], hp[0]], [hp[0]], [hp[0]]],\n                        \"y\": [[f(hp[0]), hp[1]], [hp[1]], [f(hp[0])]],\n                    }, {\"title\": title}, [2, 3, 4]])\n        steps.append(step)\n    active_index = len(steps) // 2\n    sliders = [dict(active=len(steps) // 2, steps=steps)]\n    return go.Figure(data=[\n        go.Scatter(\n            name=\"f\", x=x_linspace, y=f(x_linspace), hoverinfo=\"none\"\n        ),\n        go.Scatter(\n            name=\"Convex Hull\", x=np.append(points[:, 0], points[0][0]),\n            y=np.append(points[:, 1], points[0][1]),\n            fillcolor=\"rgba(239, 85, 59, 0.2)\", fill=\"toself\", mode=\"lines\",\n            line=dict(width=3), hoverinfo=\"none\",\n            showlegend=False\n        ),\n        go.Scatter(\n            x=[hull_points[active_index][0], hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0]), hull_points[active_index][1]],\n            mode=\"lines\",\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            line={\"color\": \"black\", \"dash\": \"dot\", \"width\": 1},\n            showlegend=False\n        ),\n        go.Scatter(\n            name=\"A\",\n            x=[hull_points[active_index][0]],\n            y=[hull_points[active_index][1]],\n            mode=f\"markers+text\",\n            text=[\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\"],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"black\"},\n        ),\n        go.Scatter(\n            name=\"B\",\n            x=[hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0])],\n            mode=f\"markers\",\n            text=[\"B\"],\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"#00CC96\"},\n        ),\n        go.Scatter(\n            name=\"$(x_i, f(x_i))$\",\n            x=points[:, 0], y=points[:, 1],\n            mode=\"markers+text\",\n            marker={\"size\": 20, \"color\": \"#ffa15a\"},\n            line={\"color\": \"rgba(239, 85, 59, 0.2)\"},\n            text=[f\"$(x_{i},f(x_{i}))$\" for i in range(1, n + 1)],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            showlegend=False,\n        ),\n    ], layout=go.Layout(\n        title=titles[active_index],\n        xaxis=dict(fixedrange=True, range=x_range),\n        yaxis=dict(fixedrange=True, scaleanchor=\"x\", scaleratio=1, range=y_range),\n        sliders=sliders,\n        legend=dict(\n            yanchor=\"top\",\n            xanchor=\"right\",\n            x=1,\n            y=1\n        ),\n        margin=dict(l=5, r=5, t=60, b=20)\n    ))\n\n\ndef sample_parabola(x):\n    return 0.15 * (x - 15) ** 2 + 15\n\n\n\n\n\ncode for show_sample_jensen_inequality\ndef show_sample_jensen_inequality(x: list):\n    fig = fig_jensen_inequality(\n        f=sample_parabola,\n        x_range=[0, 30],\n        x=np.array(x),\n        y_range=[12, 48]\n    )\n    return fig.show(renderer=\"iframe\")\n\n\n\n\n\nA function is a convex function when the line segment joining any two points on the function graph lies above or on the graph. In the simplest term, a convex function is shaped like \\(\\cup\\) and a concave function is shaped like \\(\\cap\\). If f is convex, then -f is concave.\nA visualization from Wikipedia:\n\n\ndisplay image from Wikipedia\nfrom IPython.display import Image\nImage(url='https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg', width=400)\n\n\n\n\n\n\n\nA function \\(f: X \\rightarrow \\mathbb{R}\\) is called convex if the following holds:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nand concave when:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nfor all \\(0 \\le \\lambda \\le 1\\) and for all \\(x_1, x_2 \\in X\\).\nWe will give geometric intuition for this definition in the next section.\n\n\n\n\nshow_sample_jensen_inequality(x=[2, 22])\n\n\n\n\nAn interactive visualization of the convex function: \\(f(x)=0.15(x - 15)^2 + 15\\). We will use the same parabola during this post unless stated otherwise. You can use the slider to try different values of (\\(\\lambda_1\\), \\(\\lambda_2)\\), where \\(\\lambda_1=\\lambda\\) (from the definition) and \\(\\lambda_2=1-\\lambda\\).\nWe have a line segment that joins \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\). We can sample any point along the line segment with \\((\\lambda_1 x_1 + \\lambda_2 x_2, \\lambda_1 f(x_1) + \\lambda_2 f(x_2))\\) where \\(0 \\le \\lambda_1 \\le 1\\) and \\(\\lambda_2 = 1 - \\lambda_1\\). For example:\n\nWhen \\(\\lambda_1=1\\), we get the first point\nWhen \\(\\lambda_1=0\\), we get the second point\nAnd when \\(\\lambda_1=0.5\\), we get the middle point of the line segment\nand so on… Try the slider above and notice how \\(\\lambda_1\\) and \\(\\lambda_2\\) are changing!\n\nThis point is visualized with a black point above. Let’s name it as A.\nThe light green point where the function graph intersects with the dotted line segment is represented by: \\((\\lambda_1 x_1 + \\lambda_2 x_2, f(\\lambda_1 x_1 + \\lambda_2 x_2))\\). Let’s name it as B.\nThen, the definition above is just asserting that \\(B_y \\le A_y\\) and we also have \\(A_x = B_x\\). Note that we are only showing a single line segment, but this statement should be true for all similar line segments and for all \\(0 \\le \\lambda_1 \\le 1\\).\n\n\n\n\nJensen’s inequality is a generalization of the above convex function definition for more than 2 points.\n\n\nAssume we have a convex function \\(f\\) and \\(x_1, x_2, \\cdots, x_n\\) in \\(f\\)’s domain, and also positive weights \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\) where \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then Jensen’s inequality can be stated as:\n\\[\nf(\\sum_{i=1}^n \\lambda_i x_i) \\le \\sum_{i=1}^n \\lambda_i f(x_i)\n\\]\n\n\n\n\n\n\nConcave Function\n\n\n\n\n\nThe equation is flipped for a concave function g:\n\\[\ng(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\sum_{i=1}^n \\lambda_i g(x_i)\n\\]\n\n\n\nNote that we arrive at the same definition for convex function when \\(n=2\\).\n\n\n\nA numerous proofs are already available by other posts. I encourage you to checkout the following resources:\n\nen.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\nbrilliant.org/wiki/jensens-inequality\nartofproblemsolving.com/wiki/…\nPhysics intuition (1993)\n\nHere I describe a geometric intuition, which resonates more with me.\n\n\nLet’s start with a triangle, i.e., \\(n=3\\):\n\nshow_sample_jensen_inequality(x=[2, 12, 27])\n\n\n\n\nAs before, you can use the slider to try different values of \\((\\lambda_1, \\lambda_2, \\lambda_3)\\) where \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\).\nWe have a triangle that connects the points: \\((x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3))\\).\nIn the \\(n=2\\) case, we used \\(\\lambda_1\\) and \\(\\lambda_2\\) to sample a point along the line segment. In this case, it is similar, but we can sample any point inside or on the boundaries of the triangle with:\n\\[\n\\left(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, \\lambda_1f(x_1)+\\lambda_2f(x_2)+\\lambda_3f(x_3)\\right)\n\\]\nwhere \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\). For example:\n\nWhen \\(\\lambda_i=1\\) where \\(i \\in \\{1, 2, 3\\}\\), we get the point \\((x_i, f(x_i))\\)\nWhen \\(\\lambda_1=\\lambda_2=\\lambda_3=\\frac{1}{3}\\), we get the center of mass of the triangle\n\nThe black point (named A) in the visualization represents this point.\nNote that (\\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\lambda_3\\)) describes the barycentric coordinate system. You don’t need to know it in this post, just sharing in case you’re already familiar with it.\nThe light green point where the parabola meets the dotted line segment is represented by:\n\\[\n(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, f(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3))\n\\]\nIf we name this point as B, then it is not difficult to see that Jensen’s inequality is the same as \\(B_y \\le A_y\\).\n\n\n\nIt is easy to generalize for \\(n&gt;3\\). I am adding it here for the sake of completeness:\n\nshow_sample_jensen_inequality(x=[2, 13, 22, 25])\n\n\n\n\nIn the general case, \\((\\sum_{i=1}^n \\lambda_ix_i, \\sum_{i=1}^n \\lambda_if(x_i))\\) describes a point inside or on the boundary of the convex hull enclosing the points: \\((x_1, f(x_1)), (x_2, f(x_2)), \\cdots, (x_n, f(x_n))\\). The convex hull is always above or on the convex function graph, which is why Jensen’s inequality holds true.\nA few closing notes:\n\nThe convex hull may have any number of points, including n → ∞\nWe closely approximate the convex function in some interval with the convex hull as n approaches infinity\nThe convexity definitions for functions and polygons are the same once we have enough points, i.e., n → ∞\nJensen’s inequality is useful in a probability theory setting, since \\(\\sum_{i=1}^n \\lambda_i = 1\\), including the continuous form with n → ∞.\n\n\n\n\n\n\n\n\n\n\n\n\nAM–GM inequality\n\n\n\n\n\nThe arithmetic mean-geometric mean inequality (AM-GM inequality) states that: \\[\n\\frac{x_1+x_2+\\cdots+x_n}{n} \\ge \\sqrt[n]{x_1x_2\\cdots x_n}\n\\]\nLet’s prove with Jensen’s inequality by rewriting the above with \\(\\lambda_1=\\lambda_2=\\cdots=\\lambda_n=\\frac{1}{n}\\):\n\\[\n\\sum_{i=1}^n \\lambda_i x_i \\ge \\prod_{i=1}^n x_i^{\\lambda_i}\n\\]\nSince log is a concave and monotonic function, we can apply log to both sides.\n\\[\n\\log(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\log(\\prod_{i=1}^n x_i^{\\lambda_i}) = \\sum_{i=1}^n \\lambda_i \\log(x_i)\n\\]\nThe above equation is valid due to Jensen’s inequality. Note that the same proof works for the weighted version since the proof does not rely on the fact that \\(\\lambda_i=\\frac{1}{n}\\) for all \\(i=1,2,\\cdots,n\\).\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/jensens-inequality/index.html#introduction",
    "href": "posts/jensens-inequality/index.html#introduction",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is fundamental in many fields, including machine learning and statistics. For example, it is useful in the diffusion models paper for understanding the variational lower bound. In this post, I will give a simple geometric intuition for Jensen’s inequality.\nFeel free to leave feedback on my telegram channel."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#setup",
    "href": "posts/jensens-inequality/index.html#setup",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "The post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\n\n\ncode for fig_jensen_inequality\nimport itertools\nfrom typing import Optional\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n\ndef alpha_profiles(n: int):\n    if n == 2:\n        space = np.linspace(0.01, 0.99, 100)\n        return np.column_stack((space, 1.0 - space))\n    space = np.linspace(0.01, 0.99, 15 - max(0, (n - 3) * 5))\n    space_prod = itertools.product(*[space for _ in range(n - 1)])\n    profiles = np.array(list(space_prod))\n    profiles = profiles[np.sum(profiles, axis=1) &lt; 1.0]\n    return np.concatenate([profiles, 1 - np.sum(profiles, axis=1).reshape(-1, 1)], axis=1)\n\n\ndef fig_jensen_inequality(f, x_range: list, x: np.array, y_range: Optional[list] = None):\n    x_linspace = np.linspace(x_range[0], x_range[1], 100)\n    points = np.column_stack([x, f(x)])\n    n = len(points)\n    steps = []\n    hull_points = []\n    titles = []\n    for index, alphas in enumerate(alpha_profiles(n)):\n        hp = np.average(points, weights=alphas, axis=0)\n        hull_points.append(hp)\n        title = \",\".join([\"\\\\lambda_\" + f\"{i + 1}={a:.2f}\" for i, a in enumerate(alphas)])\n        title = f\"${title}$\"\n        titles.append(title)\n        step = dict(name=index, label=index, method=\"update\",\n                    args=[{\n                        \"x\": [[hp[0], hp[0]], [hp[0]], [hp[0]]],\n                        \"y\": [[f(hp[0]), hp[1]], [hp[1]], [f(hp[0])]],\n                    }, {\"title\": title}, [2, 3, 4]])\n        steps.append(step)\n    active_index = len(steps) // 2\n    sliders = [dict(active=len(steps) // 2, steps=steps)]\n    return go.Figure(data=[\n        go.Scatter(\n            name=\"f\", x=x_linspace, y=f(x_linspace), hoverinfo=\"none\"\n        ),\n        go.Scatter(\n            name=\"Convex Hull\", x=np.append(points[:, 0], points[0][0]),\n            y=np.append(points[:, 1], points[0][1]),\n            fillcolor=\"rgba(239, 85, 59, 0.2)\", fill=\"toself\", mode=\"lines\",\n            line=dict(width=3), hoverinfo=\"none\",\n            showlegend=False\n        ),\n        go.Scatter(\n            x=[hull_points[active_index][0], hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0]), hull_points[active_index][1]],\n            mode=\"lines\",\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            line={\"color\": \"black\", \"dash\": \"dot\", \"width\": 1},\n            showlegend=False\n        ),\n        go.Scatter(\n            name=\"A\",\n            x=[hull_points[active_index][0]],\n            y=[hull_points[active_index][1]],\n            mode=f\"markers+text\",\n            text=[\"$(\\\\sum \\\\lambda_i x_i, \\\\sum \\\\lambda_i f(x_i))$\"],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"black\"},\n        ),\n        go.Scatter(\n            name=\"B\",\n            x=[hull_points[active_index][0]],\n            y=[f(hull_points[active_index][0])],\n            mode=f\"markers\",\n            text=[\"B\"],\n            textposition=\"bottom center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            marker={\"size\": 20, \"color\": \"#00CC96\"},\n        ),\n        go.Scatter(\n            name=\"$(x_i, f(x_i))$\",\n            x=points[:, 0], y=points[:, 1],\n            mode=\"markers+text\",\n            marker={\"size\": 20, \"color\": \"#ffa15a\"},\n            line={\"color\": \"rgba(239, 85, 59, 0.2)\"},\n            text=[f\"$(x_{i},f(x_{i}))$\" for i in range(1, n + 1)],\n            textposition=\"top center\",\n            hovertemplate=\"(%{x:.2f}, %{y:.2f})&lt;extra&gt;&lt;/extra&gt;\",\n            showlegend=False,\n        ),\n    ], layout=go.Layout(\n        title=titles[active_index],\n        xaxis=dict(fixedrange=True, range=x_range),\n        yaxis=dict(fixedrange=True, scaleanchor=\"x\", scaleratio=1, range=y_range),\n        sliders=sliders,\n        legend=dict(\n            yanchor=\"top\",\n            xanchor=\"right\",\n            x=1,\n            y=1\n        ),\n        margin=dict(l=5, r=5, t=60, b=20)\n    ))\n\n\ndef sample_parabola(x):\n    return 0.15 * (x - 15) ** 2 + 15\n\n\n\n\n\ncode for show_sample_jensen_inequality\ndef show_sample_jensen_inequality(x: list):\n    fig = fig_jensen_inequality(\n        f=sample_parabola,\n        x_range=[0, 30],\n        x=np.array(x),\n        y_range=[12, 48]\n    )\n    return fig.show(renderer=\"iframe\")"
  },
  {
    "objectID": "posts/jensens-inequality/index.html#convex-function",
    "href": "posts/jensens-inequality/index.html#convex-function",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "A function is a convex function when the line segment joining any two points on the function graph lies above or on the graph. In the simplest term, a convex function is shaped like \\(\\cup\\) and a concave function is shaped like \\(\\cap\\). If f is convex, then -f is concave.\nA visualization from Wikipedia:\n\n\ndisplay image from Wikipedia\nfrom IPython.display import Image\nImage(url='https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg', width=400)\n\n\n\n\n\n\n\nA function \\(f: X \\rightarrow \\mathbb{R}\\) is called convex if the following holds:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\le \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nand concave when:\n\\[\nf(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)\n\\]\nfor all \\(0 \\le \\lambda \\le 1\\) and for all \\(x_1, x_2 \\in X\\).\nWe will give geometric intuition for this definition in the next section.\n\n\n\n\nshow_sample_jensen_inequality(x=[2, 22])\n\n\n\n\nAn interactive visualization of the convex function: \\(f(x)=0.15(x - 15)^2 + 15\\). We will use the same parabola during this post unless stated otherwise. You can use the slider to try different values of (\\(\\lambda_1\\), \\(\\lambda_2)\\), where \\(\\lambda_1=\\lambda\\) (from the definition) and \\(\\lambda_2=1-\\lambda\\).\nWe have a line segment that joins \\((x_1, f(x_1))\\) and \\((x_2, f(x_2))\\). We can sample any point along the line segment with \\((\\lambda_1 x_1 + \\lambda_2 x_2, \\lambda_1 f(x_1) + \\lambda_2 f(x_2))\\) where \\(0 \\le \\lambda_1 \\le 1\\) and \\(\\lambda_2 = 1 - \\lambda_1\\). For example:\n\nWhen \\(\\lambda_1=1\\), we get the first point\nWhen \\(\\lambda_1=0\\), we get the second point\nAnd when \\(\\lambda_1=0.5\\), we get the middle point of the line segment\nand so on… Try the slider above and notice how \\(\\lambda_1\\) and \\(\\lambda_2\\) are changing!\n\nThis point is visualized with a black point above. Let’s name it as A.\nThe light green point where the function graph intersects with the dotted line segment is represented by: \\((\\lambda_1 x_1 + \\lambda_2 x_2, f(\\lambda_1 x_1 + \\lambda_2 x_2))\\). Let’s name it as B.\nThen, the definition above is just asserting that \\(B_y \\le A_y\\) and we also have \\(A_x = B_x\\). Note that we are only showing a single line segment, but this statement should be true for all similar line segments and for all \\(0 \\le \\lambda_1 \\le 1\\)."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#jensens-inequality",
    "href": "posts/jensens-inequality/index.html#jensens-inequality",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality is a generalization of the above convex function definition for more than 2 points.\n\n\nAssume we have a convex function \\(f\\) and \\(x_1, x_2, \\cdots, x_n\\) in \\(f\\)’s domain, and also positive weights \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\) where \\(\\sum_{i=1}^n \\lambda_i = 1\\). Then Jensen’s inequality can be stated as:\n\\[\nf(\\sum_{i=1}^n \\lambda_i x_i) \\le \\sum_{i=1}^n \\lambda_i f(x_i)\n\\]\n\n\n\n\n\n\nConcave Function\n\n\n\n\n\nThe equation is flipped for a concave function g:\n\\[\ng(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\sum_{i=1}^n \\lambda_i g(x_i)\n\\]\n\n\n\nNote that we arrive at the same definition for convex function when \\(n=2\\).\n\n\n\nA numerous proofs are already available by other posts. I encourage you to checkout the following resources:\n\nen.wikipedia.org/wiki/Jensen%27s_inequality#Proofs\nbrilliant.org/wiki/jensens-inequality\nartofproblemsolving.com/wiki/…\nPhysics intuition (1993)\n\nHere I describe a geometric intuition, which resonates more with me.\n\n\nLet’s start with a triangle, i.e., \\(n=3\\):\n\nshow_sample_jensen_inequality(x=[2, 12, 27])\n\n\n\n\nAs before, you can use the slider to try different values of \\((\\lambda_1, \\lambda_2, \\lambda_3)\\) where \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\).\nWe have a triangle that connects the points: \\((x_1, f(x_1)), (x_2, f(x_2)), (x_3, f(x_3))\\).\nIn the \\(n=2\\) case, we used \\(\\lambda_1\\) and \\(\\lambda_2\\) to sample a point along the line segment. In this case, it is similar, but we can sample any point inside or on the boundaries of the triangle with:\n\\[\n\\left(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, \\lambda_1f(x_1)+\\lambda_2f(x_2)+\\lambda_3f(x_3)\\right)\n\\]\nwhere \\(\\lambda_1+\\lambda_2+\\lambda_3=1\\). For example:\n\nWhen \\(\\lambda_i=1\\) where \\(i \\in \\{1, 2, 3\\}\\), we get the point \\((x_i, f(x_i))\\)\nWhen \\(\\lambda_1=\\lambda_2=\\lambda_3=\\frac{1}{3}\\), we get the center of mass of the triangle\n\nThe black point (named A) in the visualization represents this point.\nNote that (\\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\lambda_3\\)) describes the barycentric coordinate system. You don’t need to know it in this post, just sharing in case you’re already familiar with it.\nThe light green point where the parabola meets the dotted line segment is represented by:\n\\[\n(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3, f(\\lambda_1x_1+\\lambda_2x_2+\\lambda_3x_3))\n\\]\nIf we name this point as B, then it is not difficult to see that Jensen’s inequality is the same as \\(B_y \\le A_y\\).\n\n\n\nIt is easy to generalize for \\(n&gt;3\\). I am adding it here for the sake of completeness:\n\nshow_sample_jensen_inequality(x=[2, 13, 22, 25])\n\n\n\n\nIn the general case, \\((\\sum_{i=1}^n \\lambda_ix_i, \\sum_{i=1}^n \\lambda_if(x_i))\\) describes a point inside or on the boundary of the convex hull enclosing the points: \\((x_1, f(x_1)), (x_2, f(x_2)), \\cdots, (x_n, f(x_n))\\). The convex hull is always above or on the convex function graph, which is why Jensen’s inequality holds true.\nA few closing notes:\n\nThe convex hull may have any number of points, including n → ∞\nWe closely approximate the convex function in some interval with the convex hull as n approaches infinity\nThe convexity definitions for functions and polygons are the same once we have enough points, i.e., n → ∞\nJensen’s inequality is useful in a probability theory setting, since \\(\\sum_{i=1}^n \\lambda_i = 1\\), including the continuous form with n → ∞."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#applications",
    "href": "posts/jensens-inequality/index.html#applications",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "AM–GM inequality\n\n\n\n\n\nThe arithmetic mean-geometric mean inequality (AM-GM inequality) states that: \\[\n\\frac{x_1+x_2+\\cdots+x_n}{n} \\ge \\sqrt[n]{x_1x_2\\cdots x_n}\n\\]\nLet’s prove with Jensen’s inequality by rewriting the above with \\(\\lambda_1=\\lambda_2=\\cdots=\\lambda_n=\\frac{1}{n}\\):\n\\[\n\\sum_{i=1}^n \\lambda_i x_i \\ge \\prod_{i=1}^n x_i^{\\lambda_i}\n\\]\nSince log is a concave and monotonic function, we can apply log to both sides.\n\\[\n\\log(\\sum_{i=1}^n \\lambda_i x_i) \\ge \\log(\\prod_{i=1}^n x_i^{\\lambda_i}) = \\sum_{i=1}^n \\lambda_i \\log(x_i)\n\\]\nThe above equation is valid due to Jensen’s inequality. Note that the same proof works for the weighted version since the proof does not rely on the fact that \\(\\lambda_i=\\frac{1}{n}\\) for all \\(i=1,2,\\cdots,n\\)."
  },
  {
    "objectID": "posts/jensens-inequality/index.html#the-end",
    "href": "posts/jensens-inequality/index.html#the-end",
    "title": "Geometric Intuition for Jensen’s Inequality",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\nFeel free to leave feedback on my telegram channel.\n\n\n\n\n\nThe lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the sum of absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the same equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying less important features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero.\n\n\n\n\n\n\nThe best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?\n\n\n\n\n\n\nWe will use a simple loss function that illustrates circle contours instead of elliptical ones.:\n\\[\nLoss(\\beta_1,\\beta_2 | c_x, c_y)=2{(\\beta_1 - c_x)}^2 + 2{(\\beta_2 - c_y)}^2 + 100\n\\]\nOnce we understand the intution for circles, it is easy to extend to other contours such as elliptical ones. We will use \\(c_x=15\\) and \\(c_y=5\\) most of the time.\n\n\ncode for loss setup and helper functions\nfrom enum import Enum\n\nimport mpl_toolkits.mplot3d.art3d as art3d\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else:\n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n\n\ndef argmin_within_constraint(reg: Reg, t: float):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    if reg == Reg.L1:\n        mask = np.abs(B0) + np.abs(B1) &lt;= t\n    else:\n        mask = B0 * B0 + B1 * B1 &lt;= t * t\n    index = np.argmin(Z[mask])\n    return B0[mask][index], B1[mask][index]\n\n\nbeta_range = -20, 20\ncx, cy = 15, 5\nvmax = 1000\n\n\n\n\ncode for plot3d\ndef base_fig3():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0, 500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n    # draw axes\n    ax.plot(beta_range, [0, 0], color='k')\n    ax.plot([0, 0], beta_range, color='k')\n    return fig, ax\n\n\ndef plot3d(reg: Reg, t=3):\n    fig, ax = base_fig3()\n\n    # surface\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n\n    # contours\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', zdir='z', offset=0, vmax=vmax)\n    \n    # minima within regularization shape\n    mx, my = argmin_within_constraint(reg, t)\n    ax.plot([mx], [my], marker='.', markersize=10, color='r')\n\n    # regularization contraints\n    reg_shape = make_reg_shape(reg, t, color=\"black\")\n    ax.add_patch(reg_shape)\n    art3d.pathpatch_2d_to_3d(reg_shape, z=0)\n\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\n\nLet’s visualize our loss \\(2{(\\beta_1 - 15)}^2 + 2{(\\beta_2 - 5)}^2 + 100\\) with the L1 constaint \\(t=5\\), i.e., \\(|\\beta_1| + |\\beta_2| \\le 5\\) in 3D:\n\nplot3d(Reg.L1, t=5)\n\n\n\n\n\n\n\n\nIn Lasso Regression, we’re looking for \\(\\beta_1\\) and \\(\\beta_2\\) within the diamond that has the lowest loss, which is marked with the red point in the figure above. The global minima without any constraint is marked with “x”.\nThe same visualization but with the L2 constraint t=5, i.e., \\(\\beta_1^2+\\beta_2^2 \\le 5^2\\):\n\nplot3d(Reg.L2, t=5)\n\n\n\n\n\n\n\n\nThe corresponding 2D visualizations that are similar to the ones given by the Elements of Statistical Learning book:\n\n\ncode for plot2d\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n\n\ndef ax2d_init(ax):\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n\ndef plot2d(regs: [Reg], t: float):\n    fig = plt.figure(figsize=(4 * len(regs), 4))\n    axes = fig.subplots(1, len(regs))\n    for ax in axes:\n        ax2d_init(ax)\n    for reg, ax in zip(regs, axes):\n        # draw the regularization safe region\n        ax.add_patch(make_reg_shape(reg=reg, t=t))\n        loss_contour(ax)\n        # draw minima within constraint\n        mx, my = argmin_within_constraint(reg, t)\n        ax.plot([mx], [my], marker='.', markersize=10, color='r')\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot2d([Reg.L1, Reg.L2], t=5)\n\n\n\n\n\n\n\n\n\n\n\nIn both cases (L1, L2), we’re looking for a contour that just touches the constraint region. For example, when we have the following case:\n\n\ncode for plot_reg_and_circle\ndef plot_reg_and_circle(reg: Reg, t: float, cx: float, cy: float, radius: float):\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    ax.add_patch(Circle(xy=(cx, cy), radius=radius, color='b', fill=False))\n    ax.add_patch(make_reg_shape(reg, t=t))\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_reg_and_circle(Reg.L1, t=7, cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nThen it is more optimal to reduce the constraint region (i.e., the diamond) until they touch in a single point as shown below. Note that a contour has the same loss along its points.\n\nplot_reg_and_circle(Reg.L1, t=(np.sqrt(50)-4) * np.sqrt(2), cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nWhen does the Lasso pick the corner of the diamond over any other point on the edges? To simplify our problem, let’s fix a specific circle among the contours: a circle with a fixed radius \\(r\\). Now, we want to come up with a formula that gives the probability of a random tangent circle with radius \\(r\\) touching our diamond at the corners versus at any other points.\nThe above problem becomes much simpler with a visualization:\n\n\ncode for html5_video_l1_tangent_circles\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t + np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop())\n    vertices.reverse()\n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        group = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner:\n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                group.append(corner + vec)\n            else:\n                group.append(v)\n        locations.append(np.array(group))\n    return locations\n\n\ndef l2_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    angles = np.linspace(0, 2 * np.pi, 200)\n    vertices = np.column_stack([np.cos(angles), np.sin(angles)]) * (t + radius) \n    return [vertices]\n\n\ndef animate_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    if reg == Reg.L1:\n        diamond = make_reg_shape(Reg.L1, t=t)\n        circle_locations = l1_tangent_circle_locations(t, radius)\n        plots = []\n        for i, locations in enumerate(circle_locations):\n            color = 'g' if i % 2 == 0 else 'b'\n            plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    else:\n        circle_locations = l2_tangent_circle_locations(t, radius)\n        plots = [ax.plot([], [], color='b', linewidth=2)[0]]\n    \n    circle = Circle(xy=(t + radius, 0), radius=radius, color='g', fill=False)\n    ax.add_patch(circle)\n    ax.add_patch(make_reg_shape(reg, t=t))\n\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\ndef html5_video_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    ani = animate_tangent_circle_trajectories(reg, t=t, radius=radius)\n    html = ani.to_html5_video()\n    plt.close()\n    return html\n\n\n\nfrom IPython.display import HTML\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe animation above illustrates all tangent circle trajectories. The circle is green when it touches the diamond at the corners, and it is blue when it touches at other points. The total length of all blue (touching at other points) trajectories is given by \\(4\\times \\sqrt{2}\\times t\\), while the total length of all green (touching at the corners) trajectories is given by \\(2\\pi \\times r\\), where t is the constraint and r is the radius of the circle. This means that it is likely to touch at the corners when t is sufficiently small or r is sufficiently high. We illustrate one of the such cases below:\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=3, radius=10))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIn Ridge (L2), we don’t have special points like corners, and all points along the disk have the same probability of touching a random tangent circle with radius \\(r\\):\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L2, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\n\n\nI hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\nThe post contains collapsed code sections that are used to produce the visualizations. They’re optional, hence collapsed.\nFeel free to leave feedback on my telegram channel."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the sum of absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the same equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying less important features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "We will use a simple loss function that illustrates circle contours instead of elliptical ones.:\n\\[\nLoss(\\beta_1,\\beta_2 | c_x, c_y)=2{(\\beta_1 - c_x)}^2 + 2{(\\beta_2 - c_y)}^2 + 100\n\\]\nOnce we understand the intution for circles, it is easy to extend to other contours such as elliptical ones. We will use \\(c_x=15\\) and \\(c_y=5\\) most of the time.\n\n\ncode for loss setup and helper functions\nfrom enum import Enum\n\nimport mpl_toolkits.mplot3d.art3d as art3d\nimport numpy as np\nfrom matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else:\n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n\n\ndef argmin_within_constraint(reg: Reg, t: float):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    if reg == Reg.L1:\n        mask = np.abs(B0) + np.abs(B1) &lt;= t\n    else:\n        mask = B0 * B0 + B1 * B1 &lt;= t * t\n    index = np.argmin(Z[mask])\n    return B0[mask][index], B1[mask][index]\n\n\nbeta_range = -20, 20\ncx, cy = 15, 5\nvmax = 1000\n\n\n\n\ncode for plot3d\ndef base_fig3():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0, 500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n    # draw axes\n    ax.plot(beta_range, [0, 0], color='k')\n    ax.plot([0, 0], beta_range, color='k')\n    return fig, ax\n\n\ndef plot3d(reg: Reg, t=3):\n    fig, ax = base_fig3()\n\n    # surface\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n\n    # contours\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', zdir='z', offset=0, vmax=vmax)\n    \n    # minima within regularization shape\n    mx, my = argmin_within_constraint(reg, t)\n    ax.plot([mx], [my], marker='.', markersize=10, color='r')\n\n    # regularization contraints\n    reg_shape = make_reg_shape(reg, t, color=\"black\")\n    ax.add_patch(reg_shape)\n    art3d.pathpatch_2d_to_3d(reg_shape, z=0)\n\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\n\nLet’s visualize our loss \\(2{(\\beta_1 - 15)}^2 + 2{(\\beta_2 - 5)}^2 + 100\\) with the L1 constaint \\(t=5\\), i.e., \\(|\\beta_1| + |\\beta_2| \\le 5\\) in 3D:\n\nplot3d(Reg.L1, t=5)\n\n\n\n\n\n\n\n\nIn Lasso Regression, we’re looking for \\(\\beta_1\\) and \\(\\beta_2\\) within the diamond that has the lowest loss, which is marked with the red point in the figure above. The global minima without any constraint is marked with “x”.\nThe same visualization but with the L2 constraint t=5, i.e., \\(\\beta_1^2+\\beta_2^2 \\le 5^2\\):\n\nplot3d(Reg.L2, t=5)\n\n\n\n\n\n\n\n\nThe corresponding 2D visualizations that are similar to the ones given by the Elements of Statistical Learning book:\n\n\ncode for plot2d\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n\n\ndef ax2d_init(ax):\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n\ndef plot2d(regs: [Reg], t: float):\n    fig = plt.figure(figsize=(4 * len(regs), 4))\n    axes = fig.subplots(1, len(regs))\n    for ax in axes:\n        ax2d_init(ax)\n    for reg, ax in zip(regs, axes):\n        # draw the regularization safe region\n        ax.add_patch(make_reg_shape(reg=reg, t=t))\n        loss_contour(ax)\n        # draw minima within constraint\n        mx, my = argmin_within_constraint(reg, t)\n        ax.plot([mx], [my], marker='.', markersize=10, color='r')\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot2d([Reg.L1, Reg.L2], t=5)\n\n\n\n\n\n\n\n\n\n\n\nIn both cases (L1, L2), we’re looking for a contour that just touches the constraint region. For example, when we have the following case:\n\n\ncode for plot_reg_and_circle\ndef plot_reg_and_circle(reg: Reg, t: float, cx: float, cy: float, radius: float):\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    ax.add_patch(Circle(xy=(cx, cy), radius=radius, color='b', fill=False))\n    ax.add_patch(make_reg_shape(reg, t=t))\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_reg_and_circle(Reg.L1, t=7, cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nThen it is more optimal to reduce the constraint region (i.e., the diamond) until they touch in a single point as shown below. Note that a contour has the same loss along its points.\n\nplot_reg_and_circle(Reg.L1, t=(np.sqrt(50)-4) * np.sqrt(2), cx=5, cy=5, radius=4)\n\n\n\n\n\n\n\n\nWhen does the Lasso pick the corner of the diamond over any other point on the edges? To simplify our problem, let’s fix a specific circle among the contours: a circle with a fixed radius \\(r\\). Now, we want to come up with a formula that gives the probability of a random tangent circle with radius \\(r\\) touching our diamond at the corners versus at any other points.\nThe above problem becomes much simpler with a visualization:\n\n\ncode for html5_video_l1_tangent_circles\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t + np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop())\n    vertices.reverse()\n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        group = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner:\n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                group.append(corner + vec)\n            else:\n                group.append(v)\n        locations.append(np.array(group))\n    return locations\n\n\ndef l2_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    angles = np.linspace(0, 2 * np.pi, 200)\n    vertices = np.column_stack([np.cos(angles), np.sin(angles)]) * (t + radius) \n    return [vertices]\n\n\ndef animate_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax2d_init(ax)\n    if reg == Reg.L1:\n        diamond = make_reg_shape(Reg.L1, t=t)\n        circle_locations = l1_tangent_circle_locations(t, radius)\n        plots = []\n        for i, locations in enumerate(circle_locations):\n            color = 'g' if i % 2 == 0 else 'b'\n            plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    else:\n        circle_locations = l2_tangent_circle_locations(t, radius)\n        plots = [ax.plot([], [], color='b', linewidth=2)[0]]\n    \n    circle = Circle(xy=(t + radius, 0), radius=radius, color='g', fill=False)\n    ax.add_patch(circle)\n    ax.add_patch(make_reg_shape(reg, t=t))\n\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\ndef html5_video_tangent_circle_trajectories(reg: Reg, t: float, radius: float):\n    ani = animate_tangent_circle_trajectories(reg, t=t, radius=radius)\n    html = ani.to_html5_video()\n    plt.close()\n    return html\n\n\n\nfrom IPython.display import HTML\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe animation above illustrates all tangent circle trajectories. The circle is green when it touches the diamond at the corners, and it is blue when it touches at other points. The total length of all blue (touching at other points) trajectories is given by \\(4\\times \\sqrt{2}\\times t\\), while the total length of all green (touching at the corners) trajectories is given by \\(2\\pi \\times r\\), where t is the constraint and r is the radius of the circle. This means that it is likely to touch at the corners when t is sufficiently small or r is sufficiently high. We illustrate one of the such cases below:\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L1, t=3, radius=10))\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nIn Ridge (L2), we don’t have special points like corners, and all points along the disk have the same probability of touching a random tangent circle with radius \\(r\\):\n\nHTML(html5_video_tangent_circle_trajectories(Reg.L2, t=5, radius=6))\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#end",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#end",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "I hope you enjoyed this post. You can ask further questions on my telegram channel"
  },
  {
    "objectID": "posts/linear-equation/index.html",
    "href": "posts/linear-equation/index.html",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "A linear equation is an equation in the form \\(a_1x_1+\\cdots+a_nx_n+b=0\\). In two dimensions, it’s an equation of a line \\(ax+by+c=0\\), while in three dimensions, it’s an equation of a plane \\(ax+by+cz+d=0\\).\nA linear equation is the foundation of linear models in machine learning, such as linear regression and logistic regression.\nIn this post, let’s understand the intuitions behind the linear equation, such as:\n\nWhy \\(ax+by+c\\) is the distance between the point \\((x, y)\\) and the line?\nWhat are the meanings of \\((a, b)\\) and \\(c\\)?\nHow are the dot product and linear equation related?\n\nMy previous post about the dot product is a required prerequisite to this post. You need to have a good intuition about the dot product.\n\n\n\nA short summary for people in hurry.\nThe distance from the point (x, y) to the line is: \\[\n\\left|\\frac{ax+by+c}{\\sqrt{a^2+b^2}}\\right|\n\\]\nin n-dimensions:\n\\[\n\\left|\\frac{a_1x_1+\\cdots+a_nx_n+b}{\\sqrt{a_1^1+\\cdots+a_n^2}}\\right|\n\\]\nThe vector \\(\\textbf{n}=[a, b]\\) is the orthogonal vector to the line \\(ax+by+c=0\\), and \\(\\textbf{n}=[a_1, a_2, \\cdots, a_n]\\) in n-dimensions.\nThe line equation can be expressed with the dot product as \\(\\textbf{n} \\cdot {[x, y]}+c=0\\) and \\(\\textbf{n} \\cdot \\textbf{x} + b = 0\\) in n-dimensions.\n\n\n\n\n\n\nLet’s start with a simpler equation of a line in the form of \\(ax+by=0\\), where we have \\(c=0\\). Does it remind you something ? Yes, the dot product between \\((a, b)\\) and \\((x, y)\\). The dot product \\((a, b) \\cdot (x, y)\\) is zero when the two vectors are orthogonal. Oh nice, this means that (a, b) is an orthogonal vector to our line. An orthogonal vector is also known as a normal vector of a line. Also, notice that the simpler line contains the origin since \\(a\\cdot0+b\\cdot0=0\\) holds true.\nLet’s visualize our setup:\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5, grid=false) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: grid,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board1() {\n  const board = init_board(\"board1\", /*extend=*/10, /*grid=*/true);\n  const pointN = board.create(\"point\", [3, 3], {\n    name: \"n\",\n    color: \"red\",\n    snapToGrid: true,\n  })\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n  })\n  const line = board.create(\"line\", [function() {\n    return 0;\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true\n  });\n  const lineText = board.create(\"text\", [-8, -8, function() {\n    return pointN.X() + \"x + \" + pointN.Y() + \"y = 0\";\n  }], {\n    fontSize: 19\n  });\n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board1\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board1()\n\n\n\n\n\n\nWe have the following items visualized:\n\nOur line \\(ax+by=0\\) is visualized with blue\n\nIt passes through the origin\n\nA normal vector of the line \\(\\textbf{n}=[a, b]\\) is visualized with red\nNote that multiple \\(\\textbf{n}\\) vectors can represent the same line\n\nFeel free to play (by moving) with the point \\(\\textbf{n}\\) and notice how the line equation is changing with respect to the normal vector.\nWe can multiply or divide the line equation by any real number \\(k\\) without changing the equation, in other words, \\(kax+kby+kc=0\\) and \\(ax+by+c=0\\) hold true to the same set of points. Let’s use this property and divide the equation by \\(\\sqrt{a^2+b^2}\\), since it gives us \\(a^2+b^2=1\\) and \\(\\|\\textbf{n}\\|=1\\).\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nWe obtain a new normalized line equation with the coefficients: \\[\n\\begin{align}\na_{new}&=\\frac{a}{\\sqrt{a^2+b^2}}\\\\\nb_{new}&=\\frac{b}{\\sqrt{a^2+b^2}}\\\\\nc_{new}&=\\frac{c}{\\sqrt{a^2+b^2}}\n\\end{align}\n\\]\nNote that \\(a_{new}^2+b_{new}^2=1\\) holds true.\n\n\n\nFrom now on, we assume that \\(\\|\\textbf{n}\\|=1\\). Let’s check what we have:\n\nfunction make_board(name, props = {}) {\n  function rad(deg) {\n    return deg / 180 * Math.PI;\n  }\n  function vec_scale(v, s) {\n    return [v[0] * s, v[1] * s];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  const showPoint = props.showPoint || false\n  const board = init_board(name, /*extend=*/4.5);\n \n  const sliderAlpha = board.create(\"slider\", [[-1, -2.5], [1, -2.5], [0, 45, 360]], {\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const sliderC = board.create(\"slider\", [[-1, -3.5], [1, -3.5], [-4, 0, 4]], {\n    name: \"c\",\n    snapWidth: 0.1,\n    visible: props.showC || false\n  });\n  const pointN = board.create(\"point\", [function() {\n    return Math.cos(rad(sliderAlpha.Value()));\n  }, function() {\n    return Math.sin(rad(sliderAlpha.Value()));\n  }], {\n    visible: true,\n    name: \"n\",\n    color: \"#00000000\",\n    fixed: true,\n    highlight: false\n  });\n  const line = board.create(\"line\", [function() {\n    return sliderC.Value();\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true,\n    highlight: false\n  });\n  const pointP = board.create(\"point\", [3, 0], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPoint || false \n  });\n  const gliderP = board.create(\"glider\", [1, -1, line], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPointGlider || false,\n  });\n  const pointProj = board.create(\"point\", [function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[0];\n  }, function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[1];\n  }], {\n    visible: false\n  });\n  const lineProj = board.create(\"line\", [pointP, pointProj], {\n    color: \"gray\",\n    strokeWidth: 1,\n    fixed: true,\n    dash: 2,\n    straightFirst:false, \n    straightLast:false,\n    visible: showPoint \n  });\n  const normalAxis = board.create('axis', [[0, 0], pointN], {\n    visible: props.showNormalAxis || false,\n    ticks: {\n      majorHeight: 7,\n      minorHeight: 0,\n    },\n    needsRegularUpdate: true\n  });\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst: false, \n    straightLast: false, \n    lastArrow: true,\n    highlight: false\n  });\n  \n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board2\")\n\n\n\n\n\n\nWe introduced a slider that defines the angle of the normal vector \\(\\textbf{n}\\). We also restricted \\(\\textbf{n}\\) so that \\(\\|\\textbf{n}\\|=1\\). Now, our line is uniquely defined by \\(\\textbf{n}\\), which was not case before.\nLet’s introduce a point \\(\\textbf{p}=[x, y]\\) on the line:\n\nboard_div(\"board3\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board3\", {\n  showPointGlider: true,\n})\n\n\n\n\n\n\nTry moving the new green point! The point \\(\\textbf{p}=[x, y]\\) always lies on the line, in other words:\n\n\\(ax+by=0\\), where \\(\\textbf{n}=[a, b]\\)\nEquivalently \\(\\textbf{n} \\cdot \\textbf{p} = 0\\)\nThe angle defined by the points \\((\\textbf{n}, \\text{origin}, \\textbf{p})\\) is a right angle.\n\nWe have learned so far that \\(a\\) and \\(b\\) represent the normal vector \\(\\textbf{n}=[a, b]\\), and we can express the line equation with the dot product \\(\\textbf{n} \\cdot \\textbf{p} = 0\\).\nWhat is the meaning of the line equation when the point \\(\\textbf{p}\\) is outside of the line, i.e., when \\(ax+by \\neq 0\\)? Spoiler: the value is the signed distance from the point \\(\\textbf{p}\\) to our line. Let’s understand this intuitively.\nThe dot product \\(\\textbf{n} \\cdot \\textbf{p}\\) is a non-zero value when the point \\(\\textbf{p}\\) is not on the line. From the previous post we know that the dot product is the projection of the vector \\(\\textbf{p}\\) to the vector \\(\\textbf{n}\\). Imagine a 1D number line spanned by the vector \\(\\textbf{n}\\) like this:\n\nboard_div(\"board4\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board4\", {\n  showPoint: true,\n  showNormalAxis: true, \n})\n\n\n\n\n\n\nThe introduced 1D number line has ticks that represent the signed distances from our line. The projection of the point \\(\\textbf{p}\\) falls somewhere on that 1D number line (following the dotted line segment), which corresponds to the dot product. Remember that \\(ax+by\\) and \\(\\textbf{n} \\cdot \\textbf{p}\\) are the same. Play with the point \\(\\textbf{p}\\) (and with the slider) to understand when \\(ax+by\\) is zero, positive and negative. Can you see that \\(ax+by\\) (and \\(\\textbf{n} \\cdot \\textbf{p}\\)) represents the signed distance from the point \\(\\textbf{p}\\) to our line?\n\n\n\nBy this point, you need to understand the line equation of the form \\(ax+by=0\\). Now, let’s explore the line equation of the form \\(ax+by+c=0\\)!\nWe represented \\(ax+by\\) with the dot product \\(\\textbf{n} \\cdot \\textbf{p}\\), similarly \\(ax+by+c\\) can be represented with \\(\\textbf{n} \\cdot \\textbf{p}+c\\). First, we land on the 1D number line with \\(\\textbf{n} \\cdot \\textbf{p}\\), then move \\(c\\) steps forward on that 1D number line. It is easier explained with a visualization:\n\nboard_div(\"board5\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board5\", {\n  showPoint: true,\n  showNormalAxis: true,\n  showC: true,\n})\n\n\n\n\n\n\nPlay with the slider for \\(c\\), and notice the location of our line when \\(c=1\\) and \\(c=-1\\).\nCan you see that \\(ax+by+c\\) is still the signed distance from the point \\(\\textbf{p}\\) to our line? What does \\(c\\) represent?\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nFirst, we land on the 1D number line with \\(c'=\\textbf{n} \\cdot \\textbf{p}\\). Then, let’s denote \\(ax+by+c\\) as \\(c'+c\\). The \\(c'+c\\) is zero when \\(c'=-c\\); \\(1\\) when \\(c'=-c+1\\); \\(-1\\) when \\(c'=-c-1\\); and so on.\n\\(|c|\\) is the distance from the origin to our line, c is the signed distance. The signed distance is positive in the direction of the normal vector, and negative in the opposite direction.\n\n\n\n\n\n\n\nI hope you enjoyed this post. Subscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/linear-equation/index.html#introduction",
    "href": "posts/linear-equation/index.html#introduction",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "A linear equation is an equation in the form \\(a_1x_1+\\cdots+a_nx_n+b=0\\). In two dimensions, it’s an equation of a line \\(ax+by+c=0\\), while in three dimensions, it’s an equation of a plane \\(ax+by+cz+d=0\\).\nA linear equation is the foundation of linear models in machine learning, such as linear regression and logistic regression.\nIn this post, let’s understand the intuitions behind the linear equation, such as:\n\nWhy \\(ax+by+c\\) is the distance between the point \\((x, y)\\) and the line?\nWhat are the meanings of \\((a, b)\\) and \\(c\\)?\nHow are the dot product and linear equation related?\n\nMy previous post about the dot product is a required prerequisite to this post. You need to have a good intuition about the dot product."
  },
  {
    "objectID": "posts/linear-equation/index.html#cheatsheet",
    "href": "posts/linear-equation/index.html#cheatsheet",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "A short summary for people in hurry.\nThe distance from the point (x, y) to the line is: \\[\n\\left|\\frac{ax+by+c}{\\sqrt{a^2+b^2}}\\right|\n\\]\nin n-dimensions:\n\\[\n\\left|\\frac{a_1x_1+\\cdots+a_nx_n+b}{\\sqrt{a_1^1+\\cdots+a_n^2}}\\right|\n\\]\nThe vector \\(\\textbf{n}=[a, b]\\) is the orthogonal vector to the line \\(ax+by+c=0\\), and \\(\\textbf{n}=[a_1, a_2, \\cdots, a_n]\\) in n-dimensions.\nThe line equation can be expressed with the dot product as \\(\\textbf{n} \\cdot {[x, y]}+c=0\\) and \\(\\textbf{n} \\cdot \\textbf{x} + b = 0\\) in n-dimensions."
  },
  {
    "objectID": "posts/linear-equation/index.html#equation-of-a-line",
    "href": "posts/linear-equation/index.html#equation-of-a-line",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "Let’s start with a simpler equation of a line in the form of \\(ax+by=0\\), where we have \\(c=0\\). Does it remind you something ? Yes, the dot product between \\((a, b)\\) and \\((x, y)\\). The dot product \\((a, b) \\cdot (x, y)\\) is zero when the two vectors are orthogonal. Oh nice, this means that (a, b) is an orthogonal vector to our line. An orthogonal vector is also known as a normal vector of a line. Also, notice that the simpler line contains the origin since \\(a\\cdot0+b\\cdot0=0\\) holds true.\nLet’s visualize our setup:\n\nJXG = await require('https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.10.1/jsxgraphcore.js')\n\n\n\n\n\n\n\nfunction board_div(name, width=500) {\n  return html`&lt;div \n    id=${name} \n    style='max-width:${width}px; aspect-ratio: 1; \n            margin-inline: auto'&gt;\n  &lt;/div&gt;` \n}\n\n\n\n\n\n\n\nfunction init_board(name, extend=5.5, grid=false) {\n  return JXG.JSXGraph.initBoard(name, {\n    boundingbox: [-extend, extend, extend, -extend],\n    showCopyright: false,\n    showNavigation: false,\n    axis: true,\n    grid: grid,\n    pan: {\n      needTwoFingers: true\n    },\n    defaultAxes: {\n      x: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,   \n          majorHeight: 5,\n          ticksDistance: 1,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      },\n      y: {\n        ticks: {\n          insertTicks: false,\n          minorTicks: 0,\n          majorHeight: 5,\n          strokeOpacity: 0.8,\n          strokeColor: '#000000',\n          strokeWidth: 1 \n        }\n      }\n    },\n  });\n}\n\n\n\n\n\n\n\nfunction make_board1() {\n  const board = init_board(\"board1\", /*extend=*/10, /*grid=*/true);\n  const pointN = board.create(\"point\", [3, 3], {\n    name: \"n\",\n    color: \"red\",\n    snapToGrid: true,\n  })\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst:false, \n    straightLast:false, \n    lastArrow:true,\n  })\n  const line = board.create(\"line\", [function() {\n    return 0;\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true\n  });\n  const lineText = board.create(\"text\", [-8, -8, function() {\n    return pointN.X() + \"x + \" + pointN.Y() + \"y = 0\";\n  }], {\n    fontSize: 19\n  });\n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board1\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board1()\n\n\n\n\n\n\nWe have the following items visualized:\n\nOur line \\(ax+by=0\\) is visualized with blue\n\nIt passes through the origin\n\nA normal vector of the line \\(\\textbf{n}=[a, b]\\) is visualized with red\nNote that multiple \\(\\textbf{n}\\) vectors can represent the same line\n\nFeel free to play (by moving) with the point \\(\\textbf{n}\\) and notice how the line equation is changing with respect to the normal vector.\nWe can multiply or divide the line equation by any real number \\(k\\) without changing the equation, in other words, \\(kax+kby+kc=0\\) and \\(ax+by+c=0\\) hold true to the same set of points. Let’s use this property and divide the equation by \\(\\sqrt{a^2+b^2}\\), since it gives us \\(a^2+b^2=1\\) and \\(\\|\\textbf{n}\\|=1\\).\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nWe obtain a new normalized line equation with the coefficients: \\[\n\\begin{align}\na_{new}&=\\frac{a}{\\sqrt{a^2+b^2}}\\\\\nb_{new}&=\\frac{b}{\\sqrt{a^2+b^2}}\\\\\nc_{new}&=\\frac{c}{\\sqrt{a^2+b^2}}\n\\end{align}\n\\]\nNote that \\(a_{new}^2+b_{new}^2=1\\) holds true.\n\n\n\nFrom now on, we assume that \\(\\|\\textbf{n}\\|=1\\). Let’s check what we have:\n\nfunction make_board(name, props = {}) {\n  function rad(deg) {\n    return deg / 180 * Math.PI;\n  }\n  function vec_scale(v, s) {\n    return [v[0] * s, v[1] * s];\n  }\n  function vec_dot(a, b) {\n    return a[0] * b[0] + a[1] * b[1];\n  }\n  const showPoint = props.showPoint || false\n  const board = init_board(name, /*extend=*/4.5);\n \n  const sliderAlpha = board.create(\"slider\", [[-1, -2.5], [1, -2.5], [0, 45, 360]], {\n    name: \"alpha\",\n    snapWidth: 1\n  });\n  const sliderC = board.create(\"slider\", [[-1, -3.5], [1, -3.5], [-4, 0, 4]], {\n    name: \"c\",\n    snapWidth: 0.1,\n    visible: props.showC || false\n  });\n  const pointN = board.create(\"point\", [function() {\n    return Math.cos(rad(sliderAlpha.Value()));\n  }, function() {\n    return Math.sin(rad(sliderAlpha.Value()));\n  }], {\n    visible: true,\n    name: \"n\",\n    color: \"#00000000\",\n    fixed: true,\n    highlight: false\n  });\n  const line = board.create(\"line\", [function() {\n    return sliderC.Value();\n  }, function() {\n    return pointN.X();\n  }, function() {\n    return pointN.Y();\n  }], {\n    strokewidth: 3,\n    fixed: true,\n    highlight: false\n  });\n  const pointP = board.create(\"point\", [3, 0], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPoint || false \n  });\n  const gliderP = board.create(\"glider\", [1, -1, line], {\n    name: \"p\",\n    size: 7,\n    color: \"green\",\n    visible: props.showPointGlider || false,\n  });\n  const pointProj = board.create(\"point\", [function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[0];\n  }, function() {\n    let p = [pointP.X(), pointP.Y()];\n    let n = [pointN.X(), pointN.Y()];\n    let proj = vec_scale(n, vec_dot(p, n));\n    return proj[1];\n  }], {\n    visible: false\n  });\n  const lineProj = board.create(\"line\", [pointP, pointProj], {\n    color: \"gray\",\n    strokeWidth: 1,\n    fixed: true,\n    dash: 2,\n    straightFirst:false, \n    straightLast:false,\n    visible: showPoint \n  });\n  const normalAxis = board.create('axis', [[0, 0], pointN], {\n    visible: props.showNormalAxis || false,\n    ticks: {\n      majorHeight: 7,\n      minorHeight: 0,\n    },\n    needsRegularUpdate: true\n  });\n  const lineN = board.create(\"line\", [[0, 0], pointN], {\n    strokewidth: 3,\n    color: \"red\",\n    fixed: true,\n    straightFirst: false, \n    straightLast: false, \n    lastArrow: true,\n    highlight: false\n  });\n  \n  return board;\n}\n\n\n\n\n\n\n\nboard_div(\"board2\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board2\")\n\n\n\n\n\n\nWe introduced a slider that defines the angle of the normal vector \\(\\textbf{n}\\). We also restricted \\(\\textbf{n}\\) so that \\(\\|\\textbf{n}\\|=1\\). Now, our line is uniquely defined by \\(\\textbf{n}\\), which was not case before.\nLet’s introduce a point \\(\\textbf{p}=[x, y]\\) on the line:\n\nboard_div(\"board3\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board3\", {\n  showPointGlider: true,\n})\n\n\n\n\n\n\nTry moving the new green point! The point \\(\\textbf{p}=[x, y]\\) always lies on the line, in other words:\n\n\\(ax+by=0\\), where \\(\\textbf{n}=[a, b]\\)\nEquivalently \\(\\textbf{n} \\cdot \\textbf{p} = 0\\)\nThe angle defined by the points \\((\\textbf{n}, \\text{origin}, \\textbf{p})\\) is a right angle.\n\nWe have learned so far that \\(a\\) and \\(b\\) represent the normal vector \\(\\textbf{n}=[a, b]\\), and we can express the line equation with the dot product \\(\\textbf{n} \\cdot \\textbf{p} = 0\\).\nWhat is the meaning of the line equation when the point \\(\\textbf{p}\\) is outside of the line, i.e., when \\(ax+by \\neq 0\\)? Spoiler: the value is the signed distance from the point \\(\\textbf{p}\\) to our line. Let’s understand this intuitively.\nThe dot product \\(\\textbf{n} \\cdot \\textbf{p}\\) is a non-zero value when the point \\(\\textbf{p}\\) is not on the line. From the previous post we know that the dot product is the projection of the vector \\(\\textbf{p}\\) to the vector \\(\\textbf{n}\\). Imagine a 1D number line spanned by the vector \\(\\textbf{n}\\) like this:\n\nboard_div(\"board4\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board4\", {\n  showPoint: true,\n  showNormalAxis: true, \n})\n\n\n\n\n\n\nThe introduced 1D number line has ticks that represent the signed distances from our line. The projection of the point \\(\\textbf{p}\\) falls somewhere on that 1D number line (following the dotted line segment), which corresponds to the dot product. Remember that \\(ax+by\\) and \\(\\textbf{n} \\cdot \\textbf{p}\\) are the same. Play with the point \\(\\textbf{p}\\) (and with the slider) to understand when \\(ax+by\\) is zero, positive and negative. Can you see that \\(ax+by\\) (and \\(\\textbf{n} \\cdot \\textbf{p}\\)) represents the signed distance from the point \\(\\textbf{p}\\) to our line?\n\n\n\nBy this point, you need to understand the line equation of the form \\(ax+by=0\\). Now, let’s explore the line equation of the form \\(ax+by+c=0\\)!\nWe represented \\(ax+by\\) with the dot product \\(\\textbf{n} \\cdot \\textbf{p}\\), similarly \\(ax+by+c\\) can be represented with \\(\\textbf{n} \\cdot \\textbf{p}+c\\). First, we land on the 1D number line with \\(\\textbf{n} \\cdot \\textbf{p}\\), then move \\(c\\) steps forward on that 1D number line. It is easier explained with a visualization:\n\nboard_div(\"board5\", /*width=*/400)\n\n\n\n\n\n\n\nmake_board(\"board5\", {\n  showPoint: true,\n  showNormalAxis: true,\n  showC: true,\n})\n\n\n\n\n\n\nPlay with the slider for \\(c\\), and notice the location of our line when \\(c=1\\) and \\(c=-1\\).\nCan you see that \\(ax+by+c\\) is still the signed distance from the point \\(\\textbf{p}\\) to our line? What does \\(c\\) represent?\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nFirst, we land on the 1D number line with \\(c'=\\textbf{n} \\cdot \\textbf{p}\\). Then, let’s denote \\(ax+by+c\\) as \\(c'+c\\). The \\(c'+c\\) is zero when \\(c'=-c\\); \\(1\\) when \\(c'=-c+1\\); \\(-1\\) when \\(c'=-c-1\\); and so on.\n\\(|c|\\) is the distance from the origin to our line, c is the signed distance. The signed distance is positive in the direction of the normal vector, and negative in the opposite direction."
  },
  {
    "objectID": "posts/linear-equation/index.html#the-end",
    "href": "posts/linear-equation/index.html#the-end",
    "title": "Linear Equation Intuition",
    "section": "",
    "text": "I hope you enjoyed this post. Subscribe to get a notification about future posts."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper. Feel free to ask questions on my telegram channel\n\n\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jensen’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates.\n\n\n\n\n\n\nThe forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jensen’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "Linear Equation Intuition\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDot Product Intuition\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nJan 13, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Intuition for Jensen’s Inequality\n\n\n\n\n\n\nconvex function\n\n\njensen\n\n\n\n\n\n\n\n\n\nJan 4, 2025\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does L1 regularization encourage coefficients to shrink to zero?\n\n\n\n\n\n\nbasics\n\n\nloss\n\n\nl1\n\n\nl2\n\n\nlasso\n\n\nridge\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\n\n\n\n\nstable\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes Notes\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\nautoencoder\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  }
]