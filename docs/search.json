[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "href": "posts/high-resolution-image-synthesis-with-latent-diffusion-models/index.html",
    "title": "WIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes",
    "section": "",
    "text": "Notes for “High-Resolution Image Synthesis with Latent Diffusion Models”\nMy notes for the “High-Resolution Image Synthesis with Latent Diffusion Models” paper. Feel free to ask questions on my telegram channel"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html",
    "href": "posts/auto-encoding-variational-bayes/index.html",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "My notes for the “Auto-Encoding Variational Bayes” paper. Feel free to ask questions on my telegram channel\n\n\n\n\nLet us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable.\n\n\n\n\nWe use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#method",
    "href": "posts/auto-encoding-variational-bayes/index.html#method",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "Let us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\) and we can choose \\(g_\\phi(\\epsilon, x)=\\mu(x) + \\sigma(x) \\cdot \\epsilon\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable."
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "href": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "We use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]\nIn the above equation , only the reconstruction error \\(E_{q_\\phi(\\mathbf{z|x[i]})} \\left[ \\log p_\\theta(\\mathbf{x[i]|z})\\right]\\) requires estimation by sampling, since the KL-divergence term is integrated analytically.\n\n\n\n\n\n\nProof\n\n\n\n\n\nTODO: for now see the KL divergence between two multivariate normal distributions"
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper. Feel free to ask questions on my telegram channel\n\n\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates.\n\n\n\n\n\n\nThe forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "Why does L1 regularization encourage coefficients to shrink to zero?\n\n\n\n\n\n\nbasics\n\n\nloss\n\n\nl1\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nWIP: High-Resolution Image Synthesis with Latent Diffusion Models Notes\n\n\n\n\n\n\nstable\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes Notes\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\nautoencoder\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this.\n\n\n\n\n\nThe lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying unimportant features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero.\n\n\n\n\n\n\nThe best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\nimport mpl_toolkits.mplot3d.art3d as art3d\n\n\ndef loss(b0, b1, cx, cy, bias = 100, scale=2):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\ndef plot3d(reg=\"l1\", t = 3, cx = 7, cy = 1):\n    w,h = 20, 20\n    beta0 = np.linspace(-w, w, 100)\n    beta1 = np.linspace(-h, h, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    vmax = 1000\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0,500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(-w,w)\n    ax.set_ylim(-h,h)\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    # draw axes\n    ax.plot([-w, w], [0, 0], color='black')\n    ax.plot([0, 0], [-h, h], color='black')\n    \n    contr = ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm',\n                        zdir='z', offset=0, vmax=vmax)\n    if reg == \"l2\":\n        safe = Circle(xy=(0,0), radius=t, color=\"black\", fill=False, linestyle='--')\n    else:\n        safe = Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=\"black\", fill=False, linestyle='--')\n    ax.add_patch(safe)\n    art3d.pathpatch_2d_to_3d(safe, z=0)\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\nplot3d(t=5, cx=10, cy=5)\n\n\n\n\n\n\n\n\n\ndef plot2d(reg=\"l1\", t = 3, cx = 7, cy = 1):\n    w,h = 20, 20\n    beta0 = np.linspace(-w, w, 100)\n    beta1 = np.linspace(-h, h, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    vmax = 1000\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(-w,w)\n    ax.set_ylim(-h,h)\n    \n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    # draw axes\n    ax.axhline(y=0, color='black')\n    ax.axvline(x=0, color='black')\n    contr = ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    if reg == \"l2\":\n        safe = Circle(xy=(0,0), radius=t, color=\"black\", fill=False, linestyle='--')\n    else:\n        safe = Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=\"black\", fill=False, linestyle='--')\n    ax.add_patch(safe)\n    plt.tight_layout()\n    plt.show()\n\nplot2d(t=5, cx=10, cy=5)\n\n\n\n\n\n\n\n\n\ndef circle_samples(reg=\"l1\", t = 3, cx = 7, cy = 1):\n    w,h = 20, 20\n    beta0 = np.linspace(-w, w, 100)\n    beta1 = np.linspace(-h, h, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    vmax = 1000\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(-w,w)\n    ax.set_ylim(-h,h)\n    \n    # draw axes\n    ax.axhline(y=0, color='black')\n    ax.axvline(x=0, color='black')\n    if reg == \"l2\":\n        safe = Circle(xy=(0,0), radius=t, color=\"black\", fill=False, linestyle='--')\n    else:\n        safe = Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=\"black\", fill=False, linestyle='--')\n    ax.add_patch(safe)\n    plt.tight_layout()\n    plt.show()\n\ncircle_samples(t=5, cx=10, cy=5)\n\n\n\n\n\n\n\n\n\nfrom typing import Literal\nimport numpy as np\nimport itertools\nfrom enum import Enum\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom matplotlib import cm\nfrom matplotlib.path import Path\nfrom matplotlib.patches import Circle, Polygon, PathPatch\nimport mpl_toolkits.mplot3d.art3d as art3d\n\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\nbeta_range = -20, 20\ncx, cy = 5, 15 \nvmax = 1000\n\n\ndef base_fig2d():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n    return (fig, ax)\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else: \n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n  \n\ndef base_fig2d():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='black')\n    ax.axvline(x=0, color='black')\n\n    return (fig, ax)\n\n\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n   \n\n\ndef plot2d(t, reg: Reg):\n    _, ax = base_fig2d()\n\n    # draw the regularization safe region\n    ax.add_patch(make_reg_shape(reg=reg, t=t))\n    loss_contour(ax)    \n    plt.tight_layout()\n    plt.show()\n\n\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t+np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    new_vertices = []\n\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop(-1))\n    vertices.reverse() \n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        new_vertices = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner: \n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                new_vertices.append(corner + vec)\n            else:\n                new_vertices.append(v)\n        locations.append(np.array(new_vertices))\n    return locations \n\n\ndef plot_l1_and_circle(t: float, cx: float, cy: float, radius: float):\n    fig, ax = base_fig2d()\n    diamond = make_reg_shape(Reg.L1, t=t)\n    circle = Circle(xy=(cx, cy), radius=radius, color='g', fill=False)\n\n    circle_locations = l1_tangent_circle_locations(t, radius)\n    plots = []\n    for i, locations in enumerate(circle_locations): \n        color = 'g' if i % 2 == 0 else 'b'\n        plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    ax.add_patch(circle)\n    ax.add_patch(diamond)\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\nani = plot_l1_and_circle(t=6, cx=7, cy=0, radius=6)\n\nhtml=ani.to_html5_video()\nplt.close()\n\nfrom IPython.display import HTML\nHTML(html)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#introduction",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "Regularization is a common method for dealing with overfitting in Machine Learning (ML). The simplest and most widely used methods are L1 (Lasso) and L2 (Ridge). The L1 and L2 regularizations are well covered in numerous tutorials and books. However, I could not find any good geometric or intuitive explanation of why L1 encourages coefficients to shrink to zero. This post tries to address this."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#recap",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The lasso regression is a linear regression model that shrinks the coefficients by imposing a constraint on their magnitude. Namely, it constrains the absolute values of the coefficients:\n\\[\n\\begin{align}\n  \\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p|\\beta_j| \\le t\n\\end{align}\n\\]\nThe above equation is the equation (3.51) from “The Elements of Statistical Learning” (ESL) book by Hastie, Tibshirani, and Friedman.\nWe can also write the lasso in the equivalent Lagrangian form (3.52), which penalizes the sum of the absolute values of the coefficients:\n\\[\n\\hat{\\beta}^{lasso} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p|\\beta_j|\n\\right\\}\n\\]\nThe equations (3.51) and (3.52) are equivalent under the correct \\(\\lambda\\) and \\(t\\) hyperparameters. The latter equation (3.52) is more preferred in ML.\nMaking \\(t\\) sufficiently small will shrink some of the coefficients to be exactly zero, which we will give geometric intuition later. Thus, the lasso could be used for feature selection, i.e., for identifying unimportant features.\n\n\n\nThe ridge regression is similar to the lasso except it penalizes the sum-of-squares of the coefficients (3.41):\n\\[\n\\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin}\n\\left\\{\n    \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\n    +\\lambda\\sum_{j=1}^p\\beta_j^2\n\\right\\}\n\\]\nAn equivalent way to write the ridge problem is (3.42):\n\\[\n\\begin{align}\n  \\hat{\\beta}^{ridge} = \\underset{\\beta}{argmin} & \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p{x_{ij}\\beta_j} \\right)^2\\\\\n   \\text{ subject to } & \\sum_{j=1}^p\\beta_j^2 \\le t\n\\end{align}\n\\]\nThe ridge regression will shrink the coefficient towards zero when \\(t\\) is sufficiently small; however, the coefficients might not be exactly zero."
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#ridge-vs-lasso",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "The best post I found so far is https://explained.ai/regularization, which explains the difference between L1 and L2 empirically by simulating random loss functions. I really encourage you to check out the tutorial.\nAnother explanation is given by the amazing The Elements of Statistical Learning book:\n\n\n\nFigure 3.11\n\n\nThe constraint region for L2 is the disk \\(\\beta_1^2+\\beta_2^2 \\le t\\) (right figure), while it is the diamond \\(|\\beta_1|+|\\beta_2| \\le t\\) for L1 (left figure). Both methods find the first intersection point between the elliptical contours (loss) and the constrained region. The corners of the diamond have one parameter equal to zero. The diamond becomes a rhomboid in higher dimensional space, and has many corners; there are many more opportunities to intersect at the corners.\nThere are cool algebraic explanations as well, for example, given here https://www.reddit.com/r/MachineLearning/….\nThe explanations given by the book and other places are well-contained, but I could not fully grasp what’s so special about the corners w.r.t the other points on the edges. There are only 4 corners and unlimited points on the edges, so shouldn’t the probability to touch the other points be higher?"
  },
  {
    "objectID": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "href": "posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/index.html#geometric-explanation",
    "title": "Why does L1 regularization encourage coefficients to shrink to zero?",
    "section": "",
    "text": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Polygon\nimport mpl_toolkits.mplot3d.art3d as art3d\n\n\ndef loss(b0, b1, cx, cy, bias = 100, scale=2):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\n\ndef plot3d(reg=\"l1\", t = 3, cx = 7, cy = 1):\n    w,h = 20, 20\n    beta0 = np.linspace(-w, w, 100)\n    beta1 = np.linspace(-h, h, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(111, projection='3d')\n    vmax = 1000\n    ax.plot_surface(B0, B1, Z, alpha=0.7, cmap='coolwarm', vmax=vmax)\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.set_zlim(0,500)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(-w,w)\n    ax.set_ylim(-h,h)\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    # draw axes\n    ax.plot([-w, w], [0, 0], color='black')\n    ax.plot([0, 0], [-h, h], color='black')\n    \n    contr = ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm',\n                        zdir='z', offset=0, vmax=vmax)\n    if reg == \"l2\":\n        safe = Circle(xy=(0,0), radius=t, color=\"black\", fill=False, linestyle='--')\n    else:\n        safe = Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=\"black\", fill=False, linestyle='--')\n    ax.add_patch(safe)\n    art3d.pathpatch_2d_to_3d(safe, z=0)\n    ax.view_init(elev=39, azim=-106)\n    plt.tight_layout()\n    plt.show()\n\nplot3d(t=5, cx=10, cy=5)\n\n\n\n\n\n\n\n\n\ndef plot2d(reg=\"l1\", t = 3, cx = 7, cy = 1):\n    w,h = 20, 20\n    beta0 = np.linspace(-w, w, 100)\n    beta1 = np.linspace(-h, h, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    vmax = 1000\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(-w,w)\n    ax.set_ylim(-h,h)\n    \n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n    # draw axes\n    ax.axhline(y=0, color='black')\n    ax.axvline(x=0, color='black')\n    contr = ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    if reg == \"l2\":\n        safe = Circle(xy=(0,0), radius=t, color=\"black\", fill=False, linestyle='--')\n    else:\n        safe = Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=\"black\", fill=False, linestyle='--')\n    ax.add_patch(safe)\n    plt.tight_layout()\n    plt.show()\n\nplot2d(t=5, cx=10, cy=5)\n\n\n\n\n\n\n\n\n\ndef circle_samples(reg=\"l1\", t = 3, cx = 7, cy = 1):\n    w,h = 20, 20\n    beta0 = np.linspace(-w, w, 100)\n    beta1 = np.linspace(-h, h, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    vmax = 1000\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(-w,w)\n    ax.set_ylim(-h,h)\n    \n    # draw axes\n    ax.axhline(y=0, color='black')\n    ax.axvline(x=0, color='black')\n    if reg == \"l2\":\n        safe = Circle(xy=(0,0), radius=t, color=\"black\", fill=False, linestyle='--')\n    else:\n        safe = Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=\"black\", fill=False, linestyle='--')\n    ax.add_patch(safe)\n    plt.tight_layout()\n    plt.show()\n\ncircle_samples(t=5, cx=10, cy=5)\n\n\n\n\n\n\n\n\n\nfrom typing import Literal\nimport numpy as np\nimport itertools\nfrom enum import Enum\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom matplotlib import cm\nfrom matplotlib.path import Path\nfrom matplotlib.patches import Circle, Polygon, PathPatch\nimport mpl_toolkits.mplot3d.art3d as art3d\n\n\n\nclass Reg(Enum):\n    L1 = 1\n    L2 = 2\n\n\ndef loss(b0, b1, cx, cy, scale=2.0, bias=100):\n    return scale * (b0 - cx) ** 2 + scale * (b1 - cy) ** 2 + bias\n\nbeta_range = -20, 20\ncx, cy = 5, 15 \nvmax = 1000\n\n\ndef base_fig2d():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='k')\n    ax.axvline(x=0, color='k')\n\n    return (fig, ax)\n\n\ndef make_reg_shape(reg: Reg, t: float, color=\"k\"):\n    if reg == Reg.L1:\n        return Polygon(xy=[(t, 0), (0, t), (-t, 0), (0, -t)], color=color, fill=False, linestyle='--')\n    else: \n        return Circle(xy=(0, 0), radius=t, color=color, fill=False, linestyle='--')\n  \n\ndef base_fig2d():\n    # Create a figure and a 3D Axes\n    fig = plt.figure(figsize=(4, 4))\n    ax = fig.add_subplot()\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"$\\\\beta_1$\", labelpad=0)\n    ax.set_ylabel(\"$\\\\beta_2$\", labelpad=0)\n    ax.tick_params(axis='x', pad=0)\n    ax.tick_params(axis='y', pad=0)\n    ax.set_xlim(*beta_range)\n    ax.set_ylim(*beta_range)\n\n    # draw axes\n    ax.axhline(y=0, color='black')\n    ax.axvline(x=0, color='black')\n\n    return (fig, ax)\n\n\ndef loss_contour(ax):\n    beta0 = np.linspace(*beta_range, 100)\n    beta1 = np.linspace(*beta_range, 100)\n    B0, B1 = np.meshgrid(beta0, beta1)\n    Z = loss(B0, B1, cx=cx, cy=cy)\n\n    ax.contour(B0, B1, Z, levels=50, linewidths=.5, cmap='coolwarm', vmax=vmax)\n    # draw the global minima\n    ax.plot([cx], [cy], marker='x', markersize=10, color='black')\n   \n\n\ndef plot2d(t, reg: Reg):\n    _, ax = base_fig2d()\n\n    # draw the regularization safe region\n    ax.add_patch(make_reg_shape(reg=reg, t=t))\n    loss_contour(ax)    \n    plt.tight_layout()\n    plt.show()\n\n\ndef l1_tangent_circle_locations(t: float, radius: float) -&gt; list[np.array]:\n    def is_corner(v):\n        return min(abs(v[0]), abs(v[1])) &lt;= radius / np.sqrt(2) + 1e-3\n\n    vertices = make_reg_shape(Reg.L1, t=t+np.sqrt(2) * radius).get_path().interpolated(50).vertices\n    vertices = vertices.tolist()\n    new_vertices = []\n\n    while is_corner(vertices[-1]):\n        vertices.insert(0, vertices.pop(-1))\n    vertices.reverse() \n    locations = []\n    for i in range(8):\n        should_be_corner = i % 2 == 0\n        new_vertices = []\n        while len(vertices) &gt; 0 and is_corner(vertices[-1]) == should_be_corner: \n            v = vertices.pop()\n            if should_be_corner:\n                if abs(v[0]) &lt;= abs(v[1]):\n                    corner = [0, np.sign(v[1]) * t]\n                else:\n                    corner = [np.sign(v[0]) * t, 0]\n\n                vec = np.array(v) - np.array(corner)\n                vec = (vec / np.linalg.norm(vec)) * radius\n                new_vertices.append(corner + vec)\n            else:\n                new_vertices.append(v)\n        locations.append(np.array(new_vertices))\n    return locations \n\n\ndef plot_l1_and_circle(t: float, cx: float, cy: float, radius: float):\n    fig, ax = base_fig2d()\n    diamond = make_reg_shape(Reg.L1, t=t)\n    circle = Circle(xy=(cx, cy), radius=radius, color='g', fill=False)\n\n    circle_locations = l1_tangent_circle_locations(t, radius)\n    plots = []\n    for i, locations in enumerate(circle_locations): \n        color = 'g' if i % 2 == 0 else 'b'\n        plots.append(ax.plot([], [], color=color, linewidth=2)[0])\n    ax.add_patch(circle)\n    ax.add_patch(diamond)\n    def update(frame):\n        remaining = frame\n        for plot, locations in zip(plots, circle_locations):\n            need = min(remaining, len(locations))\n            remaining -= need\n            plot.set_xdata(locations[:need, 0])\n            plot.set_ydata(locations[:need, 1])\n            if remaining == 0 and need &gt; 0:\n                last = locations[need - 1]\n                circle.set_center(last)\n                circle.set_color(plot.get_color())\n\n    num_frames = np.sum([len(l) for l in circle_locations])\n    plt.tight_layout()\n    ani = animation.FuncAnimation(fig, func=update, frames=range(0, num_frames, 2), interval=30)\n    return ani\n\n\nani = plot_l1_and_circle(t=6, cx=7, cy=0, radius=6)\n\nhtml=ani.to_html5_video()\nplt.close()\n\nfrom IPython.display import HTML\nHTML(html)\n\n\n  \n  Your browser does not support the video tag."
  }
]