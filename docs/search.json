[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper. Feel free to ask questions on my telegram channel\n\n\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates.\n\n\n\n\n\n\nThe forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E_q}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small.\nEfficient training is possible by optimizing random terms of the loss, however it requires two random variables for each term, \\(\\mathbf{x_{t-1}}\\) and \\(\\mathbf{x_{t}}\\), which introduces high variance Monte Carlo estimates. We can reduce the variance:\n\\[\nE_q \\left[\nD_{KL}(q(\\mathbf{x_T|x_0}||p(\\mathbf{x_T}))\n+\n\\sum_{t\\gt1}{D_{KL}(q(\\mathbf{x_{t-1}|x_t,x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t}))}\n-\n\\log p_\\theta(\\mathbf{x_0|x_1})\n\\right]\n\\]\n\n\n\n\n\n\nProof from Appendix A\n\n\n\n\n\n\\[\n\\begin{aligned}\nL &= \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{p_\\theta(\\mathbf{x_{0:T}}) } { q(\\mathbf{x_{1:T}|x_0}) }\\\\\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t\\ge1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_t|x_{t-1}} ) } }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log p(\\mathbf{x_T})\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n\\cdot \\frac{q(\\mathbf{x_{t-1}|x_0}) }{ q(\\mathbf{x_t|x_0}) }\n- \\log \\frac{ p_\\theta(\\mathbf{x_{0}|x_1}) } { q(\\mathbf{x_1|x_0} ) }\n\\right]\\\\\n&=  -\\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\n-\\log \\frac{ p(\\mathbf{x_T}) }{q(\\mathbf{x_T|x_0})}\n-\n\\sum_{t&gt;1}{\\log \\frac{ p_\\theta(\\mathbf{x_{t-1}|x_t}) } { q(\\mathbf{x_{t-1}|x_t, x_0} ) } }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\\\\\n&=  \\mathbb{E}_{q( \\mathbf{x_{1:T}|x0} )} \\left[\nD_{KL}(q(\\mathbf{x_T|x_0} || p(\\mathbf{x}_T))\n+\n\\sum_{t&gt;1}{ D_{KL}( q(\\mathbf{x_{t-1}|x_t, x_0})||p_\\theta(\\mathbf{x_{t-1}|x_t})  )  }\n- \\log p_\\theta(\\mathbf{x_{0}|x_1})\n\\right]\n\\end{aligned}\n\\]\n\n\n\nThe equation above is tractable when conditioned on \\(\\mathbf{x_0}\\):\n\\[\nq(\\mathbf{x_{t-1}|x_t, x_0})=\\mathcal{N}(\\mathbf{x_{t-1}; \\mathbf{\\tilde{\\mu_t}(\nx_t, x_0), \\tilde{\\beta_t}}I})\n\\]\nwhere:\n\\[\n\\mathbf{\\tilde{\\mu_t}(\nx_t, x_0) =\n\\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t }{ 1 - \\bar{\\alpha}_t}x_0\n+\n\\frac{ \\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1}) }{ 1 - \\bar{\\alpha}_t} x_t\n}\n\\]\nand\n\\[\n\\tilde{\\beta}_t=\\frac{ 1-\\bar{\\alpha}_{t-1} }{1-\\bar{\\alpha}_t}\\beta_t\n\\]\nAll KL divergences in the loss are comparisons between Gaussians, which can be calculated in closed form instead of Monte Carlo estimates."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#diffusion-models-and-denoising-autoencoders",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "The forward process variances \\(\\beta_{t}\\) are constants (hyperparameters), thus \\(L_t\\) is a constant during training and can be ignored.\nWe now discuss \\(p_\\theta( \\mathbf{x_{t-1}|x_t} ) = \\mathcal{N}( \\mathbf{x_{t-1}; \\mu_\\theta(x_t, t), \\sum_\\theta(x_t, t )} )\\) . First, we set \\(\\sum_\\theta( \\mathbf{x_t, t} ) = \\sigma_t^2\\mathbf{I}\\) to untrained constant. Experimentally, \\(\\sigma_t^2=\\beta_t\\) and \\(\\sigma_t^2=\\tilde{\\beta}_t=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_t}\\beta_t\\) had similar results.\nSecond, the mean is parameterized by the following analysis of \\(\\mathbf{L_t}\\), where \\(p_\\theta(\\mathbf{x_{t-1} | x_t} )=\\mathcal{N}( \\mathbf{x_{t-1} ; \\mu_\\theta(x_t;t), \\sigma_t^2I } )\\) :\n\\[\nL_{t-1}=\\mathbb{E}_q \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\n\\tilde{\\mu}_t(x_t, x_0) - \\mu_\\theta(x_t, t)\n}\n\\Vert^2\n\\right]\n+\nC\n\\]\nwhere C is a constant that does not depend on \\(\\mathbf{\\theta}\\). The variance terms disappeared since they are the same between the forward and reverse processes. Hence, the most straightforward parameterization of \\(\\mathbf{\\mu_\\theta}\\) is a model that predicts \\(\\mathbf{\\tilde{\\mu}_t}\\).\nHowever, we can expand the equation above further by reparameterizing \\(\\mathbf{x_t(x_0, \\epsilon)=\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon}\\) \\[\n\\begin{aligned}\nL_{t-1}-C&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\mathbf{\\tilde{\\mu_t}\\left(\n  x_t(\n    x_0, \\epsilon),\n    \\frac{1}{ \\sqrt{\\bar{\\alpha}_t} }(x_t(x_0, \\epsilon) - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)\n\\right)}\n-\n\\mathbf{\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n&=E_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{1}{2\\sigma_t^2}\\Vert\n\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t(x_0, \\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon\n\\right)\n-\n\\mu_\\theta(x_t(x_0, \\epsilon), t)\n\\Vert^2\n}\n\\right]\\\\\n\\end{aligned}\n\\]\nThe above equation reveals that \\(\\mathbf{\\mu_\\theta}\\) must predict \\(\\mathbf{ \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon \\right) }\\) given \\(\\mathbf{x_t}\\). We may choose the following parameterization:\n\\[\n\\mathbf {\n\\mu_\\theta(x_t, t)=\\tilde{\\mu}_t\\left(\nx_t,\n\\frac{1}{\\sqrt{\\bar{a}_t}}(x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t))\n\\right)\n=\\frac{1}{\\sqrt{\\alpha_t}} \\left(\nx_t-\\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\n\\right)\n}\n\\]\nWhere \\(\\mathbf{\\epsilon_\\theta}\\) is a function approximator to predict \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x_t}\\). We choose the above parameterization because it is in the same form as \\(\\mathbf{\\tilde{\\mu}(x_t, x_0)}\\).\nTo sample \\(\\mathbf{x_{t-1} \\sim p_\\theta(x_{t-1}|x_t)}\\) is to compute \\(\\mathbf{x_{t-1}}=\\frac{1}{\\sqrt{\\alpha_t}}\\left( \\mathbf{x_t} - \\frac{\\beta_t}{\\sqrt{1-\\tilde{\\alpha}_t}}\\mathbf{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t\\mathbf{z}\\) , where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) . Furthermore, the loss simplifies to:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar{\\alpha}_t)}\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nTo summarize, we can train the reverse process mean function approximator \\(\\mathbf{\\mu_\\theta}\\) to predict \\(\\mathbf{\\mu_t}\\), or we can train \\(\\epsilon_\\theta\\) to predict \\(\\epsilon\\).\n\n\n\n\nIt is simpler to implement to train on the following variant of the lower variational bound, which is a weighted variational bound:\n\\[\nE_{\\mathbf{x_0}, \\epsilon} \\left[\n\\mathbf{\n\\Vert\n\\epsilon\n-\n\\epsilon_\\theta(\n\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, t\n)\n\\Vert^2\n}\n\\right]\n\\]\nThis also leads to better sample quality, since it down-weights loss terms corresponding to small t so that the network can focus on more difficult denoising tasks at larger t terms."
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html",
    "href": "posts/auto-encoding-variational-bayes/index.html",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "My notes for the “Auto-Encoding Variational Bayes” paper. Feel free to ask questions on my telegram channel\n\n\n\n\nLet us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable.\n\n\n\n\nWe use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]"
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#method",
    "href": "posts/auto-encoding-variational-bayes/index.html#method",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "Let us consider some independently and identically distributed (i.i.d) dataset \\(\\mathbf{X}=\\{\\mathbf{x[1]}, \\mathbf{x[2]}, ..., \\mathbf{x[N]}\\}\\). We assume that the data involves an unobserved random variable \\(\\mathbf{z}\\). Then, the process consists of two steps:\n\n\\(\\mathbf{z[i]}\\) is generated from some prior distribution \\(\\mathbf{p_{\\theta*}(z)}\\)\n\\(\\mathbf{x[i]}\\) is generated from some conditional distribution \\(\\mathbf{p_{\\theta*}(x|z)}\\)\n\nUnfortunately, the true parameters \\(\\mathbf{ \\theta* }\\) and the latent variables \\(\\mathbf{z[i]}\\) are unknown to us. Additionally, \\(\\mathbf{p_\\theta(z|x)}\\) is intractable, so we approximate it with \\(\\mathbf{ q_\\phi(z|x) }\\). The model \\(\\mathbf{ q_\\phi(z|x) }\\) is a probabilistic encoder model parameterized by \\(\\phi\\) . Similarly, \\(\\mathbf{p_\\theta(x|z)}\\) is a probabilistic decoder model parameterized by \\(\\mathbf{\\theta}\\).\n\n\n\nIdeally, we would like to optimize the marginal likelihoods of the dataset X:\n\\[\n\\mathbf{\n\\log p_\\theta(x[1], \\cdots,x[N])=\\sum_{i=1}^N{\\log p_\\theta(x[i])}\n}\n\\]\nWhere each term can be rewritten as:\n\\[\n\\mathbf{\n\\log p_\\theta(x[i]) = D_{KL}(q_\\phi(z|x[i])||p_\\theta(z|x[i])) + \\mathcal{L}(\\theta, \\phi, x[i])\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})\n&= \\log p_\\theta(\\mathbf{x})\\int{ \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\n\\qquad \\text{(Multiplying by 1)}\\\\\n&= \\int{\\log p_\\theta(\\mathbf{x}) \\mathbf{q_\\phi(z|x)}d\\mathbf{z}}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log p_\\theta(\\mathbf{x})\n\\right] \\qquad \\text{(Definition of Expectation)}\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\right] \\qquad \\text{(The chain rule of probability)} \\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{p_\\theta(\\mathbf{x, z})}{p_\\theta(\\mathbf{z|x})}\n\\cdot \\frac{\\mathbf{q_\\phi(z|x)}}{\\mathbf{q_\\phi(z|x)}}\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ \\mathbf{q_\\phi(z|x)} }{ p_\\theta(\\mathbf{z|x}) }\n\\right] + \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= D_{KL}( \\mathbf{q_\\phi(z|x)} || p_\\theta(\\mathbf{z|x}) )\n+ \\mathcal{L}(\\theta, \\phi, \\mathbf{x})\n\\end{aligned}\n\\]\nWe used the chain rule of probability:\n\\[\np(\\mathbf{x})=\\frac{p(\\mathbf{x,z})}{p(\\mathbf{z|x})}\n\\]\n\n\n\nThe first term is the KL-divergence between the approximate and the true posterior. Since the KL-divergence is non-negative, the second term is called (variational) lower bound. Ideally, we would like to minimize the both terms. However, it is enough to optimize the lower bound w.r.t both parameters θand φ. Minimizing the lower bound will minimize the KL-divergence as well, since they sum up to a constant value.\nThe variational lower bound, also called as the evidence lower bound (ELBO) can be also rewritten as:\n\\[\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x[i]}) &\\ge \\mathcal{L}(\\theta, \\phi; \\mathbf{x[i]})\\\\\n&= \\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n-\\log q_\\phi(\\mathbf{z|x[i]}) + \\log p_\\theta(\\mathbf{x[i], z})\n\\right]\\\\\n&= -D_{KL}(q_\\phi(\\mathbf{z|x[i]})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x[i]})} \\left[\n\\log p_\\theta(\\mathbf{x[i]|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x})\n&=\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x, z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&= \\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{x|z})p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\\\\\n&=\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log \\frac{ p_\\theta(\\mathbf{z}) }{ \\mathbf{q_\\phi(z|x)} }\n\\right]\n+\n\\mathbb{E}_{\\mathbf{q_\\phi(z|x)}} \\left[\n\\log  p_\\theta(\\mathbf{x|z})\n\\right]\\\\\n&=\n-D_{KL}(q_\\phi(\\mathbf{z|x})||p_\\theta(\\mathbf{z}))\n+\n\\mathbb{E}_{q_\\phi(\\mathbf{z|x})} \\left[\n\\log p_\\theta(\\mathbf{x|z})\n\\right]\n\\end{aligned}\n\\]\n\n\n\n\n\n\nThe sampling from \\(\\mathbf{q_\\phi(z|x)}\\) is a stochastic process which is not differentiable w.r.t. \\(\\phi\\). We can use an alternative method for generating sample from \\(\\mathbf{q_\\phi(z|x)}\\), i.e., the reparameterization trick. We can often express the random variable z as a deterministic variable \\(\\mathbf{z=g_\\phi(\\epsilon, x)}\\), where \\(\\epsilon\\) is an independent variable and \\(\\mathbf{g_\\phi}\\) is a function parameterized by \\(\\phi\\).\nThe \\(\\mathbf{q_\\phi(z|x)}\\) is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior often is a standard Gaussian distribution:\n\\[\n\\mathbf{\nq_\\phi(z|x)=\\mathcal{N}(z; \\mu, \\sigma^2) = \\mu+\\sigma\\cdot\\epsilon\n}\n\\]\nwhere \\(\\mathbf{ \\epsilon \\sim \\mathcal{N}(\\epsilon; 0, I) }\\)\nTherefore, by the reparameterization trick, sampling from an arbitrary Gaussian distribution can be performed by sampling from a standard Gaussian, scaling and shifting the result by the target mean and the deviation, which is deterministic and differentiable."
  },
  {
    "objectID": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "href": "posts/auto-encoding-variational-bayes/index.html#variational-auto-encoder",
    "title": "Auto-Encoding Variational Bayes Notes",
    "section": "",
    "text": "We use a neural network for the probabilistic encoder \\(\\mathbf{ q_\\phi(z|x)}\\) and where the parameters \\(\\phi\\) and \\(\\theta\\) are optimized jointly. We also assume that:\n\n\\(p_\\theta(\\mathbf{z})=\\mathcal{N}(\\mathbf{z; 0, I})\\) - the prior over the latent variables is a standard Gaussian\n\\(p_\\theta(\\mathbf{x|z})\\) is a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data)\n\\(q_\\phi(\\mathbf{z|x})\\) is approximately Gaussian with an approximately diagonal covariance: \\(\\log q_\\phi(\\mathbf{z|x[i]})=\\log \\mathcal{N}(\\mathbf{z; \\mu[i], \\sigma[i]^2I})\\)\n\nWe use the reparameterization trick to sample from the posterior using \\(\\mathbf{z[i, l]}=g_\\phi( \\mathbf{x[i], \\epsilon[l]} )=\\mathbf{\\mu[i]+\\sigma[i]\\cdot \\epsilon[l]}\\). In this model both \\(\\mathbf{p_\\theta(z)}\\) and \\(q_\\phi(\\mathbf{z|x})\\) are Gaussian; hence we compute the KL divergence in a closed form without estimation:\n\\[\n\\mathcal{L}(\\mathbf{\\theta, \\phi, x[i]})\n\\simeq\n\\frac{1}{2}\\sum_{j=1}^J{\\left(\n1+\\log(\\sigma[i][j]^2) - \\mu[i][j]^2 - \\sigma[i][j]^2\n\\right)}\n+\n\\frac{1}{L}\\sum_{l=1}^L{\n\\log p_\\theta(\\mathbf{x}[i] | \\mathbf{z}[i][l])\n}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "Auto-Encoding Variational Bayes Notes\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\nautoencoder\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  }
]