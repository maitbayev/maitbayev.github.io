[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "My notes for the “Denoising Diffusion Probabilistic Models” paper.\n\n\n\n\n\nWe have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small."
  },
  {
    "objectID": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "href": "posts/denoising-diffusion-probabilistic-models/index.html#background",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "We have latent variables \\(\\mathbf{x_1},\\mathbf{x_2}, ..., \\mathbf{x_T}\\) of the same dimensionality as the image \\(\\mathbf{x_0}\\). The forward process or diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the image according to a variance schedule \\(\\beta_1, ..., \\beta_T\\):\n\\[\n\\mathbf{\nq(x_t|x_{t-1})= \\mathcal{N} (x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_{t}I),\n\\quad\nq(x_{1:T}|x_0)= \\prod_{t=1}^T{q(x_t|x_{t-1})}\n}\n\\]\nThe above equation means that we start with our data \\(\\mathbf{x_0}\\), then we sample a noisy version of our image \\(\\mathbf{x_1}\\) with the mean \\(\\mathbf{x_0}\\) scaled by \\(\\mathbf{\\sqrt{1-\\beta_1}}\\) and the variance \\(\\mathbf{\\beta_1}\\). Finally, we repeat this process \\(\\mathbf{T}\\) times and arrive at pure standard Gaussian noise when small enough \\(\\mathbf{\\beta}\\) and large enough \\(\\mathbf{T}\\) values are used. This process is fixed and not learned, unlike variational auto-encoder (VAE) models.\nA nice benefit from the forward process is that we can sample \\(\\mathbf{x_t}\\) at any timestep \\(\\mathbf{t}\\) in closed form without simulating the forward process steps \\(\\mathbf{t}\\) times. Let \\(\\mathbf{\\alpha_t=1-\\beta_t}\\) and \\(\\mathbf{ \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i }\\) :\n\\[\n\\mathbf {\nq(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\n}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have access to random noise variables \\(\\{\\mathbf{\\epsilon_t^*}, \\mathbf{\\epsilon_t}\\}_{t=0}^T \\sim \\mathcal{N}(\\mathbf{\\epsilon_; 0, I})\\):\n\\[\n\\mathbf {\n\\begin{aligned}\nx_t&=\\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon^*_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon^*_{t-1}\\\\\n&= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} +\\sqrt{1-\\alpha_t \\alpha_{t-1}}\\epsilon_{t-2}\\\\\n&= \\cdots\\\\\n&= \\sqrt{\\prod_{t=1}^t{\\alpha_i}}x_0 + \\sqrt{1-\\prod_{t=1}^t{\\alpha_i}}\\epsilon_0\\\\\n&= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}\\epsilon_0\\\\\n&\\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha}_t)I)\\\\\n\\end{aligned}\n}\n\\]\nHere we used the fact that the sum of two independent Gaussian random variables remains a Gaussian with mean being sum of two means, and variance being the sum of two variances.\n\n\n\nWe can view the forward process as transitioning from our real-life complex distribution (image) to a simpler standard Gaussian distribution.\n\n\n\nThe reverse process is a learned Gaussian transition starting at a pure standard Gaussian noise \\(p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})\\), then:\n\\[\np_{\\theta}(\\mathbf{x_{t-1}|x_t}) = \\mathcal{N}(\\mathbf{x_{t-1}}; \\mathbf{\\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)})\n\\quad p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T)\\prod_{t=1}^T{p_\\theta(\\mathbf{x_{t-1}|x_t})}\n\\]\nThe above means that we start with a pure noise \\(\\mathbf{x}_T\\), then we gradually remove noises by sampling from learned means \\(\\mu_\\theta(\\mathbf{x_t; t})\\) and variances \\(\\mathbf{\\Sigma_\\theta(x_t, t)}\\) parameterized by \\(\\mathbf{\\theta}\\).\nWe can view the reverse process as transitioning from a simple standard Gaussian distribution to our real-life complex distribution inferred from the dataset. All possible images are reachable from the final standard Gaussian noise; however, fewer and fewer images will be reachable as we go in reverse. See the images below for reachable images from \\(\\mathbf{x}_{1000}\\), \\(\\mathbf{x}_{750}\\), \\(\\mathbf{x}_{500}\\), \\(\\mathbf{x}_{250}\\), \\(\\mathbf{x}_{0}\\)\n\n\n\n\nTraining is performed by optimizing the variational lower bound (or the evidence lower bound) similar to the VAE:\n\\[\n\\mathbb{E}[-\\log p_\\theta(\\mathbf{x}_0)] \\le\n\\mathbb{E}_q\\left[-\\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]=\n\\mathbb{E}\\left[\n-\\log p(\\mathbf{x_T}) - \\sum_{t\\ge1}\\log{\\frac{p_\\theta(\\mathbf{x_{t-1}|x_t})}{q(\\mathbf{x_{t}|x_{t-1}})}}\n\\right]\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nProof 1: Using Jenson’s Inequality\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p_\\theta(\\mathbf{x})&=log\\int{p_\\theta(\\mathbf{x_{0:T}})d\\mathbf{x_{1:T}}}\\\\\n&=log\\int{ \\frac {p_\\theta(\\mathbf{x_{0:T}}) q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) } {q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} d\\mathbf{x_{1:T}} }\\\\\n&=\\log \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\\left[  \\log\n  \\frac{p_\\theta(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n\\end{aligned}\n}\n\\]\nProof 2: Using KL Divergence\n\\[\n\\mathbf {\n\\begin{aligned}\n\\log p(\\mathbf{x_0})&=\\log p(\\mathbf{x_0}) \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0})  d\\mathbf{x_{1:T}}\n} \\\\\n&= \\int {\n  q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) (\\log p(\\mathbf{x_0})) d\\mathbf{x_{1:T}}\n} \\\\\n&= \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log p(\\mathbf{x_0})\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{q(\\mathbf{x_{0:T}})}{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\\\\\n&=  \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{0:T}}) p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) p_\\theta(\\mathbf{x_{0:T}})}\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\n\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ q(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }{ p_\\theta(\\mathbf{x_{1:T}|x_0})p(\\mathbf{x_0}) }\n\\right]\\\\\n&=\\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n+\nD_{KL}(q(\\mathbf{x_{1:T}}|\\mathbf{x_0}) || p_\\theta(\\mathbf{x_{1:T}|x_0}))\\\\\n&\\ge \\mathbb{E}_{q(\\mathbf{x_{1:T}}|\\mathbf{x_0})} \\left[\n\\log\n\\frac{ p_\\theta(\\mathbf{x_{0:T}}) }{ q(\\mathbf{x_{1:T}}|\\mathbf{x_0})}\n\\right]\n\\end{aligned}\n}\n\\]\nThe KL divergence is always non-negative, hence the lower bound approximation\n\n\n\nThe forward process variances \\(\\beta_t\\) are hyperparameters. The reverse process \\(p_\\theta(\\mathbf{x_{t-1} | x_t})\\) is Gaussian, when the forward process is Gaussian and \\(\\beta_t\\) are small."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madiyar’s Page",
    "section": "",
    "text": "Denoising Diffusion Probabilistic Models\n\n\n\n\n\n\ndiffusion\n\n\npaper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nMadiyar Aitbayev\n\n\n\n\n\n\nNo matching items"
  }
]